{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF: 2.5.0\n",
      "Cuda: True\n",
      "GPU: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' \n",
    "\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tf_netbuilder.files import download_checkpoint\n",
    "from tf_netbuilder_ext.extensions import register_tf_netbuilder_extensions\n",
    "from models import create_openpose_singlenet\n",
    "from dataset.generators import get_dataset\n",
    "from util import plot_to_image, probe_model_singlenet\n",
    "\n",
    "print(\"TF:\", tf.__version__)\n",
    "print(\"Cuda:\", tf.test.is_built_with_cuda())\n",
    "print(\"GPU:\", tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "'''\n",
    "tf.config.set_logical_device_configuration(\n",
    "    physical_devices[0],\n",
    "    [tf.config.LogicalDeviceConfiguration(memory_limit=7168)]\n",
    ")\n",
    "'''\n",
    "pretrained_mobilenet_v3_url = \"https://github.com/michalfaber/tf_netbuilder/releases/download/v1.0/mobilenet_v3_224_1_0.zip\"\n",
    "annot_path_train = '../datasets/coco_2017_dataset/annotations/person_keypoints_train2017.json'\n",
    "img_dir_train = '../datasets/coco_2017_dataset/train2017/'\n",
    "annot_path_val = '../datasets/coco_2017_dataset/annotations/person_keypoints_val2017.json'\n",
    "img_dir_val = '../datasets/coco_2017_dataset/val2017/'\n",
    "checkpoints_folder = './tf_ckpts_singlenet/0629_epoch300'\n",
    "output_weights = 'output_singlenet/0629_epoch300/openpose_singlenet'\n",
    "\n",
    "batch_size = 10\n",
    "lr = 2.5e-5\n",
    "max_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eucl_loss(y_true, y_pred):\n",
    "    return tf.reduce_sum(tf.math.squared_difference(y_pred, y_true))\n",
    "\n",
    "@tf.function\n",
    "def train_one_step(model, optimizer, x, y_true):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x)\n",
    "\n",
    "        losses = [eucl_loss(y_true[0], y_pred[0]),\n",
    "                  eucl_loss(y_true[0], y_pred[1]),\n",
    "                  eucl_loss(y_true[0], y_pred[2]),\n",
    "                  eucl_loss(y_true[1], y_pred[3])\n",
    "                  ]\n",
    "\n",
    "        total_loss = tf.reduce_sum(losses)\n",
    "\n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return losses, total_loss\n",
    "\n",
    "def train(ds_train, ds_val, model, optimizer, ckpt, last_epoch, last_step, max_epochs, steps_per_epoch):\n",
    "    train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "    train_loss_heatmap = tf.keras.metrics.Mean('train_loss_heatmap', dtype=tf.float32)\n",
    "    train_loss_paf = tf.keras.metrics.Mean('train_loss_paf', dtype=tf.float32)\n",
    "\n",
    "    val_loss = tf.keras.metrics.Mean('val_loss', dtype=tf.float32)\n",
    "    val_loss_heatmap = tf.keras.metrics.Mean('val_loss_heatmap', dtype=tf.float32)\n",
    "    val_loss_paf = tf.keras.metrics.Mean('val_loss_paf', dtype=tf.float32)\n",
    "\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    train_log_dir = 'logs_singlenet/gradient_tape/' + current_time + '/train'\n",
    "    train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "    val_log_dir = 'logs_singlenet/gradient_tape/' + current_time + '/val'\n",
    "    val_summary_writer = tf.summary.create_file_writer(val_log_dir)\n",
    "\n",
    "    output_paf_idx = 2\n",
    "    output_heatmap_idx = 3\n",
    "\n",
    "    # determine start epoch in case the training has been stopped manually and resumed\n",
    "    resume = last_step != 0 and (steps_per_epoch - last_step) != 0\n",
    "    if resume:\n",
    "        start_epoch = last_epoch\n",
    "    else:\n",
    "        start_epoch = last_epoch + 1\n",
    "\n",
    "    # start processing\n",
    "    for epoch in range(start_epoch, max_epochs + 1, 1):\n",
    "\n",
    "        start = timer()\n",
    "        print(\"Start processing epoch {}\".format(epoch))\n",
    "\n",
    "        # set the initial step index depending on if you resumed the processing\n",
    "        if resume:\n",
    "            step = last_step + 1\n",
    "            data_iter = ds_train.skip(last_step)\n",
    "            print(f\"Skipping {last_step} steps (May take a few minutes)...\")\n",
    "            resume = False\n",
    "        else:\n",
    "            step = 0\n",
    "            data_iter = ds_train\n",
    "\n",
    "        # process steps\n",
    "        for x, y in data_iter:\n",
    "            step += 1\n",
    "\n",
    "            losses, total_loss = train_one_step(model, optimizer, x, y)\n",
    "\n",
    "            train_loss(total_loss)\n",
    "            train_loss_heatmap(losses[output_heatmap_idx])\n",
    "            train_loss_paf(losses[output_paf_idx])\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                tf.print('Epoch', epoch, f'Step {step}/{steps_per_epoch}', 'Paf1', losses[0], 'Paf2', losses[1], 'Paf3', losses[2],\n",
    "                         'Heatmap', losses[3], 'Total loss', total_loss)\n",
    "\n",
    "                with train_summary_writer.as_default():\n",
    "                    summary_step = (epoch - 1) * steps_per_epoch + step - 1\n",
    "                    tf.summary.scalar('loss', train_loss.result(), step=summary_step)\n",
    "                    tf.summary.scalar('loss_heatmap', train_loss_heatmap.result(), step=summary_step)\n",
    "                    tf.summary.scalar('loss_paf', train_loss_paf.result(), step=summary_step)\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                figure = probe_model_singlenet(model, test_img_path=\"resources/ski_224.jpg\")\n",
    "                with train_summary_writer.as_default():\n",
    "                    tf.summary.image(\"Test prediction\", plot_to_image(figure), step=step)\n",
    "\n",
    "            if step % 1000 == 0:\n",
    "                ckpt.step.assign(step)\n",
    "                ckpt.epoch.assign(epoch)\n",
    "                save_path = manager.save()\n",
    "                print(\"Saved checkpoint for step {}: {}\".format(step, save_path))\n",
    "\n",
    "            if step >= steps_per_epoch:\n",
    "                break\n",
    "\n",
    "        print(\"Completed epoch {}. Saving weights...\".format(epoch))\n",
    "        model.save_weights(output_weights, overwrite=True)\n",
    "\n",
    "        # save checkpoint at the end of an epoch\n",
    "        ckpt.step.assign(step)\n",
    "        ckpt.epoch.assign(epoch)\n",
    "        manager.save()\n",
    "\n",
    "        # reset metrics every epoch\n",
    "        train_loss.reset_states()\n",
    "        train_loss_heatmap.reset_states()\n",
    "        train_loss_paf.reset_states()\n",
    "\n",
    "        end = timer()\n",
    "\n",
    "        print(\"Epoch training time: \" + str(timedelta(seconds=end - start)))\n",
    "\n",
    "        # calculate validation loss\n",
    "        print(\"Calculating validation losses...\")\n",
    "        for val_step, (x_val, y_val_true) in enumerate(ds_val):\n",
    "\n",
    "            if val_step % 1000 == 0:\n",
    "                print(f\"Validation step {val_step} ...\")\n",
    "\n",
    "            y_val_pred = model(x_val)\n",
    "            losses = [eucl_loss(y_val_true[0], y_val_pred[0]),\n",
    "                      eucl_loss(y_val_true[0], y_val_pred[1]),\n",
    "                      eucl_loss(y_val_true[0], y_val_pred[2]),\n",
    "                      eucl_loss(y_val_true[1], y_val_pred[3])\n",
    "                      ]\n",
    "\n",
    "            total_loss = tf.reduce_sum(losses)\n",
    "            val_loss(total_loss)\n",
    "            val_loss_heatmap(losses[output_heatmap_idx])\n",
    "            val_loss_paf(losses[output_paf_idx])\n",
    "\n",
    "        val_loss_res = val_loss.result()\n",
    "        val_loss_heatmap_res = val_loss_heatmap.result()\n",
    "        val_loss_paf_res = val_loss_paf.result()\n",
    "\n",
    "        print(f'Validation losses for epoch: {epoch} : Loss paf {val_loss_paf_res}, Loss heatmap '\n",
    "              f'{val_loss_heatmap_res}, Total loss {val_loss_res}')\n",
    "\n",
    "        with val_summary_writer.as_default():\n",
    "            tf.summary.scalar('val_loss', val_loss_res, step=epoch)\n",
    "            tf.summary.scalar('val_loss_heatmap', val_loss_heatmap_res, step=epoch)\n",
    "            tf.summary.scalar('val_loss_paf', val_loss_paf_res, step=epoch)\n",
    "\n",
    "        val_loss.reset_states()\n",
    "        val_loss_heatmap.reset_states()\n",
    "        val_loss_paf.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=4.91s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading image annot 0/118287\n",
      "Loading image annot 1000/118287\n",
      "Loading image annot 8000/118287\n",
      "Loading image annot 10000/118287\n",
      "Loading image annot 11000/118287\n",
      "Loading image annot 12000/118287\n",
      "Loading image annot 14000/118287\n",
      "Loading image annot 16000/118287\n",
      "Loading image annot 19000/118287\n",
      "Loading image annot 22000/118287\n",
      "Loading image annot 24000/118287\n",
      "Loading image annot 31000/118287\n",
      "Loading image annot 37000/118287\n",
      "Loading image annot 38000/118287\n",
      "Loading image annot 40000/118287\n",
      "Loading image annot 47000/118287\n",
      "Loading image annot 60000/118287\n",
      "Loading image annot 63000/118287\n",
      "Loading image annot 65000/118287\n",
      "Loading image annot 66000/118287\n",
      "Loading image annot 69000/118287\n",
      "Loading image annot 72000/118287\n",
      "Loading image annot 73000/118287\n",
      "Loading image annot 75000/118287\n",
      "Loading image annot 76000/118287\n",
      "Loading image annot 80000/118287\n",
      "Loading image annot 82000/118287\n",
      "Loading image annot 84000/118287\n",
      "Loading image annot 85000/118287\n",
      "Loading image annot 87000/118287\n",
      "Loading image annot 91000/118287\n",
      "Loading image annot 97000/118287\n",
      "Loading image annot 99000/118287\n",
      "Loading image annot 100000/118287\n",
      "Loading image annot 103000/118287\n",
      "Loading image annot 104000/118287\n",
      "Loading image annot 105000/118287\n",
      "Loading image annot 108000/118287\n",
      "Loading image annot 110000/118287\n",
      "Loading image annot 111000/118287\n",
      "Loading image annot 112000/118287\n",
      "Loading image annot 113000/118287\n",
      "Loading image annot 116000/118287\n",
      "Loading image annot 117000/118287\n",
      "\u001b[32m[0705 11:52:45 @argtools.py:146]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Starting a process with 'fork' method is not safe and may consume unnecessary extra CPU memory. Use 'forkserver/spawn' method (available after Py3.4) instead if you run into any issues. See https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods\n",
      "loading annotations into memory...\n",
      "Done (t=0.15s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading image annot 0/5000\n",
      "Loading image annot 2000/5000\n",
      "Loading image annot 4000/5000\n",
      "Training samples: 53044 , Validation samples: 2198\n",
      "Restored from ./tf_ckpts_singlenet/0629_epoch300/ckpt-1764\n",
      "Resumed from epoch 294, step 5304\n",
      "Start processing epoch 295\n",
      "Epoch 295 Step 10/5304 Paf1 532.899048 Paf2 508.957367 Paf3 497.89978 Heatmap 157.512115 Total loss 1697.26831\n",
      "Epoch 295 Step 20/5304 Paf1 629.84021 Paf2 598.472168 Paf3 591.288635 Heatmap 175.701355 Total loss 1995.30237\n",
      "Epoch 295 Step 30/5304 Paf1 515.188171 Paf2 479.003723 Paf3 477.828796 Heatmap 146.422913 Total loss 1618.4436\n",
      "Epoch 295 Step 40/5304 Paf1 487.345062 Paf2 470.458679 Paf3 472.001587 Heatmap 130.55481 Total loss 1560.36011\n",
      "Epoch 295 Step 50/5304 Paf1 652.39624 Paf2 623.013733 Paf3 627.172791 Heatmap 193.809 Total loss 2096.3916\n",
      "Epoch 295 Step 60/5304 Paf1 541.480896 Paf2 482.525635 Paf3 485.280762 Heatmap 161.187683 Total loss 1670.4751\n",
      "Epoch 295 Step 70/5304 Paf1 750.651611 Paf2 704.145874 Paf3 701.472656 Heatmap 193.337646 Total loss 2349.60791\n",
      "Epoch 295 Step 80/5304 Paf1 637.996155 Paf2 572.055481 Paf3 567.168335 Heatmap 216.857346 Total loss 1994.07739\n",
      "Epoch 295 Step 90/5304 Paf1 709.154419 Paf2 654.673828 Paf3 664.541 Heatmap 222.618958 Total loss 2250.98828\n",
      "Epoch 295 Step 100/5304 Paf1 862.683105 Paf2 785.397461 Paf3 803.875 Heatmap 286.099518 Total loss 2738.05518\n",
      "Epoch 295 Step 110/5304 Paf1 561.335938 Paf2 508.160095 Paf3 505.071198 Heatmap 157.330414 Total loss 1731.89771\n",
      "Epoch 295 Step 120/5304 Paf1 644.522095 Paf2 598.069 Paf3 601.269 Heatmap 187.809662 Total loss 2031.66968\n",
      "Epoch 295 Step 130/5304 Paf1 760.516846 Paf2 681.047852 Paf3 681.044922 Heatmap 218.533844 Total loss 2341.14355\n",
      "Epoch 295 Step 140/5304 Paf1 709.595154 Paf2 668.301758 Paf3 661.840332 Heatmap 226.947159 Total loss 2266.68457\n",
      "Epoch 295 Step 150/5304 Paf1 532.660461 Paf2 492.771454 Paf3 482.584137 Heatmap 123.814552 Total loss 1631.83057\n",
      "Epoch 295 Step 160/5304 Paf1 695.804932 Paf2 644.295 Paf3 655.517944 Heatmap 244.505096 Total loss 2240.12305\n",
      "Epoch 295 Step 170/5304 Paf1 540.128479 Paf2 515.686523 Paf3 518.07135 Heatmap 173.074158 Total loss 1746.96045\n",
      "Epoch 295 Step 180/5304 Paf1 552.554138 Paf2 510.0979 Paf3 510.786041 Heatmap 189.042297 Total loss 1762.48047\n",
      "Epoch 295 Step 190/5304 Paf1 664.31781 Paf2 609.74408 Paf3 641.49408 Heatmap 217.794434 Total loss 2133.35034\n",
      "Epoch 295 Step 200/5304 Paf1 644.072327 Paf2 591.01886 Paf3 582.136597 Heatmap 180.618744 Total loss 1997.84656\n",
      "Epoch 295 Step 210/5304 Paf1 529.3927 Paf2 489.559723 Paf3 487.284882 Heatmap 156.367447 Total loss 1662.60474\n",
      "Epoch 295 Step 220/5304 Paf1 563.343262 Paf2 531.015625 Paf3 534.282227 Heatmap 173.456787 Total loss 1802.0979\n",
      "Epoch 295 Step 230/5304 Paf1 713.405823 Paf2 670.323181 Paf3 680.628723 Heatmap 249.267303 Total loss 2313.625\n",
      "Epoch 295 Step 240/5304 Paf1 532.354309 Paf2 482.865082 Paf3 490.173 Heatmap 153.584839 Total loss 1658.97717\n",
      "Epoch 295 Step 250/5304 Paf1 1128.70483 Paf2 1048.73083 Paf3 1041.66455 Heatmap 338.064087 Total loss 3557.16406\n",
      "Epoch 295 Step 260/5304 Paf1 764.635 Paf2 695.703491 Paf3 735.822083 Heatmap 225.26416 Total loss 2421.4248\n",
      "Epoch 295 Step 270/5304 Paf1 715.250061 Paf2 662.914 Paf3 674.345337 Heatmap 241.662552 Total loss 2294.17188\n",
      "Epoch 295 Step 280/5304 Paf1 423.365265 Paf2 389.008606 Paf3 390.692749 Heatmap 114.050903 Total loss 1317.11755\n",
      "Epoch 295 Step 290/5304 Paf1 507.977081 Paf2 478.586121 Paf3 454.271118 Heatmap 133.949707 Total loss 1574.78406\n",
      "Epoch 295 Step 300/5304 Paf1 673.379639 Paf2 628.473 Paf3 624.226257 Heatmap 208.985611 Total loss 2135.06445\n",
      "Epoch 295 Step 310/5304 Paf1 447.584503 Paf2 407.139954 Paf3 401.571869 Heatmap 108.079208 Total loss 1364.37549\n",
      "Epoch 295 Step 320/5304 Paf1 641.753479 Paf2 592.363037 Paf3 587.420471 Heatmap 221.676392 Total loss 2043.21338\n",
      "Epoch 295 Step 330/5304 Paf1 935.706116 Paf2 873.196777 Paf3 877.959229 Heatmap 325.628326 Total loss 3012.49023\n",
      "Epoch 295 Step 340/5304 Paf1 652.08075 Paf2 616.538269 Paf3 634.603 Heatmap 214.384598 Total loss 2117.60669\n",
      "Epoch 295 Step 350/5304 Paf1 619.147766 Paf2 571.933594 Paf3 565.745483 Heatmap 196.791809 Total loss 1953.61865\n",
      "Epoch 295 Step 360/5304 Paf1 572.575134 Paf2 537.531189 Paf3 521.884 Heatmap 151.090637 Total loss 1783.08093\n",
      "Epoch 295 Step 370/5304 Paf1 669.312866 Paf2 636.947876 Paf3 624.002563 Heatmap 213.858887 Total loss 2144.12207\n",
      "Epoch 295 Step 380/5304 Paf1 692.479614 Paf2 667.026245 Paf3 649.073547 Heatmap 199.523239 Total loss 2208.10254\n",
      "Epoch 295 Step 390/5304 Paf1 523.536438 Paf2 491.923401 Paf3 499.59375 Heatmap 162.496857 Total loss 1677.55042\n",
      "Epoch 295 Step 400/5304 Paf1 579.64978 Paf2 529.405273 Paf3 547.576416 Heatmap 186.756256 Total loss 1843.3877\n",
      "Epoch 295 Step 410/5304 Paf1 584.637695 Paf2 553.948303 Paf3 526.610718 Heatmap 157.107834 Total loss 1822.30444\n",
      "Epoch 295 Step 420/5304 Paf1 593.559692 Paf2 561.776428 Paf3 557.83667 Heatmap 188.23288 Total loss 1901.40576\n",
      "Epoch 295 Step 430/5304 Paf1 952.283569 Paf2 904.511536 Paf3 914.164673 Heatmap 302.781311 Total loss 3073.74121\n",
      "Epoch 295 Step 440/5304 Paf1 820.053467 Paf2 784.692383 Paf3 779.032837 Heatmap 242.731018 Total loss 2626.50977\n",
      "Epoch 295 Step 450/5304 Paf1 719.168152 Paf2 672.67926 Paf3 678.366699 Heatmap 211.266083 Total loss 2281.48022\n",
      "Epoch 295 Step 460/5304 Paf1 827.880798 Paf2 780.408691 Paf3 794.495239 Heatmap 267.313232 Total loss 2670.09814\n",
      "Epoch 295 Step 470/5304 Paf1 817.616638 Paf2 791.286072 Paf3 782.958862 Heatmap 288.064087 Total loss 2679.92578\n",
      "Epoch 295 Step 480/5304 Paf1 802.450134 Paf2 747.987671 Paf3 754.774597 Heatmap 249.078156 Total loss 2554.29053\n",
      "Epoch 295 Step 490/5304 Paf1 681.603333 Paf2 658.149597 Paf3 661.39093 Heatmap 212.654419 Total loss 2213.79834\n",
      "Epoch 295 Step 500/5304 Paf1 780.491394 Paf2 716.574768 Paf3 718.69751 Heatmap 215.189819 Total loss 2430.95361\n",
      "Epoch 295 Step 510/5304 Paf1 488.024109 Paf2 450.440399 Paf3 437.660065 Heatmap 124.549644 Total loss 1500.67419\n",
      "Epoch 295 Step 520/5304 Paf1 837.859802 Paf2 794.853943 Paf3 789.005859 Heatmap 260.982117 Total loss 2682.70166\n",
      "Epoch 295 Step 530/5304 Paf1 928.298889 Paf2 840.945068 Paf3 842.550659 Heatmap 253.199341 Total loss 2864.9939\n",
      "Epoch 295 Step 540/5304 Paf1 814.087 Paf2 766.26239 Paf3 762.843201 Heatmap 249.537277 Total loss 2592.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 295 Step 550/5304 Paf1 446.759155 Paf2 423.652954 Paf3 409.639832 Heatmap 121.427948 Total loss 1401.47986\n",
      "Epoch 295 Step 560/5304 Paf1 925.836731 Paf2 883.80011 Paf3 885.547302 Heatmap 270.64151 Total loss 2965.82568\n",
      "Epoch 295 Step 570/5304 Paf1 921.525146 Paf2 883.07196 Paf3 858.827881 Heatmap 248.493332 Total loss 2911.91846\n",
      "Epoch 295 Step 580/5304 Paf1 657.262878 Paf2 614.529297 Paf3 606.675171 Heatmap 214.955688 Total loss 2093.4231\n",
      "Epoch 295 Step 590/5304 Paf1 548.160645 Paf2 490.531616 Paf3 500.524719 Heatmap 142.324768 Total loss 1681.54175\n",
      "Epoch 295 Step 600/5304 Paf1 785.998291 Paf2 736.568 Paf3 730.578552 Heatmap 270.496155 Total loss 2523.64111\n",
      "Epoch 295 Step 610/5304 Paf1 556.391968 Paf2 533.737366 Paf3 524.51123 Heatmap 161.930298 Total loss 1776.57092\n",
      "Epoch 295 Step 620/5304 Paf1 427.12381 Paf2 378.607269 Paf3 386.979279 Heatmap 134.5495 Total loss 1327.25989\n",
      "Epoch 295 Step 630/5304 Paf1 708.95575 Paf2 651.543823 Paf3 658.207703 Heatmap 190.674042 Total loss 2209.38135\n",
      "Epoch 295 Step 640/5304 Paf1 751.745667 Paf2 714.891296 Paf3 721.768494 Heatmap 235.844269 Total loss 2424.24976\n",
      "Epoch 295 Step 650/5304 Paf1 513.105042 Paf2 472.27594 Paf3 484.51947 Heatmap 156.891571 Total loss 1626.79199\n",
      "Epoch 295 Step 660/5304 Paf1 516.060608 Paf2 466.948944 Paf3 479.653351 Heatmap 147.651505 Total loss 1610.31445\n",
      "Epoch 295 Step 670/5304 Paf1 486.827667 Paf2 467.284271 Paf3 464.0578 Heatmap 143.597565 Total loss 1561.76733\n",
      "Epoch 295 Step 680/5304 Paf1 668.720276 Paf2 604.087341 Paf3 589.252625 Heatmap 190.89389 Total loss 2052.9541\n",
      "Epoch 295 Step 690/5304 Paf1 576.488831 Paf2 528.144348 Paf3 533.493774 Heatmap 159.614532 Total loss 1797.74146\n",
      "Epoch 295 Step 700/5304 Paf1 502.477875 Paf2 478.036652 Paf3 461.639709 Heatmap 164.913391 Total loss 1607.06763\n",
      "Epoch 295 Step 710/5304 Paf1 515.13269 Paf2 502.88324 Paf3 489.07193 Heatmap 153.70076 Total loss 1660.78857\n",
      "Epoch 295 Step 720/5304 Paf1 701.081421 Paf2 637.817505 Paf3 649.560608 Heatmap 184.902466 Total loss 2173.36206\n",
      "Epoch 295 Step 730/5304 Paf1 458.309906 Paf2 451.679077 Paf3 437.855347 Heatmap 132.165756 Total loss 1480.01013\n",
      "Epoch 295 Step 740/5304 Paf1 922.826 Paf2 868.221619 Paf3 866.389832 Heatmap 294.890411 Total loss 2952.32788\n",
      "Epoch 295 Step 750/5304 Paf1 606.441101 Paf2 546.542786 Paf3 549.903137 Heatmap 150.077972 Total loss 1852.96497\n",
      "Epoch 295 Step 760/5304 Paf1 686.815552 Paf2 656.01239 Paf3 646.679504 Heatmap 197.721802 Total loss 2187.22925\n",
      "Epoch 295 Step 770/5304 Paf1 682.496643 Paf2 616.482422 Paf3 622.457886 Heatmap 182.716156 Total loss 2104.15308\n",
      "Epoch 295 Step 780/5304 Paf1 577.582092 Paf2 560.437256 Paf3 534.848877 Heatmap 168.761383 Total loss 1841.62952\n",
      "Epoch 295 Step 790/5304 Paf1 657.906433 Paf2 615.363647 Paf3 618.886719 Heatmap 215.737549 Total loss 2107.89429\n",
      "Epoch 295 Step 800/5304 Paf1 705.708069 Paf2 640.169556 Paf3 641.444214 Heatmap 212.697983 Total loss 2200.02\n",
      "Epoch 295 Step 810/5304 Paf1 518.180054 Paf2 494.271484 Paf3 500.528107 Heatmap 148.991028 Total loss 1661.9707\n",
      "Epoch 295 Step 820/5304 Paf1 784.122314 Paf2 727.19812 Paf3 740.496826 Heatmap 219.04451 Total loss 2470.86182\n",
      "Epoch 295 Step 830/5304 Paf1 554.856323 Paf2 516.952 Paf3 514.735352 Heatmap 159.623291 Total loss 1746.16699\n",
      "Epoch 295 Step 840/5304 Paf1 907.847778 Paf2 859.001831 Paf3 847.932434 Heatmap 365.781982 Total loss 2980.56396\n",
      "Epoch 295 Step 850/5304 Paf1 530.387512 Paf2 512.524231 Paf3 513.313721 Heatmap 144.493286 Total loss 1700.71875\n",
      "Epoch 295 Step 860/5304 Paf1 692.945801 Paf2 626.231628 Paf3 629.629822 Heatmap 203.585724 Total loss 2152.39307\n",
      "Epoch 295 Step 870/5304 Paf1 740.941895 Paf2 687.725 Paf3 683.244873 Heatmap 230.383606 Total loss 2342.29541\n",
      "Epoch 295 Step 880/5304 Paf1 764.549744 Paf2 731.565308 Paf3 711.018555 Heatmap 220.46051 Total loss 2427.59399\n",
      "Epoch 295 Step 890/5304 Paf1 854.885071 Paf2 768.171936 Paf3 778.305542 Heatmap 216.702545 Total loss 2618.06494\n",
      "Epoch 295 Step 900/5304 Paf1 710.876953 Paf2 657.684631 Paf3 669.316711 Heatmap 205.373734 Total loss 2243.25195\n",
      "Epoch 295 Step 910/5304 Paf1 640.594 Paf2 605.075562 Paf3 602.361938 Heatmap 189.440155 Total loss 2037.47168\n",
      "Epoch 295 Step 920/5304 Paf1 1011.05963 Paf2 958.326355 Paf3 960.124756 Heatmap 274.613647 Total loss 3204.12451\n",
      "Epoch 295 Step 930/5304 Paf1 597.711914 Paf2 563.3125 Paf3 546.15332 Heatmap 192.154526 Total loss 1899.33228\n",
      "Epoch 295 Step 940/5304 Paf1 1091.55383 Paf2 1015.75012 Paf3 1004.54675 Heatmap 302.924194 Total loss 3414.7749\n",
      "Epoch 295 Step 950/5304 Paf1 601.369 Paf2 562.701599 Paf3 555.041 Heatmap 152.53891 Total loss 1871.65051\n",
      "Epoch 295 Step 960/5304 Paf1 663.342285 Paf2 634.124939 Paf3 623.114 Heatmap 188.836533 Total loss 2109.41797\n",
      "Epoch 295 Step 970/5304 Paf1 1058.75928 Paf2 994.748657 Paf3 1006.67603 Heatmap 361.678955 Total loss 3421.86279\n",
      "Epoch 295 Step 980/5304 Paf1 523.79187 Paf2 503.960388 Paf3 493.265137 Heatmap 166.971741 Total loss 1687.98901\n",
      "Epoch 295 Step 990/5304 Paf1 609.566528 Paf2 572.028564 Paf3 579.837 Heatmap 157.834442 Total loss 1919.26648\n",
      "Epoch 295 Step 1000/5304 Paf1 769.285706 Paf2 717.907166 Paf3 701.639648 Heatmap 212.705048 Total loss 2401.5376\n",
      "Saved checkpoint for step 1000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1765\n",
      "Epoch 295 Step 1010/5304 Paf1 580.018555 Paf2 516.221069 Paf3 516.586121 Heatmap 167.252625 Total loss 1780.07837\n",
      "Epoch 295 Step 1020/5304 Paf1 791.187866 Paf2 757.535828 Paf3 730.804321 Heatmap 293.374359 Total loss 2572.90234\n",
      "Epoch 295 Step 1030/5304 Paf1 767.432251 Paf2 744.25293 Paf3 737.525085 Heatmap 249.754349 Total loss 2498.9646\n",
      "Epoch 295 Step 1040/5304 Paf1 610.944092 Paf2 587.52 Paf3 575.581055 Heatmap 219.608704 Total loss 1993.65381\n",
      "Epoch 295 Step 1050/5304 Paf1 870.069702 Paf2 821.920898 Paf3 817.659546 Heatmap 340.827942 Total loss 2850.47803\n",
      "Epoch 295 Step 1060/5304 Paf1 902.885925 Paf2 822.827637 Paf3 820.213562 Heatmap 272.373169 Total loss 2818.30029\n",
      "Epoch 295 Step 1070/5304 Paf1 783.996582 Paf2 778.736511 Paf3 762.792664 Heatmap 225.135376 Total loss 2550.66113\n",
      "Epoch 295 Step 1080/5304 Paf1 495.597015 Paf2 468.125946 Paf3 471.208405 Heatmap 148.232101 Total loss 1583.16345\n",
      "Epoch 295 Step 1090/5304 Paf1 603.568115 Paf2 552.948486 Paf3 559.642334 Heatmap 204.389893 Total loss 1920.54883\n",
      "Epoch 295 Step 1100/5304 Paf1 571.65 Paf2 527.239136 Paf3 538.216248 Heatmap 164.520325 Total loss 1801.62573\n",
      "Epoch 295 Step 1110/5304 Paf1 587.026611 Paf2 529.660583 Paf3 514.37323 Heatmap 188.685074 Total loss 1819.74561\n",
      "Epoch 295 Step 1120/5304 Paf1 713.062195 Paf2 680.550842 Paf3 684.629883 Heatmap 240.075043 Total loss 2318.31787\n",
      "Epoch 295 Step 1130/5304 Paf1 731.653076 Paf2 664.299 Paf3 676.312683 Heatmap 224.661407 Total loss 2296.92627\n",
      "Epoch 295 Step 1140/5304 Paf1 624.155762 Paf2 597.459 Paf3 586.777 Heatmap 193.697067 Total loss 2002.08887\n",
      "Epoch 295 Step 1150/5304 Paf1 730.100464 Paf2 674.090637 Paf3 679.815918 Heatmap 228.302063 Total loss 2312.30908\n",
      "Epoch 295 Step 1160/5304 Paf1 548.265442 Paf2 513.3302 Paf3 532.900818 Heatmap 167.460983 Total loss 1761.95752\n",
      "Epoch 295 Step 1170/5304 Paf1 601.698181 Paf2 554.389709 Paf3 542.470154 Heatmap 175.920044 Total loss 1874.47803\n",
      "Epoch 295 Step 1180/5304 Paf1 671.352051 Paf2 608.752197 Paf3 621.208496 Heatmap 189.615479 Total loss 2090.92822\n",
      "Epoch 295 Step 1190/5304 Paf1 780.623657 Paf2 695.17218 Paf3 731.908813 Heatmap 228.471283 Total loss 2436.17603\n",
      "Epoch 295 Step 1200/5304 Paf1 809.95874 Paf2 766.020752 Paf3 755.036804 Heatmap 248.754181 Total loss 2579.77051\n",
      "Epoch 295 Step 1210/5304 Paf1 576.874878 Paf2 518.939941 Paf3 525.40271 Heatmap 198.192505 Total loss 1819.41\n",
      "Epoch 295 Step 1220/5304 Paf1 714.168762 Paf2 673.589294 Paf3 666.589722 Heatmap 215.147858 Total loss 2269.49561\n",
      "Epoch 295 Step 1230/5304 Paf1 662.407593 Paf2 649.691345 Paf3 644.240784 Heatmap 173.746017 Total loss 2130.08569\n",
      "Epoch 295 Step 1240/5304 Paf1 946.373352 Paf2 887.058533 Paf3 901.820923 Heatmap 284.211945 Total loss 3019.46484\n",
      "Epoch 295 Step 1250/5304 Paf1 546.148193 Paf2 510.914032 Paf3 491.694336 Heatmap 136.460159 Total loss 1685.2168\n",
      "Epoch 295 Step 1260/5304 Paf1 664.144653 Paf2 632.885254 Paf3 634.505859 Heatmap 186.76947 Total loss 2118.30518\n",
      "Epoch 295 Step 1270/5304 Paf1 745.523804 Paf2 723.335 Paf3 721.534546 Heatmap 278.011292 Total loss 2468.40479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 295 Step 1280/5304 Paf1 627.852966 Paf2 568.160889 Paf3 580.300903 Heatmap 191.72908 Total loss 1968.04395\n",
      "Epoch 295 Step 1290/5304 Paf1 417.925415 Paf2 384.723846 Paf3 392.700104 Heatmap 131.663391 Total loss 1327.01282\n",
      "Epoch 295 Step 1300/5304 Paf1 695.310913 Paf2 654.325623 Paf3 656.633179 Heatmap 244.514465 Total loss 2250.78418\n",
      "Epoch 295 Step 1310/5304 Paf1 800.478394 Paf2 750.641174 Paf3 755.150391 Heatmap 233.60466 Total loss 2539.87476\n",
      "Epoch 295 Step 1320/5304 Paf1 675.138184 Paf2 630.806458 Paf3 642.573181 Heatmap 217.0224 Total loss 2165.54\n",
      "Epoch 295 Step 1330/5304 Paf1 539.206299 Paf2 503.906891 Paf3 509.716461 Heatmap 147.831177 Total loss 1700.66077\n",
      "Epoch 295 Step 1340/5304 Paf1 1008.76123 Paf2 958.660034 Paf3 960.474731 Heatmap 344.365906 Total loss 3272.26172\n",
      "Epoch 295 Step 1350/5304 Paf1 813.461853 Paf2 771.471252 Paf3 767.496887 Heatmap 261.676941 Total loss 2614.10693\n",
      "Epoch 295 Step 1360/5304 Paf1 606.033142 Paf2 574.422058 Paf3 561.388123 Heatmap 181.484818 Total loss 1923.32812\n",
      "Epoch 295 Step 1370/5304 Paf1 593.2 Paf2 537.372498 Paf3 545.36554 Heatmap 195.763092 Total loss 1871.70117\n",
      "Epoch 295 Step 1380/5304 Paf1 1035.49524 Paf2 992.361389 Paf3 978.507568 Heatmap 320.864441 Total loss 3327.22876\n",
      "Epoch 295 Step 1390/5304 Paf1 516.485352 Paf2 492.766663 Paf3 494.313171 Heatmap 138.99765 Total loss 1642.56274\n",
      "Epoch 295 Step 1400/5304 Paf1 562.288086 Paf2 585.476562 Paf3 566.994507 Heatmap 199.54483 Total loss 1914.30396\n",
      "Epoch 295 Step 1410/5304 Paf1 730.431946 Paf2 679.808472 Paf3 689.571045 Heatmap 237.18399 Total loss 2336.99561\n",
      "Epoch 295 Step 1420/5304 Paf1 534.821411 Paf2 487.344421 Paf3 506.392212 Heatmap 141.455902 Total loss 1670.01392\n",
      "Epoch 295 Step 1430/5304 Paf1 580.24292 Paf2 537.085327 Paf3 530.419678 Heatmap 140.814789 Total loss 1788.56274\n",
      "Epoch 295 Step 1440/5304 Paf1 714.194397 Paf2 682.228577 Paf3 677.020264 Heatmap 241.142761 Total loss 2314.58594\n",
      "Epoch 295 Step 1450/5304 Paf1 752.284363 Paf2 725.819885 Paf3 731.724426 Heatmap 273.005554 Total loss 2482.83423\n",
      "Epoch 295 Step 1460/5304 Paf1 929.645 Paf2 888.835144 Paf3 883.73761 Heatmap 302.498779 Total loss 3004.71655\n",
      "Epoch 295 Step 1470/5304 Paf1 645.163513 Paf2 590.236877 Paf3 606.33075 Heatmap 174.184052 Total loss 2015.91516\n",
      "Epoch 295 Step 1480/5304 Paf1 666.961304 Paf2 630.630188 Paf3 596.546448 Heatmap 192.653946 Total loss 2086.79199\n",
      "Epoch 295 Step 1490/5304 Paf1 843.565186 Paf2 790.595 Paf3 767.701599 Heatmap 230.530563 Total loss 2632.39233\n",
      "Epoch 295 Step 1500/5304 Paf1 651.39917 Paf2 609.082336 Paf3 618.412415 Heatmap 190.038483 Total loss 2068.93237\n",
      "Epoch 295 Step 1510/5304 Paf1 680.893616 Paf2 630.768311 Paf3 640.859497 Heatmap 217.269623 Total loss 2169.79102\n",
      "Epoch 295 Step 1520/5304 Paf1 483.130066 Paf2 448.944763 Paf3 430.093597 Heatmap 148.109253 Total loss 1510.27771\n",
      "Epoch 295 Step 1530/5304 Paf1 628.240662 Paf2 555.384155 Paf3 577.725342 Heatmap 185.322342 Total loss 1946.67236\n",
      "Epoch 295 Step 1540/5304 Paf1 503.833923 Paf2 473.053314 Paf3 484.340118 Heatmap 196.913116 Total loss 1658.14038\n",
      "Epoch 295 Step 1550/5304 Paf1 582.535095 Paf2 563.052063 Paf3 546.087158 Heatmap 172.139587 Total loss 1863.81396\n",
      "Epoch 295 Step 1560/5304 Paf1 819.074646 Paf2 777.725098 Paf3 774.696594 Heatmap 265.59375 Total loss 2637.09\n",
      "Epoch 295 Step 1570/5304 Paf1 814.190063 Paf2 765.849365 Paf3 751.032471 Heatmap 253.732361 Total loss 2584.8042\n",
      "Epoch 295 Step 1580/5304 Paf1 579.009155 Paf2 530.467896 Paf3 521.430664 Heatmap 179.448212 Total loss 1810.35596\n",
      "Epoch 295 Step 1590/5304 Paf1 784.037842 Paf2 737.166748 Paf3 713.897827 Heatmap 225.264771 Total loss 2460.36719\n",
      "Epoch 295 Step 1600/5304 Paf1 965.81842 Paf2 930.809692 Paf3 923.856567 Heatmap 330.331116 Total loss 3150.81592\n",
      "Epoch 295 Step 1610/5304 Paf1 766.321899 Paf2 733.336426 Paf3 729.199829 Heatmap 256.518127 Total loss 2485.37622\n",
      "Epoch 295 Step 1620/5304 Paf1 881.215088 Paf2 834.251831 Paf3 827.35437 Heatmap 271.808319 Total loss 2814.62964\n",
      "Epoch 295 Step 1630/5304 Paf1 632.908875 Paf2 615.49762 Paf3 629.609619 Heatmap 185.94516 Total loss 2063.96143\n",
      "Epoch 295 Step 1640/5304 Paf1 590.652954 Paf2 561.551514 Paf3 553.681335 Heatmap 185.259613 Total loss 1891.14539\n",
      "Epoch 295 Step 1650/5304 Paf1 744.406372 Paf2 690.886597 Paf3 688.290039 Heatmap 210.218918 Total loss 2333.802\n",
      "Epoch 295 Step 1660/5304 Paf1 578.229309 Paf2 536.091675 Paf3 527.511414 Heatmap 162.187592 Total loss 1804.02\n",
      "Epoch 295 Step 1670/5304 Paf1 718.509277 Paf2 694.513 Paf3 692.342773 Heatmap 247.103638 Total loss 2352.46875\n",
      "Epoch 295 Step 1680/5304 Paf1 560.246582 Paf2 525.250488 Paf3 515.826477 Heatmap 146.36322 Total loss 1747.68677\n",
      "Epoch 295 Step 1690/5304 Paf1 602.94458 Paf2 592.355408 Paf3 570.608887 Heatmap 185.86058 Total loss 1951.76953\n",
      "Epoch 295 Step 1700/5304 Paf1 937.089417 Paf2 870.995605 Paf3 867.307861 Heatmap 306.03479 Total loss 2981.42773\n",
      "Epoch 295 Step 1710/5304 Paf1 478.379669 Paf2 434.047577 Paf3 445.156647 Heatmap 147.244308 Total loss 1504.82812\n",
      "Epoch 295 Step 1720/5304 Paf1 576.808777 Paf2 526.926819 Paf3 518.375183 Heatmap 157.83075 Total loss 1779.94153\n",
      "Epoch 295 Step 1730/5304 Paf1 627.818054 Paf2 598.947876 Paf3 592.111755 Heatmap 191.692719 Total loss 2010.57031\n",
      "Epoch 295 Step 1740/5304 Paf1 595.236328 Paf2 551.753906 Paf3 564.133 Heatmap 167.768616 Total loss 1878.89185\n",
      "Epoch 295 Step 1750/5304 Paf1 1157.11902 Paf2 1078.36743 Paf3 1099.38879 Heatmap 366.387024 Total loss 3701.26221\n",
      "Epoch 295 Step 1760/5304 Paf1 632.979919 Paf2 600.880432 Paf3 601.586548 Heatmap 192.419647 Total loss 2027.86658\n",
      "Epoch 295 Step 1770/5304 Paf1 757.233093 Paf2 725.02655 Paf3 721.885681 Heatmap 240.694382 Total loss 2444.83984\n",
      "Epoch 295 Step 1780/5304 Paf1 565.172241 Paf2 531.262878 Paf3 537.163696 Heatmap 169.634979 Total loss 1803.23376\n",
      "Epoch 295 Step 1790/5304 Paf1 593.975098 Paf2 557.488037 Paf3 550.647644 Heatmap 148.550095 Total loss 1850.66089\n",
      "Epoch 295 Step 1800/5304 Paf1 674.559204 Paf2 639.943604 Paf3 628.428833 Heatmap 235.821136 Total loss 2178.75293\n",
      "Epoch 295 Step 1810/5304 Paf1 535.141846 Paf2 496.735596 Paf3 500.583923 Heatmap 192.140289 Total loss 1724.60168\n",
      "Epoch 295 Step 1820/5304 Paf1 648.046265 Paf2 605.026489 Paf3 612.61377 Heatmap 215.513794 Total loss 2081.2002\n",
      "Epoch 295 Step 1830/5304 Paf1 849.790344 Paf2 796.717 Paf3 787.528137 Heatmap 244.22908 Total loss 2678.26465\n",
      "Epoch 295 Step 1840/5304 Paf1 559.938171 Paf2 496.776093 Paf3 515.423401 Heatmap 160.008026 Total loss 1732.14563\n",
      "Epoch 295 Step 1850/5304 Paf1 870.811584 Paf2 856.035095 Paf3 847.084839 Heatmap 297.878418 Total loss 2871.81\n",
      "Epoch 295 Step 1860/5304 Paf1 471.994415 Paf2 448.704529 Paf3 449.434326 Heatmap 169.724838 Total loss 1539.85815\n",
      "Epoch 295 Step 1870/5304 Paf1 418.064423 Paf2 382.618195 Paf3 370.09967 Heatmap 138.699524 Total loss 1309.48181\n",
      "Epoch 295 Step 1880/5304 Paf1 726.467224 Paf2 696.327942 Paf3 676.312 Heatmap 252.559448 Total loss 2351.6665\n",
      "Epoch 295 Step 1890/5304 Paf1 516.849548 Paf2 497.95462 Paf3 504.690979 Heatmap 144.411301 Total loss 1663.90649\n",
      "Epoch 295 Step 1900/5304 Paf1 589.701965 Paf2 542.426 Paf3 571.923889 Heatmap 208.151855 Total loss 1912.20361\n",
      "Epoch 295 Step 1910/5304 Paf1 444.845 Paf2 436.869202 Paf3 419.506073 Heatmap 134.581116 Total loss 1435.80139\n",
      "Epoch 295 Step 1920/5304 Paf1 462.492249 Paf2 435.590576 Paf3 434.051178 Heatmap 141.75769 Total loss 1473.8916\n",
      "Epoch 295 Step 1930/5304 Paf1 828.784668 Paf2 812.915344 Paf3 797.427063 Heatmap 324.467773 Total loss 2763.59473\n",
      "Epoch 295 Step 1940/5304 Paf1 576.468 Paf2 542.740295 Paf3 533.608582 Heatmap 170.850983 Total loss 1823.66785\n",
      "Epoch 295 Step 1950/5304 Paf1 568.154602 Paf2 528.345764 Paf3 542.365051 Heatmap 180.435791 Total loss 1819.30127\n",
      "Epoch 295 Step 1960/5304 Paf1 541.473816 Paf2 502.610046 Paf3 495.709839 Heatmap 192.391876 Total loss 1732.18555\n",
      "Epoch 295 Step 1970/5304 Paf1 724.970398 Paf2 686.078491 Paf3 689.073303 Heatmap 229.361328 Total loss 2329.4834\n",
      "Epoch 295 Step 1980/5304 Paf1 709.691223 Paf2 643.291931 Paf3 660.17218 Heatmap 216.036987 Total loss 2229.19238\n",
      "Epoch 295 Step 1990/5304 Paf1 584.229065 Paf2 567.60791 Paf3 564.430542 Heatmap 178.421616 Total loss 1894.68909\n",
      "Epoch 295 Step 2000/5304 Paf1 848.308899 Paf2 813.226135 Paf3 810.317078 Heatmap 310.623383 Total loss 2782.47559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint for step 2000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1766\n",
      "Epoch 295 Step 2010/5304 Paf1 791.403625 Paf2 766.485901 Paf3 759.455933 Heatmap 219.10202 Total loss 2536.44751\n",
      "Epoch 295 Step 2020/5304 Paf1 707.369385 Paf2 649.283813 Paf3 638.043152 Heatmap 204.66745 Total loss 2199.36377\n",
      "Epoch 295 Step 2030/5304 Paf1 761.681824 Paf2 722.978638 Paf3 712.382568 Heatmap 192.095184 Total loss 2389.13818\n",
      "Epoch 295 Step 2040/5304 Paf1 467.562286 Paf2 414.197815 Paf3 414.031342 Heatmap 120.751526 Total loss 1416.54297\n",
      "Epoch 295 Step 2050/5304 Paf1 450.952728 Paf2 430.147339 Paf3 433.926788 Heatmap 190.439911 Total loss 1505.4668\n",
      "Epoch 295 Step 2060/5304 Paf1 862.188904 Paf2 804.651611 Paf3 829.193665 Heatmap 235.760468 Total loss 2731.79468\n",
      "Epoch 295 Step 2070/5304 Paf1 772.975342 Paf2 703.136353 Paf3 711.305481 Heatmap 188.363434 Total loss 2375.78076\n",
      "Epoch 295 Step 2080/5304 Paf1 605.318726 Paf2 565.522461 Paf3 570.231812 Heatmap 197.195892 Total loss 1938.26892\n",
      "Epoch 295 Step 2090/5304 Paf1 737.336792 Paf2 687.615173 Paf3 687.712036 Heatmap 178.890503 Total loss 2291.55444\n",
      "Epoch 295 Step 2100/5304 Paf1 728.625244 Paf2 688.939575 Paf3 696.206848 Heatmap 225.905716 Total loss 2339.67725\n",
      "Epoch 295 Step 2110/5304 Paf1 636.205627 Paf2 605.737793 Paf3 606.850464 Heatmap 217.147 Total loss 2065.94092\n",
      "Epoch 295 Step 2120/5304 Paf1 469.562408 Paf2 455.754028 Paf3 464.796509 Heatmap 141.250687 Total loss 1531.36353\n",
      "Epoch 295 Step 2130/5304 Paf1 683.537537 Paf2 628.903381 Paf3 649.824585 Heatmap 209.64386 Total loss 2171.90942\n",
      "Epoch 295 Step 2140/5304 Paf1 592.065247 Paf2 554.542908 Paf3 562.471069 Heatmap 164.606323 Total loss 1873.68555\n",
      "Epoch 295 Step 2150/5304 Paf1 707.586304 Paf2 669.59668 Paf3 672.418945 Heatmap 208.130371 Total loss 2257.73242\n",
      "Epoch 295 Step 2160/5304 Paf1 696.868286 Paf2 651.73114 Paf3 658.088074 Heatmap 212.952042 Total loss 2219.6394\n",
      "Epoch 295 Step 2170/5304 Paf1 883.503418 Paf2 815.439575 Paf3 784.183594 Heatmap 251.876709 Total loss 2735.00342\n",
      "Epoch 295 Step 2180/5304 Paf1 763.808472 Paf2 683.761169 Paf3 684.661 Heatmap 253.305588 Total loss 2385.53613\n",
      "Epoch 295 Step 2190/5304 Paf1 649.398315 Paf2 618.830322 Paf3 619.178894 Heatmap 187.915649 Total loss 2075.32324\n",
      "Epoch 295 Step 2200/5304 Paf1 881.045349 Paf2 805.001 Paf3 791.031372 Heatmap 251.022522 Total loss 2728.10034\n",
      "Epoch 295 Step 2210/5304 Paf1 481.073 Paf2 440.894897 Paf3 430.558868 Heatmap 133.446121 Total loss 1485.9729\n",
      "Epoch 295 Step 2220/5304 Paf1 505.747314 Paf2 468.205444 Paf3 453.597809 Heatmap 149.758362 Total loss 1577.30896\n",
      "Epoch 295 Step 2230/5304 Paf1 810.038 Paf2 779.268494 Paf3 763.124146 Heatmap 236.470398 Total loss 2588.90112\n",
      "Epoch 295 Step 2240/5304 Paf1 555.864 Paf2 548.215088 Paf3 543.921265 Heatmap 146.971893 Total loss 1794.97229\n",
      "Epoch 295 Step 2250/5304 Paf1 712.185913 Paf2 651.019775 Paf3 665.078247 Heatmap 202.978424 Total loss 2231.26221\n",
      "Epoch 295 Step 2260/5304 Paf1 764.905884 Paf2 729.749451 Paf3 737.293274 Heatmap 246.003265 Total loss 2477.95166\n",
      "Epoch 295 Step 2270/5304 Paf1 789.722961 Paf2 759.78717 Paf3 750.078125 Heatmap 208.678284 Total loss 2508.2666\n",
      "Epoch 295 Step 2280/5304 Paf1 647.238708 Paf2 601.584 Paf3 622.297363 Heatmap 214.450989 Total loss 2085.57104\n",
      "Epoch 295 Step 2290/5304 Paf1 457.615601 Paf2 451.155 Paf3 429.448425 Heatmap 124.54715 Total loss 1462.76624\n",
      "Epoch 295 Step 2300/5304 Paf1 889.457153 Paf2 838.570801 Paf3 836.596191 Heatmap 298.541565 Total loss 2863.16553\n",
      "Epoch 295 Step 2310/5304 Paf1 371.059448 Paf2 344.937866 Paf3 343.342743 Heatmap 101.641342 Total loss 1160.98145\n",
      "Epoch 295 Step 2320/5304 Paf1 641.342529 Paf2 614.654846 Paf3 596.644653 Heatmap 200.849869 Total loss 2053.4917\n",
      "Epoch 295 Step 2330/5304 Paf1 619.195862 Paf2 579.649658 Paf3 585.910583 Heatmap 197.279205 Total loss 1982.03528\n",
      "Epoch 295 Step 2340/5304 Paf1 927.131104 Paf2 902.535156 Paf3 905.736572 Heatmap 319.139771 Total loss 3054.54248\n",
      "Epoch 295 Step 2350/5304 Paf1 588.328735 Paf2 528.730713 Paf3 554.933 Heatmap 156.50058 Total loss 1828.49304\n",
      "Epoch 295 Step 2360/5304 Paf1 785.141846 Paf2 725.745911 Paf3 720.383911 Heatmap 241.715668 Total loss 2472.9873\n",
      "Epoch 295 Step 2370/5304 Paf1 553.308472 Paf2 517.366882 Paf3 522.270447 Heatmap 153.335281 Total loss 1746.28101\n",
      "Epoch 295 Step 2380/5304 Paf1 622.933167 Paf2 579.693726 Paf3 569.686096 Heatmap 163.265427 Total loss 1935.57849\n",
      "Epoch 295 Step 2390/5304 Paf1 642.50885 Paf2 610.973816 Paf3 640.082947 Heatmap 211.238266 Total loss 2104.80396\n",
      "Epoch 295 Step 2400/5304 Paf1 511.601318 Paf2 489.53717 Paf3 471.847656 Heatmap 143.490112 Total loss 1616.47632\n",
      "Epoch 295 Step 2410/5304 Paf1 421.603882 Paf2 392.578705 Paf3 395.07077 Heatmap 134.555893 Total loss 1343.80933\n",
      "Epoch 295 Step 2420/5304 Paf1 730.040833 Paf2 705.372314 Paf3 702.261414 Heatmap 206.804138 Total loss 2344.47852\n",
      "Epoch 295 Step 2430/5304 Paf1 684.520203 Paf2 633.671143 Paf3 627.942383 Heatmap 194.765533 Total loss 2140.89941\n",
      "Epoch 295 Step 2440/5304 Paf1 446.724426 Paf2 408.012329 Paf3 406.023804 Heatmap 140.778778 Total loss 1401.53931\n",
      "Epoch 295 Step 2450/5304 Paf1 660.491211 Paf2 612.930481 Paf3 589.149475 Heatmap 204.777283 Total loss 2067.34839\n",
      "Epoch 295 Step 2460/5304 Paf1 798.331482 Paf2 766.133 Paf3 770.556396 Heatmap 288.324921 Total loss 2623.3457\n",
      "Epoch 295 Step 2470/5304 Paf1 513.663147 Paf2 493.735046 Paf3 472.826 Heatmap 159.98642 Total loss 1640.21057\n",
      "Epoch 295 Step 2480/5304 Paf1 799.371887 Paf2 765.386902 Paf3 741.469604 Heatmap 236.511307 Total loss 2542.73975\n",
      "Epoch 295 Step 2490/5304 Paf1 624.748962 Paf2 560.870605 Paf3 561.881775 Heatmap 164.589249 Total loss 1912.09058\n",
      "Epoch 295 Step 2500/5304 Paf1 792.270081 Paf2 753.731812 Paf3 737.721924 Heatmap 230.040863 Total loss 2513.76465\n",
      "Epoch 295 Step 2510/5304 Paf1 604.46405 Paf2 580.008362 Paf3 575.383057 Heatmap 178.454971 Total loss 1938.31042\n",
      "Epoch 295 Step 2520/5304 Paf1 559.809 Paf2 516.34082 Paf3 522.714417 Heatmap 150.944122 Total loss 1749.80847\n",
      "Epoch 295 Step 2530/5304 Paf1 883.65332 Paf2 843.490295 Paf3 859.151306 Heatmap 335.430664 Total loss 2921.72559\n",
      "Epoch 295 Step 2540/5304 Paf1 765.27417 Paf2 740.661377 Paf3 717.154358 Heatmap 230.950989 Total loss 2454.04102\n",
      "Epoch 295 Step 2550/5304 Paf1 574.540588 Paf2 549.023682 Paf3 538.088 Heatmap 156.616394 Total loss 1818.26855\n",
      "Epoch 295 Step 2560/5304 Paf1 599.950073 Paf2 558.626953 Paf3 554.711792 Heatmap 185.072693 Total loss 1898.36157\n",
      "Epoch 295 Step 2570/5304 Paf1 593.946533 Paf2 556.627869 Paf3 559.366211 Heatmap 201.127594 Total loss 1911.06824\n",
      "Epoch 295 Step 2580/5304 Paf1 678.953 Paf2 632.345825 Paf3 632.361145 Heatmap 244.707047 Total loss 2188.36694\n",
      "Epoch 295 Step 2590/5304 Paf1 341.619507 Paf2 298.431396 Paf3 300.840149 Heatmap 75.6830902 Total loss 1016.57416\n",
      "Epoch 295 Step 2600/5304 Paf1 555.012573 Paf2 491.113159 Paf3 508.024719 Heatmap 170.988922 Total loss 1725.1394\n",
      "Epoch 295 Step 2610/5304 Paf1 571.11322 Paf2 527.071411 Paf3 530.02478 Heatmap 161.939011 Total loss 1790.14844\n",
      "Epoch 295 Step 2620/5304 Paf1 610.063354 Paf2 554.167236 Paf3 558.294434 Heatmap 175.251266 Total loss 1897.77637\n",
      "Epoch 295 Step 2630/5304 Paf1 755.164795 Paf2 716.892944 Paf3 704.172791 Heatmap 262.995 Total loss 2439.22559\n",
      "Epoch 295 Step 2640/5304 Paf1 552.292725 Paf2 501.510925 Paf3 496.857422 Heatmap 163.848068 Total loss 1714.50928\n",
      "Epoch 295 Step 2650/5304 Paf1 506.950806 Paf2 471.409119 Paf3 467.828491 Heatmap 143.027466 Total loss 1589.21582\n",
      "Epoch 295 Step 2660/5304 Paf1 602.376526 Paf2 573.837708 Paf3 571.451172 Heatmap 149.317535 Total loss 1896.98291\n",
      "Epoch 295 Step 2670/5304 Paf1 760.863708 Paf2 699.400085 Paf3 679.999268 Heatmap 208.108765 Total loss 2348.37183\n",
      "Epoch 295 Step 2680/5304 Paf1 753.695068 Paf2 710.448853 Paf3 710.697144 Heatmap 233.908951 Total loss 2408.75\n",
      "Epoch 295 Step 2690/5304 Paf1 550.636414 Paf2 532.537598 Paf3 533.931641 Heatmap 151.261765 Total loss 1768.36743\n",
      "Epoch 295 Step 2700/5304 Paf1 1174.95312 Paf2 1093.17664 Paf3 1106.74609 Heatmap 352.883759 Total loss 3727.75977\n",
      "Epoch 295 Step 2710/5304 Paf1 763.4 Paf2 705.349426 Paf3 705.052612 Heatmap 241.862427 Total loss 2415.66455\n",
      "Epoch 295 Step 2720/5304 Paf1 810.782532 Paf2 732.64624 Paf3 749.931885 Heatmap 215.264709 Total loss 2508.62524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 295 Step 2730/5304 Paf1 855.588623 Paf2 800.630127 Paf3 798.685303 Heatmap 275.414398 Total loss 2730.31836\n",
      "Epoch 295 Step 2740/5304 Paf1 644.54187 Paf2 634.671936 Paf3 635.685791 Heatmap 202.56041 Total loss 2117.46\n",
      "Epoch 295 Step 2750/5304 Paf1 711.183105 Paf2 673.064392 Paf3 678.927612 Heatmap 250.301117 Total loss 2313.47632\n",
      "Epoch 295 Step 2760/5304 Paf1 643.632507 Paf2 617.979797 Paf3 598.504639 Heatmap 195.480469 Total loss 2055.59741\n",
      "Epoch 295 Step 2770/5304 Paf1 683.392334 Paf2 619.169189 Paf3 623.969116 Heatmap 224.147659 Total loss 2150.67822\n",
      "Epoch 295 Step 2780/5304 Paf1 539.723267 Paf2 490.768555 Paf3 487.119 Heatmap 156.859207 Total loss 1674.47\n",
      "Epoch 295 Step 2790/5304 Paf1 377.486023 Paf2 351.947144 Paf3 356.190155 Heatmap 122.728683 Total loss 1208.35205\n",
      "Epoch 295 Step 2800/5304 Paf1 755.990601 Paf2 710.947632 Paf3 697.455261 Heatmap 243.030548 Total loss 2407.42407\n",
      "Epoch 295 Step 2810/5304 Paf1 850.638855 Paf2 807.922 Paf3 813.088501 Heatmap 304.736969 Total loss 2776.38623\n",
      "Epoch 295 Step 2820/5304 Paf1 672.945251 Paf2 641.683105 Paf3 640.176086 Heatmap 199.893326 Total loss 2154.69775\n",
      "Epoch 295 Step 2830/5304 Paf1 592.783813 Paf2 533.65686 Paf3 534.527039 Heatmap 148.489777 Total loss 1809.45752\n",
      "Epoch 295 Step 2840/5304 Paf1 627.810303 Paf2 584.137207 Paf3 585.373352 Heatmap 208.488113 Total loss 2005.80896\n",
      "Epoch 295 Step 2850/5304 Paf1 731.114746 Paf2 707.927551 Paf3 708.000122 Heatmap 213.317932 Total loss 2360.36035\n",
      "Epoch 295 Step 2860/5304 Paf1 580.691528 Paf2 524.404663 Paf3 530.968506 Heatmap 174.469391 Total loss 1810.53406\n",
      "Epoch 295 Step 2870/5304 Paf1 519.678345 Paf2 494.29129 Paf3 490.807648 Heatmap 141.417816 Total loss 1646.19507\n",
      "Epoch 295 Step 2880/5304 Paf1 785.960876 Paf2 745.855347 Paf3 738.806 Heatmap 235.952972 Total loss 2506.5752\n",
      "Epoch 295 Step 2890/5304 Paf1 682.207031 Paf2 627.403137 Paf3 656.450562 Heatmap 215.365723 Total loss 2181.42627\n",
      "Epoch 295 Step 2900/5304 Paf1 803.313354 Paf2 751.199402 Paf3 743.232117 Heatmap 223.331146 Total loss 2521.07593\n",
      "Epoch 295 Step 2910/5304 Paf1 671.208191 Paf2 592.313416 Paf3 599.486206 Heatmap 162.856796 Total loss 2025.86462\n",
      "Epoch 295 Step 2920/5304 Paf1 458.406982 Paf2 425.026337 Paf3 428.856171 Heatmap 139.10318 Total loss 1451.3927\n",
      "Epoch 295 Step 2930/5304 Paf1 773.24292 Paf2 735.562866 Paf3 729.246 Heatmap 275.994751 Total loss 2514.04639\n",
      "Epoch 295 Step 2940/5304 Paf1 554.683899 Paf2 513.938904 Paf3 508.223389 Heatmap 185.9617 Total loss 1762.80786\n",
      "Epoch 295 Step 2950/5304 Paf1 521.785278 Paf2 482.887085 Paf3 492.245636 Heatmap 159.142075 Total loss 1656.06\n",
      "Epoch 295 Step 2960/5304 Paf1 605.13562 Paf2 561.617065 Paf3 587.068176 Heatmap 149.319641 Total loss 1903.1405\n",
      "Epoch 295 Step 2970/5304 Paf1 605.814697 Paf2 585.258362 Paf3 579.332 Heatmap 206.34906 Total loss 1976.75403\n",
      "Epoch 295 Step 2980/5304 Paf1 497.216034 Paf2 461.493378 Paf3 473.601929 Heatmap 171.568054 Total loss 1603.87939\n",
      "Epoch 295 Step 2990/5304 Paf1 495.943207 Paf2 457.761566 Paf3 464.564697 Heatmap 165.843307 Total loss 1584.11279\n",
      "Epoch 295 Step 3000/5304 Paf1 553.925171 Paf2 516.70166 Paf3 510.927124 Heatmap 165.427185 Total loss 1746.9812\n",
      "Saved checkpoint for step 3000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1767\n",
      "Epoch 295 Step 3010/5304 Paf1 633.052124 Paf2 605.424622 Paf3 607.395264 Heatmap 202.35 Total loss 2048.22217\n",
      "Epoch 295 Step 3020/5304 Paf1 797.370544 Paf2 773.931824 Paf3 760.994629 Heatmap 305.367126 Total loss 2637.66406\n",
      "Epoch 295 Step 3030/5304 Paf1 566.00293 Paf2 551.567871 Paf3 528.038147 Heatmap 156.255981 Total loss 1801.86499\n",
      "Epoch 295 Step 3040/5304 Paf1 985.824036 Paf2 939.385315 Paf3 952.031921 Heatmap 402.884979 Total loss 3280.12622\n",
      "Epoch 295 Step 3050/5304 Paf1 559.913696 Paf2 530.133423 Paf3 530.487549 Heatmap 174.494598 Total loss 1795.0293\n",
      "Epoch 295 Step 3060/5304 Paf1 627.891296 Paf2 595.911438 Paf3 590.177795 Heatmap 191.487396 Total loss 2005.4679\n",
      "Epoch 295 Step 3070/5304 Paf1 682.94165 Paf2 636.474609 Paf3 668.587708 Heatmap 196.075012 Total loss 2184.0791\n",
      "Epoch 295 Step 3080/5304 Paf1 805.898376 Paf2 731.228638 Paf3 708.677429 Heatmap 235.592896 Total loss 2481.39722\n",
      "Epoch 295 Step 3090/5304 Paf1 646.716431 Paf2 604.432 Paf3 616.272583 Heatmap 222.642776 Total loss 2090.06372\n",
      "Epoch 295 Step 3100/5304 Paf1 499.946625 Paf2 462.867554 Paf3 455.220367 Heatmap 122.55043 Total loss 1540.58496\n",
      "Epoch 295 Step 3110/5304 Paf1 640.487 Paf2 593.445068 Paf3 584.19574 Heatmap 157.532135 Total loss 1975.66\n",
      "Epoch 295 Step 3120/5304 Paf1 628.206055 Paf2 603.437256 Paf3 595.842407 Heatmap 185.985291 Total loss 2013.47095\n",
      "Epoch 295 Step 3130/5304 Paf1 519.129517 Paf2 491.998566 Paf3 487.762787 Heatmap 156.272919 Total loss 1655.16382\n",
      "Epoch 295 Step 3140/5304 Paf1 657.199402 Paf2 618.686218 Paf3 620.826111 Heatmap 208.982147 Total loss 2105.69385\n",
      "Epoch 295 Step 3150/5304 Paf1 569.020081 Paf2 527.95282 Paf3 522.295227 Heatmap 154.920013 Total loss 1774.18811\n",
      "Epoch 295 Step 3160/5304 Paf1 721.374207 Paf2 650.660461 Paf3 679.526428 Heatmap 210.365799 Total loss 2261.92676\n",
      "Epoch 295 Step 3170/5304 Paf1 703.911194 Paf2 661.753296 Paf3 658.785706 Heatmap 234.830902 Total loss 2259.28125\n",
      "Epoch 295 Step 3180/5304 Paf1 1058.75049 Paf2 1010.74957 Paf3 994.156921 Heatmap 350.728638 Total loss 3414.3855\n",
      "Epoch 295 Step 3190/5304 Paf1 1051.42749 Paf2 998.481 Paf3 1000.3725 Heatmap 330.332214 Total loss 3380.61328\n",
      "Epoch 295 Step 3200/5304 Paf1 472.645233 Paf2 440.403717 Paf3 425.037354 Heatmap 118.038635 Total loss 1456.125\n",
      "Epoch 295 Step 3210/5304 Paf1 840.323059 Paf2 815.519287 Paf3 822.514771 Heatmap 271.184357 Total loss 2749.5415\n",
      "Epoch 295 Step 3220/5304 Paf1 556.240784 Paf2 493.672394 Paf3 499.428864 Heatmap 150.455643 Total loss 1699.79773\n",
      "Epoch 295 Step 3230/5304 Paf1 678.445312 Paf2 617.593689 Paf3 604.695862 Heatmap 188.859741 Total loss 2089.59473\n",
      "Epoch 295 Step 3240/5304 Paf1 805.642273 Paf2 740.357544 Paf3 762.00415 Heatmap 256.479309 Total loss 2564.48315\n",
      "Epoch 295 Step 3250/5304 Paf1 477.128967 Paf2 460.198303 Paf3 450.448242 Heatmap 152.298767 Total loss 1540.07422\n",
      "Epoch 295 Step 3260/5304 Paf1 628.397949 Paf2 605.401306 Paf3 610.710266 Heatmap 195.449219 Total loss 2039.95874\n",
      "Epoch 295 Step 3270/5304 Paf1 660.088928 Paf2 608.989502 Paf3 606.662903 Heatmap 188.800079 Total loss 2064.5415\n",
      "Epoch 295 Step 3280/5304 Paf1 792.620789 Paf2 756.052856 Paf3 753.231934 Heatmap 270.342407 Total loss 2572.24805\n",
      "Epoch 295 Step 3290/5304 Paf1 982.598389 Paf2 923.405945 Paf3 939.784058 Heatmap 310.231812 Total loss 3156.02026\n",
      "Epoch 295 Step 3300/5304 Paf1 673.540771 Paf2 639.853577 Paf3 647.632874 Heatmap 227.921417 Total loss 2188.94873\n",
      "Epoch 295 Step 3310/5304 Paf1 547.862854 Paf2 501.953186 Paf3 500.894165 Heatmap 128.938507 Total loss 1679.64868\n",
      "Epoch 295 Step 3320/5304 Paf1 862.829773 Paf2 848.906921 Paf3 844.182129 Heatmap 284.818512 Total loss 2840.7373\n",
      "Epoch 295 Step 3330/5304 Paf1 763.02594 Paf2 710.719604 Paf3 715.020142 Heatmap 191.175018 Total loss 2379.94092\n",
      "Epoch 295 Step 3340/5304 Paf1 747.333252 Paf2 714.868591 Paf3 717.881348 Heatmap 246.533539 Total loss 2426.6167\n",
      "Epoch 295 Step 3350/5304 Paf1 856.90094 Paf2 834.020935 Paf3 848.287415 Heatmap 287.478455 Total loss 2826.68774\n",
      "Epoch 295 Step 3360/5304 Paf1 638.873779 Paf2 636.899475 Paf3 622.282532 Heatmap 209.01149 Total loss 2107.06714\n",
      "Epoch 295 Step 3370/5304 Paf1 621.133667 Paf2 585.789917 Paf3 583.603943 Heatmap 178.585464 Total loss 1969.11304\n",
      "Epoch 295 Step 3380/5304 Paf1 560.545349 Paf2 515.740845 Paf3 518.332825 Heatmap 176.21405 Total loss 1770.83301\n",
      "Epoch 295 Step 3390/5304 Paf1 871.449585 Paf2 820.603943 Paf3 811.953857 Heatmap 263.474487 Total loss 2767.48193\n",
      "Epoch 295 Step 3400/5304 Paf1 731.64032 Paf2 697.851868 Paf3 696.416 Heatmap 247.836594 Total loss 2373.74487\n",
      "Epoch 295 Step 3410/5304 Paf1 661.900085 Paf2 618.403259 Paf3 622.743469 Heatmap 207.778046 Total loss 2110.82471\n",
      "Epoch 295 Step 3420/5304 Paf1 703.027527 Paf2 675.207031 Paf3 647.360107 Heatmap 234.999115 Total loss 2260.59375\n",
      "Epoch 295 Step 3430/5304 Paf1 695.266296 Paf2 641.851379 Paf3 666.210938 Heatmap 190.816071 Total loss 2194.14453\n",
      "Epoch 295 Step 3440/5304 Paf1 676.274963 Paf2 687.602356 Paf3 681.936646 Heatmap 214.908783 Total loss 2260.72266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 295 Step 3450/5304 Paf1 606.47406 Paf2 581.308899 Paf3 571.192383 Heatmap 207.425369 Total loss 1966.40063\n",
      "Epoch 295 Step 3460/5304 Paf1 838.456787 Paf2 817.701172 Paf3 785.683 Heatmap 248.765335 Total loss 2690.60645\n",
      "Epoch 295 Step 3470/5304 Paf1 559.803345 Paf2 501.995209 Paf3 502.588867 Heatmap 139.032913 Total loss 1703.42041\n",
      "Epoch 295 Step 3480/5304 Paf1 534.961853 Paf2 473.800232 Paf3 454.383667 Heatmap 162.599289 Total loss 1625.74512\n",
      "Epoch 295 Step 3490/5304 Paf1 1472.06213 Paf2 1401.34326 Paf3 1391.58093 Heatmap 489.257416 Total loss 4754.24365\n",
      "Epoch 295 Step 3500/5304 Paf1 967.688721 Paf2 884.567322 Paf3 886.425964 Heatmap 290.786804 Total loss 3029.46875\n",
      "Epoch 295 Step 3510/5304 Paf1 729.035828 Paf2 687.44281 Paf3 689.001892 Heatmap 223.480133 Total loss 2328.96069\n",
      "Epoch 295 Step 3520/5304 Paf1 414.093933 Paf2 382.226471 Paf3 394.37561 Heatmap 124.544823 Total loss 1315.24084\n",
      "Epoch 295 Step 3530/5304 Paf1 471.216034 Paf2 423.533783 Paf3 429.529633 Heatmap 173.494812 Total loss 1497.77417\n",
      "Epoch 295 Step 3540/5304 Paf1 717.161438 Paf2 671.203125 Paf3 672.755859 Heatmap 265.640198 Total loss 2326.7605\n",
      "Epoch 295 Step 3550/5304 Paf1 516.472351 Paf2 491.854889 Paf3 498.394745 Heatmap 167.181503 Total loss 1673.90356\n",
      "Epoch 295 Step 3560/5304 Paf1 902.354065 Paf2 869.139526 Paf3 871.774292 Heatmap 366.905518 Total loss 3010.17334\n",
      "Epoch 295 Step 3570/5304 Paf1 617.663452 Paf2 563.960754 Paf3 580.043701 Heatmap 185.059 Total loss 1946.72705\n",
      "Epoch 295 Step 3580/5304 Paf1 849.170654 Paf2 802.256775 Paf3 818.313232 Heatmap 274.129578 Total loss 2743.87036\n",
      "Epoch 295 Step 3590/5304 Paf1 654.471 Paf2 593.470764 Paf3 582.984741 Heatmap 184.992279 Total loss 2015.91882\n",
      "Epoch 295 Step 3600/5304 Paf1 403.99295 Paf2 403.708954 Paf3 384.64566 Heatmap 106.436691 Total loss 1298.78418\n",
      "Epoch 295 Step 3610/5304 Paf1 631.357788 Paf2 603.973206 Paf3 611.463867 Heatmap 188.442688 Total loss 2035.23755\n",
      "Epoch 295 Step 3620/5304 Paf1 664.136658 Paf2 624.022766 Paf3 622.235901 Heatmap 194.645081 Total loss 2105.04053\n",
      "Epoch 295 Step 3630/5304 Paf1 874.176941 Paf2 786.201416 Paf3 804.202698 Heatmap 270.384979 Total loss 2734.96606\n",
      "Epoch 295 Step 3640/5304 Paf1 674.981201 Paf2 639.219849 Paf3 620.261536 Heatmap 199.946747 Total loss 2134.40918\n",
      "Epoch 295 Step 3650/5304 Paf1 755.162903 Paf2 697.061279 Paf3 706.766235 Heatmap 219.455231 Total loss 2378.44556\n",
      "Epoch 295 Step 3660/5304 Paf1 812.072815 Paf2 774.562561 Paf3 770.720825 Heatmap 312.21405 Total loss 2669.57031\n",
      "Epoch 295 Step 3670/5304 Paf1 698.410461 Paf2 664.490295 Paf3 656.190857 Heatmap 208.029083 Total loss 2227.12061\n",
      "Epoch 295 Step 3680/5304 Paf1 667.07489 Paf2 620.84845 Paf3 596.025757 Heatmap 179.213074 Total loss 2063.16211\n",
      "Epoch 295 Step 3690/5304 Paf1 914.7323 Paf2 863.251587 Paf3 869.204041 Heatmap 295.201 Total loss 2942.38892\n",
      "Epoch 295 Step 3700/5304 Paf1 808.042236 Paf2 775.109314 Paf3 792.294739 Heatmap 228.545059 Total loss 2603.99146\n",
      "Epoch 295 Step 3710/5304 Paf1 757.524353 Paf2 729.006348 Paf3 706.437683 Heatmap 257.618347 Total loss 2450.58691\n",
      "Epoch 295 Step 3720/5304 Paf1 866.129578 Paf2 802.096252 Paf3 798.383667 Heatmap 276.84967 Total loss 2743.45923\n",
      "Epoch 295 Step 3730/5304 Paf1 775.74762 Paf2 691.939148 Paf3 717.138184 Heatmap 208.028961 Total loss 2392.854\n",
      "Epoch 295 Step 3740/5304 Paf1 569.210205 Paf2 535.228088 Paf3 533.393799 Heatmap 200.12059 Total loss 1837.95264\n",
      "Epoch 295 Step 3750/5304 Paf1 768.457 Paf2 743.51178 Paf3 742.374329 Heatmap 249.588959 Total loss 2503.93213\n",
      "Epoch 295 Step 3760/5304 Paf1 751.087646 Paf2 691.342896 Paf3 696.471558 Heatmap 225.347122 Total loss 2364.24927\n",
      "Epoch 295 Step 3770/5304 Paf1 596.977783 Paf2 575.431824 Paf3 564.808655 Heatmap 199.841156 Total loss 1937.05945\n",
      "Epoch 295 Step 3780/5304 Paf1 600.180176 Paf2 572.717041 Paf3 559.34491 Heatmap 172.197464 Total loss 1904.43958\n",
      "Epoch 295 Step 3790/5304 Paf1 885.550659 Paf2 876.165955 Paf3 870.594177 Heatmap 317.895935 Total loss 2950.20654\n",
      "Epoch 295 Step 3800/5304 Paf1 773.550354 Paf2 718.868835 Paf3 726.169128 Heatmap 244.365768 Total loss 2462.9541\n",
      "Epoch 295 Step 3810/5304 Paf1 683.823364 Paf2 619.547729 Paf3 627.846191 Heatmap 207.151611 Total loss 2138.3689\n",
      "Epoch 295 Step 3820/5304 Paf1 658.660461 Paf2 624.871338 Paf3 622.556458 Heatmap 185.197861 Total loss 2091.28613\n",
      "Epoch 295 Step 3830/5304 Paf1 750.627563 Paf2 711.720337 Paf3 705.741821 Heatmap 228.204758 Total loss 2396.29443\n",
      "Epoch 295 Step 3840/5304 Paf1 658.686462 Paf2 639.103577 Paf3 639.355957 Heatmap 214.967987 Total loss 2152.11401\n",
      "Epoch 295 Step 3850/5304 Paf1 437.71109 Paf2 408.065369 Paf3 402.070831 Heatmap 126.279602 Total loss 1374.12695\n",
      "Epoch 295 Step 3860/5304 Paf1 558.488 Paf2 505.587036 Paf3 517.108093 Heatmap 168.52681 Total loss 1749.70984\n",
      "Epoch 295 Step 3870/5304 Paf1 618.312378 Paf2 578.44873 Paf3 579.605042 Heatmap 180.581024 Total loss 1956.94714\n",
      "Epoch 295 Step 3880/5304 Paf1 621.023315 Paf2 576.720825 Paf3 603.36145 Heatmap 168.77417 Total loss 1969.87976\n",
      "Epoch 295 Step 3890/5304 Paf1 685.960449 Paf2 667.769104 Paf3 671.115417 Heatmap 184.311081 Total loss 2209.15601\n",
      "Epoch 295 Step 3900/5304 Paf1 757.105103 Paf2 728.272278 Paf3 722.022522 Heatmap 237.324432 Total loss 2444.72437\n",
      "Epoch 295 Step 3910/5304 Paf1 484.2453 Paf2 445.779022 Paf3 445.958221 Heatmap 141.40152 Total loss 1517.38403\n",
      "Epoch 295 Step 3920/5304 Paf1 539.933105 Paf2 508.108826 Paf3 521.084045 Heatmap 151.87027 Total loss 1720.99634\n",
      "Epoch 295 Step 3930/5304 Paf1 872.438782 Paf2 816.520874 Paf3 817.169861 Heatmap 315.587097 Total loss 2821.7168\n",
      "Epoch 295 Step 3940/5304 Paf1 738.910889 Paf2 695.024231 Paf3 693.630615 Heatmap 216.238159 Total loss 2343.80371\n",
      "Epoch 295 Step 3950/5304 Paf1 605.600647 Paf2 586.748962 Paf3 590.829102 Heatmap 194.045563 Total loss 1977.22424\n",
      "Epoch 295 Step 3960/5304 Paf1 653.644836 Paf2 597.372742 Paf3 603.109558 Heatmap 192.759415 Total loss 2046.88647\n",
      "Epoch 295 Step 3970/5304 Paf1 686.966797 Paf2 660.636841 Paf3 646.908386 Heatmap 219.899155 Total loss 2214.41113\n",
      "Epoch 295 Step 3980/5304 Paf1 421.488342 Paf2 393.488342 Paf3 396.915222 Heatmap 122.702469 Total loss 1334.59436\n",
      "Epoch 295 Step 3990/5304 Paf1 609.006714 Paf2 579.091 Paf3 576.532532 Heatmap 165.162384 Total loss 1929.7926\n",
      "Epoch 295 Step 4000/5304 Paf1 704.264404 Paf2 691.726807 Paf3 682.812439 Heatmap 240.602203 Total loss 2319.40576\n",
      "Saved checkpoint for step 4000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1768\n",
      "Epoch 295 Step 4010/5304 Paf1 747.654541 Paf2 705.94696 Paf3 705.121094 Heatmap 251.708588 Total loss 2410.43115\n",
      "Epoch 295 Step 4020/5304 Paf1 739.239929 Paf2 698.483337 Paf3 689.657959 Heatmap 221.801422 Total loss 2349.18262\n",
      "Epoch 295 Step 4030/5304 Paf1 730.276672 Paf2 670.528137 Paf3 679.4328 Heatmap 198.720856 Total loss 2278.9585\n",
      "Epoch 295 Step 4040/5304 Paf1 630.489624 Paf2 607.935303 Paf3 593.11377 Heatmap 206.516296 Total loss 2038.05493\n",
      "Epoch 295 Step 4050/5304 Paf1 479.143158 Paf2 432.802734 Paf3 436.385681 Heatmap 145.283875 Total loss 1493.61548\n",
      "Epoch 295 Step 4060/5304 Paf1 777.127625 Paf2 761.491455 Paf3 739.348877 Heatmap 238.348633 Total loss 2516.31665\n",
      "Epoch 295 Step 4070/5304 Paf1 611.420959 Paf2 581.535889 Paf3 583.478455 Heatmap 183.580292 Total loss 1960.0155\n",
      "Epoch 295 Step 4080/5304 Paf1 950.831909 Paf2 915.809448 Paf3 914.167 Heatmap 299.967468 Total loss 3080.77588\n",
      "Epoch 295 Step 4090/5304 Paf1 596.372131 Paf2 552.206787 Paf3 541.480774 Heatmap 175.599243 Total loss 1865.65894\n",
      "Epoch 295 Step 4100/5304 Paf1 694.965088 Paf2 656.17749 Paf3 654.480225 Heatmap 191.364105 Total loss 2196.98682\n",
      "Epoch 295 Step 4110/5304 Paf1 544.996399 Paf2 509.052307 Paf3 505.808044 Heatmap 176.380066 Total loss 1736.23682\n",
      "Epoch 295 Step 4120/5304 Paf1 644.289185 Paf2 612.760437 Paf3 614.675659 Heatmap 191.292694 Total loss 2063.01807\n",
      "Epoch 295 Step 4130/5304 Paf1 634.556763 Paf2 621.420654 Paf3 620.556213 Heatmap 241.117767 Total loss 2117.65137\n",
      "Epoch 295 Step 4140/5304 Paf1 847.65979 Paf2 745.191895 Paf3 766.980347 Heatmap 241.245102 Total loss 2601.07715\n",
      "Epoch 295 Step 4150/5304 Paf1 1015.14203 Paf2 967.096924 Paf3 955.441833 Heatmap 361.419556 Total loss 3299.10034\n",
      "Epoch 295 Step 4160/5304 Paf1 639.815674 Paf2 578.0755 Paf3 584.006592 Heatmap 159.817078 Total loss 1961.71484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 295 Step 4170/5304 Paf1 834.220886 Paf2 781.33606 Paf3 791.577576 Heatmap 233.565765 Total loss 2640.7002\n",
      "Epoch 295 Step 4180/5304 Paf1 957.43219 Paf2 907.183105 Paf3 894.930786 Heatmap 303.530945 Total loss 3063.0769\n",
      "Epoch 295 Step 4190/5304 Paf1 588.168701 Paf2 533.254333 Paf3 531.634033 Heatmap 176.947144 Total loss 1830.00427\n",
      "Epoch 295 Step 4200/5304 Paf1 672.83783 Paf2 631.381653 Paf3 621.299133 Heatmap 193.494446 Total loss 2119.01318\n",
      "Epoch 295 Step 4210/5304 Paf1 841.69165 Paf2 796.092041 Paf3 805.719604 Heatmap 300.836731 Total loss 2744.34\n",
      "Epoch 295 Step 4220/5304 Paf1 514.855103 Paf2 479.183807 Paf3 473.652405 Heatmap 175.102463 Total loss 1642.79382\n",
      "Epoch 295 Step 4230/5304 Paf1 779.782471 Paf2 743.507324 Paf3 738.205933 Heatmap 257.868042 Total loss 2519.36377\n",
      "Epoch 295 Step 4240/5304 Paf1 649.552063 Paf2 578.406616 Paf3 588.647 Heatmap 176.578186 Total loss 1993.18384\n",
      "Epoch 295 Step 4250/5304 Paf1 611.950317 Paf2 581.473755 Paf3 580.11438 Heatmap 172.2659 Total loss 1945.80432\n",
      "Epoch 295 Step 4260/5304 Paf1 599.201904 Paf2 541.541199 Paf3 546.790161 Heatmap 175.950409 Total loss 1863.48376\n",
      "Epoch 295 Step 4270/5304 Paf1 563.52533 Paf2 497.389832 Paf3 503.170624 Heatmap 171.911072 Total loss 1735.99683\n",
      "Epoch 295 Step 4280/5304 Paf1 607.384705 Paf2 590.649536 Paf3 587.421753 Heatmap 176.320358 Total loss 1961.77637\n",
      "Epoch 295 Step 4290/5304 Paf1 703.313599 Paf2 661.017273 Paf3 665.170349 Heatmap 207.266144 Total loss 2236.76733\n",
      "Epoch 295 Step 4300/5304 Paf1 496.271912 Paf2 428.293762 Paf3 445.316467 Heatmap 155.693939 Total loss 1525.57605\n",
      "Epoch 295 Step 4310/5304 Paf1 461.53833 Paf2 432.059052 Paf3 433.063354 Heatmap 152.690796 Total loss 1479.35156\n",
      "Epoch 295 Step 4320/5304 Paf1 609.996216 Paf2 547.868 Paf3 536.659 Heatmap 181.795654 Total loss 1876.31885\n",
      "Epoch 295 Step 4330/5304 Paf1 605.212097 Paf2 556.726257 Paf3 547.750488 Heatmap 210.853333 Total loss 1920.54224\n",
      "Epoch 295 Step 4340/5304 Paf1 650.753357 Paf2 613.372375 Paf3 616.813 Heatmap 220.171936 Total loss 2101.1106\n",
      "Epoch 295 Step 4350/5304 Paf1 768.196594 Paf2 741.161438 Paf3 728.081909 Heatmap 223.583923 Total loss 2461.02393\n",
      "Epoch 295 Step 4360/5304 Paf1 688.904419 Paf2 655.652893 Paf3 649.975952 Heatmap 232.341705 Total loss 2226.875\n",
      "Epoch 295 Step 4370/5304 Paf1 594.1875 Paf2 583.671387 Paf3 561.256226 Heatmap 168.445557 Total loss 1907.56067\n",
      "Epoch 295 Step 4380/5304 Paf1 490.240173 Paf2 463.575317 Paf3 459.713715 Heatmap 135.228 Total loss 1548.7572\n",
      "Epoch 295 Step 4390/5304 Paf1 505.06311 Paf2 456.939514 Paf3 468.668152 Heatmap 149.633514 Total loss 1580.3042\n",
      "Epoch 295 Step 4400/5304 Paf1 753.427673 Paf2 693.884 Paf3 711.352112 Heatmap 217.447342 Total loss 2376.11108\n",
      "Epoch 295 Step 4410/5304 Paf1 991.583801 Paf2 913.791321 Paf3 908.520813 Heatmap 278.772064 Total loss 3092.66797\n",
      "Epoch 295 Step 4420/5304 Paf1 609.270874 Paf2 587.635254 Paf3 579.54895 Heatmap 179.742279 Total loss 1956.19739\n",
      "Epoch 295 Step 4430/5304 Paf1 591.933655 Paf2 562.201172 Paf3 554.372192 Heatmap 161.864532 Total loss 1870.37146\n",
      "Epoch 295 Step 4440/5304 Paf1 635.37854 Paf2 575.870422 Paf3 582.462646 Heatmap 172.973938 Total loss 1966.68555\n",
      "Epoch 295 Step 4450/5304 Paf1 826.059 Paf2 797.524292 Paf3 790.006531 Heatmap 241.260651 Total loss 2654.85059\n",
      "Epoch 295 Step 4460/5304 Paf1 557.375549 Paf2 537.363281 Paf3 533.514282 Heatmap 201.421 Total loss 1829.67407\n",
      "Epoch 295 Step 4470/5304 Paf1 738.874817 Paf2 670.959778 Paf3 692.246948 Heatmap 218.234131 Total loss 2320.31567\n",
      "Epoch 295 Step 4480/5304 Paf1 669.728271 Paf2 628.288696 Paf3 636.374817 Heatmap 225.380386 Total loss 2159.77222\n",
      "Epoch 295 Step 4490/5304 Paf1 588.017 Paf2 565.640259 Paf3 565.652283 Heatmap 216.081024 Total loss 1935.3905\n",
      "Epoch 295 Step 4500/5304 Paf1 507.035919 Paf2 440.545166 Paf3 451.048065 Heatmap 149.298859 Total loss 1547.92798\n",
      "Epoch 295 Step 4510/5304 Paf1 538.089783 Paf2 521.087036 Paf3 514.646484 Heatmap 181.201691 Total loss 1755.0249\n",
      "Epoch 295 Step 4520/5304 Paf1 671.192444 Paf2 633.694702 Paf3 647.459045 Heatmap 221.249619 Total loss 2173.59595\n",
      "Epoch 295 Step 4530/5304 Paf1 920.699951 Paf2 870.23877 Paf3 866.391846 Heatmap 301.813477 Total loss 2959.14404\n",
      "Epoch 295 Step 4540/5304 Paf1 902.154297 Paf2 864.291321 Paf3 852.442139 Heatmap 342.691406 Total loss 2961.5791\n",
      "Epoch 295 Step 4550/5304 Paf1 660.73822 Paf2 629.614075 Paf3 635.238098 Heatmap 248.47702 Total loss 2174.06738\n",
      "Epoch 295 Step 4560/5304 Paf1 616.056885 Paf2 580.571411 Paf3 557.925476 Heatmap 253.38942 Total loss 2007.94312\n",
      "Epoch 295 Step 4570/5304 Paf1 518.642761 Paf2 485.388947 Paf3 481.208344 Heatmap 161.310547 Total loss 1646.55066\n",
      "Epoch 295 Step 4580/5304 Paf1 738.660645 Paf2 702.968628 Paf3 713.838867 Heatmap 196.790314 Total loss 2352.2583\n",
      "Epoch 295 Step 4590/5304 Paf1 957.900635 Paf2 914.591309 Paf3 913.896423 Heatmap 259.483337 Total loss 3045.87158\n",
      "Epoch 295 Step 4600/5304 Paf1 580.944031 Paf2 533.291626 Paf3 526.7854 Heatmap 185.734558 Total loss 1826.75562\n",
      "Epoch 295 Step 4610/5304 Paf1 613.184448 Paf2 582.441284 Paf3 577.026917 Heatmap 196.747375 Total loss 1969.4\n",
      "Epoch 295 Step 4620/5304 Paf1 781.248108 Paf2 721.018127 Paf3 714.554565 Heatmap 258.496368 Total loss 2475.31714\n",
      "Epoch 295 Step 4630/5304 Paf1 1043.05188 Paf2 996.174438 Paf3 988.986938 Heatmap 362.15213 Total loss 3390.36523\n",
      "Epoch 295 Step 4640/5304 Paf1 809.777527 Paf2 778.166443 Paf3 754.603577 Heatmap 299.090698 Total loss 2641.63818\n",
      "Epoch 295 Step 4650/5304 Paf1 793.30719 Paf2 755.167114 Paf3 746.488525 Heatmap 220.955948 Total loss 2515.91895\n",
      "Epoch 295 Step 4660/5304 Paf1 634.503845 Paf2 556.746216 Paf3 560.048889 Heatmap 180.155121 Total loss 1931.45398\n",
      "Epoch 295 Step 4670/5304 Paf1 745.736877 Paf2 699.131287 Paf3 685.702393 Heatmap 244.608414 Total loss 2375.17896\n",
      "Epoch 295 Step 4680/5304 Paf1 729.997437 Paf2 669.757263 Paf3 678.023682 Heatmap 195.613708 Total loss 2273.39209\n",
      "Epoch 295 Step 4690/5304 Paf1 584.728271 Paf2 537.211548 Paf3 555.833252 Heatmap 171.379669 Total loss 1849.15271\n",
      "Epoch 295 Step 4700/5304 Paf1 531.62323 Paf2 480.345947 Paf3 497.71637 Heatmap 122.368889 Total loss 1632.05444\n",
      "Epoch 295 Step 4710/5304 Paf1 612.093445 Paf2 588.663574 Paf3 592.192688 Heatmap 202.128265 Total loss 1995.078\n",
      "Epoch 295 Step 4720/5304 Paf1 1006.73859 Paf2 915.176453 Paf3 905.286133 Heatmap 270.864563 Total loss 3098.06567\n",
      "Epoch 295 Step 4730/5304 Paf1 721.50415 Paf2 684.030884 Paf3 665.906311 Heatmap 243.085678 Total loss 2314.5271\n",
      "Epoch 295 Step 4740/5304 Paf1 467.095123 Paf2 439.427124 Paf3 440.905487 Heatmap 128.057785 Total loss 1475.48547\n",
      "Epoch 295 Step 4750/5304 Paf1 684.130676 Paf2 626.114136 Paf3 622.404419 Heatmap 192.874847 Total loss 2125.52417\n",
      "Epoch 295 Step 4760/5304 Paf1 598.94751 Paf2 552.529968 Paf3 558.229858 Heatmap 196.70784 Total loss 1906.41528\n",
      "Epoch 295 Step 4770/5304 Paf1 576.575562 Paf2 548.766174 Paf3 534.796875 Heatmap 176.805206 Total loss 1836.94385\n",
      "Epoch 295 Step 4780/5304 Paf1 857.822 Paf2 821.254089 Paf3 807.118408 Heatmap 272.372375 Total loss 2758.56689\n",
      "Epoch 295 Step 4790/5304 Paf1 600.964172 Paf2 558.051575 Paf3 565.407104 Heatmap 160.575256 Total loss 1884.99805\n",
      "Epoch 295 Step 4800/5304 Paf1 898.662781 Paf2 803.649109 Paf3 830.043 Heatmap 288.184204 Total loss 2820.53906\n",
      "Epoch 295 Step 4810/5304 Paf1 629.173584 Paf2 593.615112 Paf3 603.6651 Heatmap 170.948456 Total loss 1997.40222\n",
      "Epoch 295 Step 4820/5304 Paf1 565.074829 Paf2 514.025146 Paf3 521.398438 Heatmap 165.373947 Total loss 1765.87231\n",
      "Epoch 295 Step 4830/5304 Paf1 823.579956 Paf2 736.849731 Paf3 754.912231 Heatmap 315.949768 Total loss 2631.29175\n",
      "Epoch 295 Step 4840/5304 Paf1 989.858459 Paf2 933.883179 Paf3 926.296814 Heatmap 265.773 Total loss 3115.81152\n",
      "Epoch 295 Step 4850/5304 Paf1 876.793518 Paf2 818.472107 Paf3 812.456787 Heatmap 249.944397 Total loss 2757.66675\n",
      "Epoch 295 Step 4860/5304 Paf1 546.624146 Paf2 507.549316 Paf3 496.592072 Heatmap 147.603745 Total loss 1698.36926\n",
      "Epoch 295 Step 4870/5304 Paf1 526.716187 Paf2 506.729889 Paf3 511.930756 Heatmap 161.141266 Total loss 1706.51807\n",
      "Epoch 295 Step 4880/5304 Paf1 873.718079 Paf2 833.58606 Paf3 809.055542 Heatmap 272.601532 Total loss 2788.96143\n",
      "Epoch 295 Step 4890/5304 Paf1 643.700623 Paf2 615.863098 Paf3 607.007324 Heatmap 216.715698 Total loss 2083.28662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 295 Step 4900/5304 Paf1 616.8302 Paf2 562.383606 Paf3 552.67865 Heatmap 176.713058 Total loss 1908.60559\n",
      "Epoch 295 Step 4910/5304 Paf1 400.202881 Paf2 375.210236 Paf3 377.434387 Heatmap 94.6385651 Total loss 1247.48608\n",
      "Epoch 295 Step 4920/5304 Paf1 724.608459 Paf2 690.734863 Paf3 699.075195 Heatmap 249.182968 Total loss 2363.60156\n",
      "Epoch 295 Step 4930/5304 Paf1 500.710938 Paf2 458.113861 Paf3 460.254395 Heatmap 138.225708 Total loss 1557.30493\n",
      "Epoch 295 Step 4940/5304 Paf1 744.859 Paf2 680.519775 Paf3 679.478333 Heatmap 217.315765 Total loss 2322.17285\n",
      "Epoch 295 Step 4950/5304 Paf1 605.071716 Paf2 570.529 Paf3 581.499695 Heatmap 158.212128 Total loss 1915.3125\n",
      "Epoch 295 Step 4960/5304 Paf1 439.715454 Paf2 415.435303 Paf3 411.691589 Heatmap 127.370483 Total loss 1394.21289\n",
      "Epoch 295 Step 4970/5304 Paf1 475.020203 Paf2 462.474823 Paf3 455.149628 Heatmap 134.43605 Total loss 1527.08069\n",
      "Epoch 295 Step 4980/5304 Paf1 481.272461 Paf2 438.974854 Paf3 430.264679 Heatmap 141.312805 Total loss 1491.82483\n",
      "Epoch 295 Step 4990/5304 Paf1 965.806091 Paf2 888.342 Paf3 915.9776 Heatmap 290.286621 Total loss 3060.41211\n",
      "Epoch 295 Step 5000/5304 Paf1 708.081177 Paf2 667.101135 Paf3 680.825317 Heatmap 229.824188 Total loss 2285.83203\n",
      "Saved checkpoint for step 5000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1769\n",
      "Epoch 295 Step 5010/5304 Paf1 665.022461 Paf2 621.057495 Paf3 652.189209 Heatmap 205.715424 Total loss 2143.98462\n",
      "Epoch 295 Step 5020/5304 Paf1 565.043 Paf2 518.067871 Paf3 494.829926 Heatmap 183.875137 Total loss 1761.81592\n",
      "Epoch 295 Step 5030/5304 Paf1 819.130737 Paf2 754.016418 Paf3 760.975342 Heatmap 221.858093 Total loss 2555.98071\n",
      "Epoch 295 Step 5040/5304 Paf1 504.902374 Paf2 464.933899 Paf3 472.942963 Heatmap 153.597046 Total loss 1596.37634\n",
      "Epoch 295 Step 5050/5304 Paf1 682.812439 Paf2 644.277161 Paf3 646.517944 Heatmap 219.830688 Total loss 2193.43823\n",
      "Epoch 295 Step 5060/5304 Paf1 823.291321 Paf2 778.229126 Paf3 777.656677 Heatmap 228.92894 Total loss 2608.1062\n",
      "Epoch 295 Step 5070/5304 Paf1 732.434326 Paf2 696.232361 Paf3 693.033081 Heatmap 194.255157 Total loss 2315.95508\n",
      "Epoch 295 Step 5080/5304 Paf1 603.440369 Paf2 556.989868 Paf3 556.00293 Heatmap 185.708954 Total loss 1902.14209\n",
      "Epoch 295 Step 5090/5304 Paf1 545.550659 Paf2 515.984131 Paf3 494.159729 Heatmap 163.879303 Total loss 1719.57385\n",
      "Epoch 295 Step 5100/5304 Paf1 668.424 Paf2 612.530579 Paf3 623.776489 Heatmap 191.367355 Total loss 2096.09839\n",
      "Epoch 295 Step 5110/5304 Paf1 903.247375 Paf2 870.065796 Paf3 867.517761 Heatmap 284.549347 Total loss 2925.38037\n",
      "Epoch 295 Step 5120/5304 Paf1 796.621399 Paf2 752.063782 Paf3 745.971252 Heatmap 244.931183 Total loss 2539.58765\n",
      "Epoch 295 Step 5130/5304 Paf1 615.951904 Paf2 604.838745 Paf3 602.799072 Heatmap 200.598694 Total loss 2024.18848\n",
      "Epoch 295 Step 5140/5304 Paf1 319.879883 Paf2 293.924683 Paf3 286.967 Heatmap 90.7200317 Total loss 991.491577\n",
      "Epoch 295 Step 5150/5304 Paf1 913.229614 Paf2 855.913635 Paf3 858.229187 Heatmap 276.99408 Total loss 2904.3667\n",
      "Epoch 295 Step 5160/5304 Paf1 690.29834 Paf2 629.328613 Paf3 639.074402 Heatmap 222.358292 Total loss 2181.05957\n",
      "Epoch 295 Step 5170/5304 Paf1 546.913879 Paf2 505.42749 Paf3 490.52356 Heatmap 165.154373 Total loss 1708.01929\n",
      "Epoch 295 Step 5180/5304 Paf1 616.363159 Paf2 582.41748 Paf3 562.949585 Heatmap 238.83905 Total loss 2000.56934\n",
      "Epoch 295 Step 5190/5304 Paf1 567.372864 Paf2 527.598145 Paf3 524.143066 Heatmap 154.286438 Total loss 1773.40039\n",
      "Epoch 295 Step 5200/5304 Paf1 476.18396 Paf2 422.206787 Paf3 433.11908 Heatmap 137.372086 Total loss 1468.88184\n",
      "Epoch 295 Step 5210/5304 Paf1 956.459839 Paf2 911.573486 Paf3 913.949524 Heatmap 291.802216 Total loss 3073.78516\n",
      "Epoch 295 Step 5220/5304 Paf1 842.576416 Paf2 822.52124 Paf3 837.517578 Heatmap 334.488129 Total loss 2837.10352\n",
      "Epoch 295 Step 5230/5304 Paf1 792.736938 Paf2 725.713379 Paf3 729.913574 Heatmap 239.228989 Total loss 2487.59277\n",
      "Epoch 295 Step 5240/5304 Paf1 469.043396 Paf2 460.354675 Paf3 454.48822 Heatmap 153.728973 Total loss 1537.61523\n",
      "Epoch 295 Step 5250/5304 Paf1 590.701782 Paf2 564.861511 Paf3 559.662292 Heatmap 190.107819 Total loss 1905.33337\n",
      "Epoch 295 Step 5260/5304 Paf1 584.346497 Paf2 530.853088 Paf3 529.82312 Heatmap 157.711182 Total loss 1802.73389\n",
      "Epoch 295 Step 5270/5304 Paf1 597.795166 Paf2 563.777344 Paf3 574.389099 Heatmap 203.176453 Total loss 1939.13806\n",
      "Epoch 295 Step 5280/5304 Paf1 566.781372 Paf2 547.737366 Paf3 523.429 Heatmap 158.91925 Total loss 1796.86707\n",
      "Epoch 295 Step 5290/5304 Paf1 1108.61987 Paf2 1037.19922 Paf3 1024.8811 Heatmap 358.740021 Total loss 3529.44019\n",
      "Epoch 295 Step 5300/5304 Paf1 652.446777 Paf2 611.121338 Paf3 621.631958 Heatmap 208.436096 Total loss 2093.63623\n",
      "Completed epoch 295. Saving weights...\n",
      "Epoch training time: 0:16:26.392992\n",
      "Calculating validation losses...\n",
      "Validation step 0 ...\n",
      "Validation losses for epoch: 295 : Loss paf 689.177490234375, Loss heatmap 228.8511505126953, Total loss 2337.77197265625\n",
      "Start processing epoch 296\n",
      "Epoch 296 Step 10/5304 Paf1 738.906372 Paf2 697.515 Paf3 706.754761 Heatmap 212.315506 Total loss 2355.4917\n",
      "Epoch 296 Step 20/5304 Paf1 783.727722 Paf2 737.652466 Paf3 729.166687 Heatmap 252.723785 Total loss 2503.27051\n",
      "Epoch 296 Step 30/5304 Paf1 570.584229 Paf2 514.400452 Paf3 539.392761 Heatmap 159.84671 Total loss 1784.22412\n",
      "Epoch 296 Step 40/5304 Paf1 579.292 Paf2 533.308716 Paf3 521.614075 Heatmap 179.817642 Total loss 1814.03247\n",
      "Epoch 296 Step 50/5304 Paf1 1057.26648 Paf2 986.565918 Paf3 984.084595 Heatmap 369.84198 Total loss 3397.75879\n",
      "Epoch 296 Step 60/5304 Paf1 594.076172 Paf2 547.107056 Paf3 544.593506 Heatmap 178.933075 Total loss 1864.70984\n",
      "Epoch 296 Step 70/5304 Paf1 711.114075 Paf2 682.333374 Paf3 660.208374 Heatmap 265.763306 Total loss 2319.41919\n",
      "Epoch 296 Step 80/5304 Paf1 698.376831 Paf2 644.151428 Paf3 670.319275 Heatmap 244.742 Total loss 2257.5896\n",
      "Epoch 296 Step 90/5304 Paf1 698.371826 Paf2 661.946594 Paf3 663.109863 Heatmap 237.034424 Total loss 2260.46265\n",
      "Epoch 296 Step 100/5304 Paf1 932.57 Paf2 878.063782 Paf3 872.845398 Heatmap 271.052277 Total loss 2954.53149\n",
      "Epoch 296 Step 110/5304 Paf1 669.455 Paf2 639.305054 Paf3 628.51416 Heatmap 207.960815 Total loss 2145.23486\n",
      "Epoch 296 Step 120/5304 Paf1 511.092224 Paf2 471.974609 Paf3 473.326874 Heatmap 136.153839 Total loss 1592.54761\n",
      "Epoch 296 Step 130/5304 Paf1 580.327942 Paf2 562.933411 Paf3 556.758057 Heatmap 176.652222 Total loss 1876.67163\n",
      "Epoch 296 Step 140/5304 Paf1 928.630493 Paf2 913.526245 Paf3 915.816956 Heatmap 393.260651 Total loss 3151.23438\n",
      "Epoch 296 Step 150/5304 Paf1 826.059082 Paf2 787.126465 Paf3 797.241516 Heatmap 255.888138 Total loss 2666.31519\n",
      "Epoch 296 Step 160/5304 Paf1 513.839355 Paf2 461.738739 Paf3 472.317 Heatmap 142.148499 Total loss 1590.04358\n",
      "Epoch 296 Step 170/5304 Paf1 537.810364 Paf2 493.069031 Paf3 489.23111 Heatmap 157.588684 Total loss 1677.69922\n",
      "Epoch 296 Step 180/5304 Paf1 590.1651 Paf2 520.291443 Paf3 530.27771 Heatmap 163.195953 Total loss 1803.93018\n",
      "Epoch 296 Step 190/5304 Paf1 566.638916 Paf2 522.790039 Paf3 525.84259 Heatmap 185.350525 Total loss 1800.62207\n",
      "Epoch 296 Step 200/5304 Paf1 610.451233 Paf2 570.9021 Paf3 572.460938 Heatmap 173.08548 Total loss 1926.89966\n",
      "Epoch 296 Step 210/5304 Paf1 974.990906 Paf2 909.390686 Paf3 915.610596 Heatmap 333.918152 Total loss 3133.9104\n",
      "Epoch 296 Step 220/5304 Paf1 618.708191 Paf2 609.210754 Paf3 596.286133 Heatmap 198.111389 Total loss 2022.31641\n",
      "Epoch 296 Step 230/5304 Paf1 676.105591 Paf2 631.969055 Paf3 648.118958 Heatmap 240.022369 Total loss 2196.21606\n",
      "Epoch 296 Step 240/5304 Paf1 784.841431 Paf2 741.885742 Paf3 743.194214 Heatmap 256.502228 Total loss 2526.42358\n",
      "Epoch 296 Step 250/5304 Paf1 750.352356 Paf2 726.505127 Paf3 718.780945 Heatmap 224.331757 Total loss 2419.97021\n",
      "Epoch 296 Step 260/5304 Paf1 725.99054 Paf2 672.548218 Paf3 665.887451 Heatmap 205.129868 Total loss 2269.55615\n",
      "Epoch 296 Step 270/5304 Paf1 850.155457 Paf2 828.614807 Paf3 801.606812 Heatmap 295.345459 Total loss 2775.72266\n",
      "Epoch 296 Step 280/5304 Paf1 702.132935 Paf2 645.733032 Paf3 634.865723 Heatmap 204.889191 Total loss 2187.62085\n",
      "Epoch 296 Step 290/5304 Paf1 655.177734 Paf2 591.421631 Paf3 585.873718 Heatmap 185.280624 Total loss 2017.75366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296 Step 300/5304 Paf1 634.536377 Paf2 581.39093 Paf3 580.786072 Heatmap 169.758453 Total loss 1966.4718\n",
      "Epoch 296 Step 310/5304 Paf1 712.566895 Paf2 670.18634 Paf3 684.24408 Heatmap 278.860291 Total loss 2345.85742\n",
      "Epoch 296 Step 320/5304 Paf1 509.007385 Paf2 455.866028 Paf3 456.538574 Heatmap 120.285263 Total loss 1541.69727\n",
      "Epoch 296 Step 330/5304 Paf1 722.01825 Paf2 691.1 Paf3 692.773071 Heatmap 223.862717 Total loss 2329.75391\n",
      "Epoch 296 Step 340/5304 Paf1 795.450745 Paf2 738.822632 Paf3 765.703613 Heatmap 247.180756 Total loss 2547.15771\n",
      "Epoch 296 Step 350/5304 Paf1 621.462341 Paf2 577.442322 Paf3 563.883057 Heatmap 172.949127 Total loss 1935.73682\n",
      "Epoch 296 Step 360/5304 Paf1 731.608948 Paf2 686.681519 Paf3 705.440674 Heatmap 222.007874 Total loss 2345.73901\n",
      "Epoch 296 Step 370/5304 Paf1 804.297729 Paf2 759.697449 Paf3 766.009277 Heatmap 242.576584 Total loss 2572.58105\n",
      "Epoch 296 Step 380/5304 Paf1 894.440552 Paf2 819.040771 Paf3 831.130798 Heatmap 273.779175 Total loss 2818.39111\n",
      "Epoch 296 Step 390/5304 Paf1 496.834076 Paf2 464.698975 Paf3 453.480316 Heatmap 158.069 Total loss 1573.0824\n",
      "Epoch 296 Step 400/5304 Paf1 613.790649 Paf2 579.854736 Paf3 580.819519 Heatmap 194.725494 Total loss 1969.19043\n",
      "Epoch 296 Step 410/5304 Paf1 728.452393 Paf2 686.268494 Paf3 684.775757 Heatmap 263.269165 Total loss 2362.76587\n",
      "Epoch 296 Step 420/5304 Paf1 517.491 Paf2 485.954742 Paf3 474.072083 Heatmap 142.559708 Total loss 1620.07764\n",
      "Epoch 296 Step 430/5304 Paf1 876.482361 Paf2 820.425537 Paf3 813.557556 Heatmap 249.153595 Total loss 2759.61914\n",
      "Epoch 296 Step 440/5304 Paf1 623.565247 Paf2 588.182373 Paf3 580.603943 Heatmap 166.519806 Total loss 1958.87134\n",
      "Epoch 296 Step 450/5304 Paf1 627.515 Paf2 583.311218 Paf3 580.796692 Heatmap 153.021362 Total loss 1944.64429\n",
      "Epoch 296 Step 460/5304 Paf1 663.419861 Paf2 628.936768 Paf3 622.895 Heatmap 185.766693 Total loss 2101.01855\n",
      "Epoch 296 Step 470/5304 Paf1 834.250916 Paf2 796.721619 Paf3 782.709167 Heatmap 328.95755 Total loss 2742.63916\n",
      "Epoch 296 Step 480/5304 Paf1 640.656128 Paf2 608.695618 Paf3 607.842 Heatmap 249.417023 Total loss 2106.61084\n",
      "Epoch 296 Step 490/5304 Paf1 644.666504 Paf2 628.409912 Paf3 640.059814 Heatmap 183.622864 Total loss 2096.75903\n",
      "Epoch 296 Step 500/5304 Paf1 800.091553 Paf2 765.504089 Paf3 771.298889 Heatmap 264.040161 Total loss 2600.93481\n",
      "Epoch 296 Step 510/5304 Paf1 669.299194 Paf2 600.727844 Paf3 608.239807 Heatmap 194.0802 Total loss 2072.34717\n",
      "Epoch 296 Step 520/5304 Paf1 611.144226 Paf2 598.01886 Paf3 587.646667 Heatmap 170.675232 Total loss 1967.48499\n",
      "Epoch 296 Step 530/5304 Paf1 1031.40234 Paf2 965.717834 Paf3 960.874878 Heatmap 273.770599 Total loss 3231.76562\n",
      "Epoch 296 Step 540/5304 Paf1 973.242493 Paf2 878.545227 Paf3 876.995544 Heatmap 294.172485 Total loss 3022.95557\n",
      "Epoch 296 Step 550/5304 Paf1 934.040955 Paf2 895.910645 Paf3 889.175903 Heatmap 313.294495 Total loss 3032.42212\n",
      "Epoch 296 Step 560/5304 Paf1 706.003113 Paf2 663.56189 Paf3 664.773193 Heatmap 221.885498 Total loss 2256.22363\n",
      "Epoch 296 Step 570/5304 Paf1 486.166931 Paf2 456.143616 Paf3 450.526672 Heatmap 140.376221 Total loss 1533.21338\n",
      "Epoch 296 Step 580/5304 Paf1 958.41333 Paf2 911.733032 Paf3 904.816 Heatmap 290.199432 Total loss 3065.16162\n",
      "Epoch 296 Step 590/5304 Paf1 652.867249 Paf2 625.0448 Paf3 627.508362 Heatmap 175.378418 Total loss 2080.79883\n",
      "Epoch 296 Step 600/5304 Paf1 815.899841 Paf2 746.901367 Paf3 753.401794 Heatmap 250.51593 Total loss 2566.71899\n",
      "Epoch 296 Step 610/5304 Paf1 1038.12708 Paf2 991.344238 Paf3 997.057739 Heatmap 328.557434 Total loss 3355.08643\n",
      "Epoch 296 Step 620/5304 Paf1 574.978 Paf2 539.161438 Paf3 528.780457 Heatmap 178.194672 Total loss 1821.1145\n",
      "Epoch 296 Step 630/5304 Paf1 514.767273 Paf2 480.833771 Paf3 466.3862 Heatmap 168.773422 Total loss 1630.76074\n",
      "Epoch 296 Step 640/5304 Paf1 634.924683 Paf2 599.778137 Paf3 606.603943 Heatmap 186.846802 Total loss 2028.15356\n",
      "Epoch 296 Step 650/5304 Paf1 634.873047 Paf2 597.62616 Paf3 585.705322 Heatmap 215.833923 Total loss 2034.03857\n",
      "Epoch 296 Step 660/5304 Paf1 556.203674 Paf2 513.714417 Paf3 519.083557 Heatmap 152.762207 Total loss 1741.76392\n",
      "Epoch 296 Step 670/5304 Paf1 456.651276 Paf2 421.908447 Paf3 421.228516 Heatmap 123.772957 Total loss 1423.56116\n",
      "Epoch 296 Step 680/5304 Paf1 534.378174 Paf2 510.790527 Paf3 518.733826 Heatmap 154.18335 Total loss 1718.08594\n",
      "Epoch 296 Step 690/5304 Paf1 828.787415 Paf2 789.675171 Paf3 787.444397 Heatmap 259.993958 Total loss 2665.90088\n",
      "Epoch 296 Step 700/5304 Paf1 607.604675 Paf2 556.794189 Paf3 575.668152 Heatmap 139.152557 Total loss 1879.2196\n",
      "Epoch 296 Step 710/5304 Paf1 740.163696 Paf2 697.900574 Paf3 703.571045 Heatmap 218.938507 Total loss 2360.57373\n",
      "Epoch 296 Step 720/5304 Paf1 839.383179 Paf2 806.364929 Paf3 808.667908 Heatmap 301.122589 Total loss 2755.53857\n",
      "Epoch 296 Step 730/5304 Paf1 735.366211 Paf2 675.858459 Paf3 678.147949 Heatmap 196.855026 Total loss 2286.22754\n",
      "Epoch 296 Step 740/5304 Paf1 1002.87659 Paf2 973.875916 Paf3 973.067322 Heatmap 366.802917 Total loss 3316.62256\n",
      "Epoch 296 Step 750/5304 Paf1 619.296936 Paf2 559.431641 Paf3 561.598877 Heatmap 220.687027 Total loss 1961.0144\n",
      "Epoch 296 Step 760/5304 Paf1 634.179626 Paf2 613.378357 Paf3 606.069214 Heatmap 173.067413 Total loss 2026.69458\n",
      "Epoch 296 Step 770/5304 Paf1 618.165466 Paf2 571.439453 Paf3 574.426453 Heatmap 192.987518 Total loss 1957.01892\n",
      "Epoch 296 Step 780/5304 Paf1 761.907715 Paf2 689.66571 Paf3 720.254944 Heatmap 240.027588 Total loss 2411.85596\n",
      "Epoch 296 Step 790/5304 Paf1 825.223328 Paf2 752.812744 Paf3 765.713806 Heatmap 224.099304 Total loss 2567.84912\n",
      "Epoch 296 Step 800/5304 Paf1 861.590698 Paf2 825.224915 Paf3 833.516357 Heatmap 259.525879 Total loss 2779.85791\n",
      "Epoch 296 Step 810/5304 Paf1 599.669373 Paf2 576.50946 Paf3 580.120789 Heatmap 192.771362 Total loss 1949.07104\n",
      "Epoch 296 Step 820/5304 Paf1 707.580811 Paf2 665.670044 Paf3 680.480347 Heatmap 249.189331 Total loss 2302.92041\n",
      "Epoch 296 Step 830/5304 Paf1 998.829 Paf2 957.508179 Paf3 937.403198 Heatmap 299.117218 Total loss 3192.85742\n",
      "Epoch 296 Step 840/5304 Paf1 779.611206 Paf2 738.673706 Paf3 715.721802 Heatmap 241.615723 Total loss 2475.62256\n",
      "Epoch 296 Step 850/5304 Paf1 519.0849 Paf2 470.125 Paf3 465.764038 Heatmap 137.496384 Total loss 1592.47034\n",
      "Epoch 296 Step 860/5304 Paf1 689.785095 Paf2 638.541443 Paf3 644.309631 Heatmap 169.986206 Total loss 2142.62231\n",
      "Epoch 296 Step 870/5304 Paf1 629.253906 Paf2 564.315857 Paf3 558.408081 Heatmap 222.013046 Total loss 1973.99097\n",
      "Epoch 296 Step 880/5304 Paf1 809.575195 Paf2 779.546143 Paf3 783.936523 Heatmap 301.151794 Total loss 2674.20972\n",
      "Epoch 296 Step 890/5304 Paf1 468.763885 Paf2 445.597412 Paf3 439.999451 Heatmap 135.194046 Total loss 1489.55481\n",
      "Epoch 296 Step 900/5304 Paf1 814.486 Paf2 731.212646 Paf3 727.460876 Heatmap 213.195129 Total loss 2486.35474\n",
      "Epoch 296 Step 910/5304 Paf1 642.639771 Paf2 577.360901 Paf3 584.314941 Heatmap 178.090012 Total loss 1982.40576\n",
      "Epoch 296 Step 920/5304 Paf1 800.987915 Paf2 743.967834 Paf3 743.505554 Heatmap 230.477356 Total loss 2518.93872\n",
      "Epoch 296 Step 930/5304 Paf1 732.217468 Paf2 676.371216 Paf3 682.718384 Heatmap 200.21759 Total loss 2291.52466\n",
      "Epoch 296 Step 940/5304 Paf1 636.695 Paf2 597.692932 Paf3 605.99646 Heatmap 194.705688 Total loss 2035.09009\n",
      "Epoch 296 Step 950/5304 Paf1 732.454285 Paf2 696.778687 Paf3 692.914 Heatmap 214.549469 Total loss 2336.69629\n",
      "Epoch 296 Step 960/5304 Paf1 833.149231 Paf2 806.626465 Paf3 807.324463 Heatmap 264.079102 Total loss 2711.1792\n",
      "Epoch 296 Step 970/5304 Paf1 550.454651 Paf2 490.722321 Paf3 499.483734 Heatmap 168.639282 Total loss 1709.3\n",
      "Epoch 296 Step 980/5304 Paf1 611.779968 Paf2 568.061 Paf3 560.03186 Heatmap 173.987732 Total loss 1913.8606\n",
      "Epoch 296 Step 990/5304 Paf1 883.810669 Paf2 806.52948 Paf3 796.83551 Heatmap 285.332092 Total loss 2772.50781\n",
      "Epoch 296 Step 1000/5304 Paf1 727.656189 Paf2 670.784241 Paf3 684.865417 Heatmap 237.501373 Total loss 2320.80713\n",
      "Saved checkpoint for step 1000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1771\n",
      "Epoch 296 Step 1010/5304 Paf1 666.969727 Paf2 621.103 Paf3 618.748047 Heatmap 193.476074 Total loss 2100.29688\n",
      "Epoch 296 Step 1020/5304 Paf1 921.203369 Paf2 884.699585 Paf3 855.810852 Heatmap 296.381866 Total loss 2958.0957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296 Step 1030/5304 Paf1 599.139587 Paf2 563.569214 Paf3 563.384155 Heatmap 176.973053 Total loss 1903.06592\n",
      "Epoch 296 Step 1040/5304 Paf1 645.850769 Paf2 625.095581 Paf3 618.367371 Heatmap 211.472778 Total loss 2100.78638\n",
      "Epoch 296 Step 1050/5304 Paf1 685.294373 Paf2 656.457581 Paf3 649.77533 Heatmap 228.091278 Total loss 2219.61865\n",
      "Epoch 296 Step 1060/5304 Paf1 433.957 Paf2 389.480072 Paf3 395.127472 Heatmap 116.94838 Total loss 1335.51294\n",
      "Epoch 296 Step 1070/5304 Paf1 549.328125 Paf2 504.16272 Paf3 517.735535 Heatmap 181.167496 Total loss 1752.3938\n",
      "Epoch 296 Step 1080/5304 Paf1 710.69873 Paf2 678.401917 Paf3 695.518127 Heatmap 269.606873 Total loss 2354.22559\n",
      "Epoch 296 Step 1090/5304 Paf1 418.548676 Paf2 392.360413 Paf3 386.022491 Heatmap 108.922623 Total loss 1305.85425\n",
      "Epoch 296 Step 1100/5304 Paf1 826.682434 Paf2 786.1203 Paf3 778.553223 Heatmap 309.867065 Total loss 2701.22314\n",
      "Epoch 296 Step 1110/5304 Paf1 728.996643 Paf2 704.629 Paf3 695.09021 Heatmap 214.639694 Total loss 2343.35571\n",
      "Epoch 296 Step 1120/5304 Paf1 508.48822 Paf2 476.679688 Paf3 455.866913 Heatmap 159.181396 Total loss 1600.21631\n",
      "Epoch 296 Step 1130/5304 Paf1 715.078 Paf2 647.470703 Paf3 634.734802 Heatmap 192.737061 Total loss 2190.02051\n",
      "Epoch 296 Step 1140/5304 Paf1 773.981201 Paf2 707.034302 Paf3 714.441772 Heatmap 264.114197 Total loss 2459.57153\n",
      "Epoch 296 Step 1150/5304 Paf1 573.226074 Paf2 521.834412 Paf3 532.264343 Heatmap 152.81311 Total loss 1780.13794\n",
      "Epoch 296 Step 1160/5304 Paf1 897.877075 Paf2 831.286499 Paf3 840.632202 Heatmap 210.173752 Total loss 2779.96948\n",
      "Epoch 296 Step 1170/5304 Paf1 719.730347 Paf2 689.120178 Paf3 679.742188 Heatmap 271.923218 Total loss 2360.51611\n",
      "Epoch 296 Step 1180/5304 Paf1 646.622131 Paf2 608.872437 Paf3 603.295776 Heatmap 217.922852 Total loss 2076.71338\n",
      "Epoch 296 Step 1190/5304 Paf1 612.204041 Paf2 553.154541 Paf3 560.364502 Heatmap 174.96666 Total loss 1900.68982\n",
      "Epoch 296 Step 1200/5304 Paf1 1231.5199 Paf2 1135.83 Paf3 1132.81958 Heatmap 367.5755 Total loss 3867.74487\n",
      "Epoch 296 Step 1210/5304 Paf1 723.316223 Paf2 688.910889 Paf3 685.317871 Heatmap 213.749588 Total loss 2311.29443\n",
      "Epoch 296 Step 1220/5304 Paf1 920.665405 Paf2 881.280212 Paf3 868.319702 Heatmap 274.351562 Total loss 2944.6167\n",
      "Epoch 296 Step 1230/5304 Paf1 705.349915 Paf2 664.9422 Paf3 670.422729 Heatmap 200.269669 Total loss 2240.98438\n",
      "Epoch 296 Step 1240/5304 Paf1 887.531799 Paf2 842.9422 Paf3 851.535217 Heatmap 270.642517 Total loss 2852.65186\n",
      "Epoch 296 Step 1250/5304 Paf1 864.956909 Paf2 806.186218 Paf3 812.278076 Heatmap 276.245697 Total loss 2759.66699\n",
      "Epoch 296 Step 1260/5304 Paf1 1186.37854 Paf2 1161.45569 Paf3 1158.57263 Heatmap 427.752502 Total loss 3934.15942\n",
      "Epoch 296 Step 1270/5304 Paf1 697.997681 Paf2 655.207153 Paf3 649.413 Heatmap 204.108398 Total loss 2206.72632\n",
      "Epoch 296 Step 1280/5304 Paf1 598.988831 Paf2 558.655518 Paf3 565.470337 Heatmap 186.235825 Total loss 1909.35046\n",
      "Epoch 296 Step 1290/5304 Paf1 1117.73096 Paf2 1049.02332 Paf3 1064.20776 Heatmap 395.011292 Total loss 3625.97339\n",
      "Epoch 296 Step 1300/5304 Paf1 666.326538 Paf2 636.608826 Paf3 624.800537 Heatmap 202.619827 Total loss 2130.35571\n",
      "Epoch 296 Step 1310/5304 Paf1 804.871887 Paf2 765.880066 Paf3 795.463623 Heatmap 275.847 Total loss 2642.0625\n",
      "Epoch 296 Step 1320/5304 Paf1 619.053528 Paf2 576.585876 Paf3 593.037 Heatmap 203.432648 Total loss 1992.10901\n",
      "Epoch 296 Step 1330/5304 Paf1 514.640625 Paf2 494.437866 Paf3 486.013367 Heatmap 149.15036 Total loss 1644.24219\n",
      "Epoch 296 Step 1340/5304 Paf1 659.077698 Paf2 629.675293 Paf3 630.633667 Heatmap 248.238297 Total loss 2167.625\n",
      "Epoch 296 Step 1350/5304 Paf1 869.418152 Paf2 820.010498 Paf3 826.480591 Heatmap 236.68924 Total loss 2752.59863\n",
      "Epoch 296 Step 1360/5304 Paf1 681.785 Paf2 636.114258 Paf3 625.22113 Heatmap 202.379791 Total loss 2145.5\n",
      "Epoch 296 Step 1370/5304 Paf1 472.371613 Paf2 445.259308 Paf3 445.591095 Heatmap 138.501709 Total loss 1501.72363\n",
      "Epoch 296 Step 1380/5304 Paf1 577.608765 Paf2 538.014893 Paf3 543.042847 Heatmap 195.490051 Total loss 1854.15649\n",
      "Epoch 296 Step 1390/5304 Paf1 592.556152 Paf2 552.052 Paf3 560.498413 Heatmap 168.192459 Total loss 1873.29907\n",
      "Epoch 296 Step 1400/5304 Paf1 671.806702 Paf2 636.424561 Paf3 630.729553 Heatmap 212.481628 Total loss 2151.44238\n",
      "Epoch 296 Step 1410/5304 Paf1 792.940857 Paf2 721.412537 Paf3 727.912292 Heatmap 263.246552 Total loss 2505.51221\n",
      "Epoch 296 Step 1420/5304 Paf1 944.052368 Paf2 881.637634 Paf3 894.20105 Heatmap 351.260498 Total loss 3071.15137\n",
      "Epoch 296 Step 1430/5304 Paf1 654.038208 Paf2 596.975708 Paf3 616.259277 Heatmap 192.764832 Total loss 2060.03809\n",
      "Epoch 296 Step 1440/5304 Paf1 1014.74 Paf2 947.219604 Paf3 974.257568 Heatmap 281.975464 Total loss 3218.19263\n",
      "Epoch 296 Step 1450/5304 Paf1 560.065491 Paf2 506.96225 Paf3 524.179138 Heatmap 159.091049 Total loss 1750.29785\n",
      "Epoch 296 Step 1460/5304 Paf1 751.181458 Paf2 705.606445 Paf3 706.007141 Heatmap 266.024902 Total loss 2428.81982\n",
      "Epoch 296 Step 1470/5304 Paf1 897.82489 Paf2 851.442871 Paf3 858.968872 Heatmap 310.49762 Total loss 2918.73438\n",
      "Epoch 296 Step 1480/5304 Paf1 571.438599 Paf2 522.347839 Paf3 522.652222 Heatmap 158.719879 Total loss 1775.15845\n",
      "Epoch 296 Step 1490/5304 Paf1 622.380493 Paf2 609.887512 Paf3 617.375793 Heatmap 186.444077 Total loss 2036.08789\n",
      "Epoch 296 Step 1500/5304 Paf1 945.945129 Paf2 868.384705 Paf3 876.884888 Heatmap 322.068 Total loss 3013.28271\n",
      "Epoch 296 Step 1510/5304 Paf1 689.690186 Paf2 663.00293 Paf3 669.109558 Heatmap 238.01265 Total loss 2259.81543\n",
      "Epoch 296 Step 1520/5304 Paf1 594.898071 Paf2 560.322205 Paf3 566.951904 Heatmap 186.682114 Total loss 1908.85425\n",
      "Epoch 296 Step 1530/5304 Paf1 705.746887 Paf2 666.356689 Paf3 651.288879 Heatmap 252.086472 Total loss 2275.479\n",
      "Epoch 296 Step 1540/5304 Paf1 678.661438 Paf2 622.637085 Paf3 609.07843 Heatmap 206.783142 Total loss 2117.16016\n",
      "Epoch 296 Step 1550/5304 Paf1 746.854858 Paf2 697.649841 Paf3 678.484314 Heatmap 219.219666 Total loss 2342.2085\n",
      "Epoch 296 Step 1560/5304 Paf1 605.500183 Paf2 583.911 Paf3 579.666687 Heatmap 196.375427 Total loss 1965.45325\n",
      "Epoch 296 Step 1570/5304 Paf1 649.068176 Paf2 617.582397 Paf3 612.001038 Heatmap 207.158539 Total loss 2085.81\n",
      "Epoch 296 Step 1580/5304 Paf1 764.24646 Paf2 733.367737 Paf3 722.494751 Heatmap 260.369446 Total loss 2480.47852\n",
      "Epoch 296 Step 1590/5304 Paf1 780.214355 Paf2 714.785156 Paf3 724.420471 Heatmap 251.655106 Total loss 2471.0752\n",
      "Epoch 296 Step 1600/5304 Paf1 655.428589 Paf2 608.051575 Paf3 600.078918 Heatmap 209.899719 Total loss 2073.45898\n",
      "Epoch 296 Step 1610/5304 Paf1 545.100647 Paf2 496.88205 Paf3 511.30777 Heatmap 166.961456 Total loss 1720.25195\n",
      "Epoch 296 Step 1620/5304 Paf1 727.426697 Paf2 679.448059 Paf3 686.207336 Heatmap 192.687927 Total loss 2285.77\n",
      "Epoch 296 Step 1630/5304 Paf1 678.466797 Paf2 635.669067 Paf3 666.079 Heatmap 198.322189 Total loss 2178.53711\n",
      "Epoch 296 Step 1640/5304 Paf1 729.680176 Paf2 665.683411 Paf3 663.981628 Heatmap 190.525848 Total loss 2249.87109\n",
      "Epoch 296 Step 1650/5304 Paf1 753.657 Paf2 716.239868 Paf3 705.145264 Heatmap 219.819901 Total loss 2394.86206\n",
      "Epoch 296 Step 1660/5304 Paf1 494.400513 Paf2 484.904907 Paf3 483.319672 Heatmap 147.912338 Total loss 1610.53735\n",
      "Epoch 296 Step 1670/5304 Paf1 768.95282 Paf2 729.09375 Paf3 739.137573 Heatmap 250.321564 Total loss 2487.50586\n",
      "Epoch 296 Step 1680/5304 Paf1 887.779236 Paf2 863.813904 Paf3 873.803589 Heatmap 292.360901 Total loss 2917.75781\n",
      "Epoch 296 Step 1690/5304 Paf1 495.658478 Paf2 452.508362 Paf3 449.607666 Heatmap 141.486984 Total loss 1539.26147\n",
      "Epoch 296 Step 1700/5304 Paf1 910.929138 Paf2 850.985229 Paf3 854.101746 Heatmap 365.272949 Total loss 2981.28906\n",
      "Epoch 296 Step 1710/5304 Paf1 1040.63269 Paf2 993.250732 Paf3 995.325684 Heatmap 386.488525 Total loss 3415.69775\n",
      "Epoch 296 Step 1720/5304 Paf1 826.66394 Paf2 767.972595 Paf3 772.347168 Heatmap 295.125458 Total loss 2662.10913\n",
      "Epoch 296 Step 1730/5304 Paf1 591.338623 Paf2 563.243 Paf3 566.69928 Heatmap 215.145096 Total loss 1936.4259\n",
      "Epoch 296 Step 1740/5304 Paf1 591.168152 Paf2 594.676453 Paf3 574.5047 Heatmap 178.208939 Total loss 1938.55823\n",
      "Epoch 296 Step 1750/5304 Paf1 1014.50415 Paf2 953.314636 Paf3 963.235718 Heatmap 356.88446 Total loss 3287.93896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296 Step 1760/5304 Paf1 636.308411 Paf2 609.960083 Paf3 608.659058 Heatmap 184.809082 Total loss 2039.73669\n",
      "Epoch 296 Step 1770/5304 Paf1 510.379913 Paf2 472.897766 Paf3 486.93396 Heatmap 161.031982 Total loss 1631.24365\n",
      "Epoch 296 Step 1780/5304 Paf1 636.123535 Paf2 579.634888 Paf3 603.642639 Heatmap 212.597855 Total loss 2031.9989\n",
      "Epoch 296 Step 1790/5304 Paf1 1173.75989 Paf2 1095.31226 Paf3 1084.51392 Heatmap 393.833 Total loss 3747.41919\n",
      "Epoch 296 Step 1800/5304 Paf1 538.627441 Paf2 493.128326 Paf3 480.766083 Heatmap 156.987106 Total loss 1669.50891\n",
      "Epoch 296 Step 1810/5304 Paf1 1099.32556 Paf2 1031.87915 Paf3 1041.36853 Heatmap 359.380859 Total loss 3531.9541\n",
      "Epoch 296 Step 1820/5304 Paf1 739.872192 Paf2 702.449 Paf3 719.763 Heatmap 235.330673 Total loss 2397.41479\n",
      "Epoch 296 Step 1830/5304 Paf1 614.834412 Paf2 575.300293 Paf3 554.694214 Heatmap 168.957703 Total loss 1913.78662\n",
      "Epoch 296 Step 1840/5304 Paf1 488.445312 Paf2 453.183624 Paf3 445.481323 Heatmap 155.050934 Total loss 1542.16113\n",
      "Epoch 296 Step 1850/5304 Paf1 588.745728 Paf2 524.030273 Paf3 530.724 Heatmap 182.52243 Total loss 1826.02246\n",
      "Epoch 296 Step 1860/5304 Paf1 524.652161 Paf2 483.590515 Paf3 491.634735 Heatmap 166.580734 Total loss 1666.45813\n",
      "Epoch 296 Step 1870/5304 Paf1 734.708496 Paf2 672.075439 Paf3 677.802307 Heatmap 206.209061 Total loss 2290.79541\n",
      "Epoch 296 Step 1880/5304 Paf1 806.513428 Paf2 764.873047 Paf3 741.384 Heatmap 297.72583 Total loss 2610.49634\n",
      "Epoch 296 Step 1890/5304 Paf1 611.06781 Paf2 591.501465 Paf3 572.321167 Heatmap 217.559799 Total loss 1992.45032\n",
      "Epoch 296 Step 1900/5304 Paf1 535.482788 Paf2 504.965179 Paf3 479.759308 Heatmap 165.345795 Total loss 1685.5531\n",
      "Epoch 296 Step 1910/5304 Paf1 807.008301 Paf2 749.733887 Paf3 738.851501 Heatmap 239.340607 Total loss 2534.93433\n",
      "Epoch 296 Step 1920/5304 Paf1 954.493713 Paf2 891.269775 Paf3 906.318054 Heatmap 307.781189 Total loss 3059.86279\n",
      "Epoch 296 Step 1930/5304 Paf1 735.591 Paf2 692.967041 Paf3 699.288391 Heatmap 221.628479 Total loss 2349.4751\n",
      "Epoch 296 Step 1940/5304 Paf1 700.895447 Paf2 653.934875 Paf3 651.360229 Heatmap 176.121552 Total loss 2182.31201\n",
      "Epoch 296 Step 1950/5304 Paf1 839.835754 Paf2 816.569092 Paf3 773.644165 Heatmap 298.199707 Total loss 2728.24854\n",
      "Epoch 296 Step 1960/5304 Paf1 817.788574 Paf2 762.263916 Paf3 776.663086 Heatmap 305.009705 Total loss 2661.72534\n",
      "Epoch 296 Step 1970/5304 Paf1 729.474121 Paf2 663.813843 Paf3 673.784607 Heatmap 212.884521 Total loss 2279.95703\n",
      "Epoch 296 Step 1980/5304 Paf1 771.180237 Paf2 750.616821 Paf3 740.879211 Heatmap 246.43161 Total loss 2509.10791\n",
      "Epoch 296 Step 1990/5304 Paf1 674.75946 Paf2 629.053406 Paf3 615.354187 Heatmap 192.019424 Total loss 2111.18652\n",
      "Epoch 296 Step 2000/5304 Paf1 826.444275 Paf2 803.33429 Paf3 797.008667 Heatmap 274.010925 Total loss 2700.7981\n",
      "Saved checkpoint for step 2000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1772\n",
      "Epoch 296 Step 2010/5304 Paf1 843.170532 Paf2 795.56781 Paf3 786.395508 Heatmap 236.182495 Total loss 2661.31641\n",
      "Epoch 296 Step 2020/5304 Paf1 658.644653 Paf2 636.282288 Paf3 636.129333 Heatmap 209.957581 Total loss 2141.01392\n",
      "Epoch 296 Step 2030/5304 Paf1 689.852966 Paf2 652.171753 Paf3 643.459839 Heatmap 201.511978 Total loss 2186.99658\n",
      "Epoch 296 Step 2040/5304 Paf1 599.830139 Paf2 529.673645 Paf3 544.745056 Heatmap 155.70668 Total loss 1829.95557\n",
      "Epoch 296 Step 2050/5304 Paf1 567.019165 Paf2 524.628052 Paf3 519.830139 Heatmap 161.455444 Total loss 1772.93286\n",
      "Epoch 296 Step 2060/5304 Paf1 529.056946 Paf2 503.250977 Paf3 495.378479 Heatmap 126.957809 Total loss 1654.64417\n",
      "Epoch 296 Step 2070/5304 Paf1 700.007751 Paf2 658.469238 Paf3 656.485962 Heatmap 187.75592 Total loss 2202.71899\n",
      "Epoch 296 Step 2080/5304 Paf1 645.31 Paf2 604.155457 Paf3 602.916 Heatmap 179.994781 Total loss 2032.37622\n",
      "Epoch 296 Step 2090/5304 Paf1 693.640625 Paf2 654.589172 Paf3 647.416809 Heatmap 222.733154 Total loss 2218.37964\n",
      "Epoch 296 Step 2100/5304 Paf1 756.917358 Paf2 714.912415 Paf3 718.149658 Heatmap 249.030579 Total loss 2439.01\n",
      "Epoch 296 Step 2110/5304 Paf1 654.404846 Paf2 596.931824 Paf3 609.283386 Heatmap 188.399231 Total loss 2049.01929\n",
      "Epoch 296 Step 2120/5304 Paf1 457.411316 Paf2 420.279785 Paf3 406.428894 Heatmap 124.324142 Total loss 1408.44409\n",
      "Epoch 296 Step 2130/5304 Paf1 670.598572 Paf2 615.904114 Paf3 629.221191 Heatmap 182.704651 Total loss 2098.42847\n",
      "Epoch 296 Step 2140/5304 Paf1 603.379761 Paf2 568.994 Paf3 590.573547 Heatmap 183.152237 Total loss 1946.09961\n",
      "Epoch 296 Step 2150/5304 Paf1 539.935669 Paf2 496.517365 Paf3 503.21875 Heatmap 158.927811 Total loss 1698.59961\n",
      "Epoch 296 Step 2160/5304 Paf1 558.901184 Paf2 496.510803 Paf3 512.485168 Heatmap 166.541962 Total loss 1734.43909\n",
      "Epoch 296 Step 2170/5304 Paf1 554.000793 Paf2 503.974091 Paf3 482.832153 Heatmap 166.174469 Total loss 1706.98145\n",
      "Epoch 296 Step 2180/5304 Paf1 927.762695 Paf2 899.835571 Paf3 892.271423 Heatmap 260.163666 Total loss 2980.0332\n",
      "Epoch 296 Step 2190/5304 Paf1 700.479126 Paf2 663.858704 Paf3 661.967285 Heatmap 251.618225 Total loss 2277.92334\n",
      "Epoch 296 Step 2200/5304 Paf1 700.713684 Paf2 653.984802 Paf3 667.851929 Heatmap 262.519592 Total loss 2285.07\n",
      "Epoch 296 Step 2210/5304 Paf1 553.508362 Paf2 504.223663 Paf3 508.560455 Heatmap 135.372513 Total loss 1701.66504\n",
      "Epoch 296 Step 2220/5304 Paf1 554.136719 Paf2 532.532043 Paf3 515.195679 Heatmap 176.447464 Total loss 1778.31177\n",
      "Epoch 296 Step 2230/5304 Paf1 1069.52283 Paf2 1010.79443 Paf3 981.29 Heatmap 345.634583 Total loss 3407.24194\n",
      "Epoch 296 Step 2240/5304 Paf1 564.29126 Paf2 536.102844 Paf3 541.661865 Heatmap 209.004379 Total loss 1851.0603\n",
      "Epoch 296 Step 2250/5304 Paf1 510.482941 Paf2 462.51358 Paf3 466.008667 Heatmap 143.743774 Total loss 1582.74902\n",
      "Epoch 296 Step 2260/5304 Paf1 422.219238 Paf2 388.001526 Paf3 384.462799 Heatmap 113.526627 Total loss 1308.21021\n",
      "Epoch 296 Step 2270/5304 Paf1 653.268127 Paf2 610.03418 Paf3 610.810852 Heatmap 202.222412 Total loss 2076.33545\n",
      "Epoch 296 Step 2280/5304 Paf1 414.936768 Paf2 376.937 Paf3 386.428223 Heatmap 102.276527 Total loss 1280.57849\n",
      "Epoch 296 Step 2290/5304 Paf1 548.236511 Paf2 513.269104 Paf3 509.529022 Heatmap 134.130356 Total loss 1705.16504\n",
      "Epoch 296 Step 2300/5304 Paf1 629.005798 Paf2 574.313965 Paf3 585.042725 Heatmap 191.164062 Total loss 1979.52661\n",
      "Epoch 296 Step 2310/5304 Paf1 1027.05676 Paf2 972.322937 Paf3 959.506714 Heatmap 345.496216 Total loss 3304.38257\n",
      "Epoch 296 Step 2320/5304 Paf1 567.178467 Paf2 531.263428 Paf3 517.943054 Heatmap 139.715424 Total loss 1756.10034\n",
      "Epoch 296 Step 2330/5304 Paf1 653.80957 Paf2 623.774109 Paf3 623.440063 Heatmap 227.649094 Total loss 2128.67285\n",
      "Epoch 296 Step 2340/5304 Paf1 530.177673 Paf2 498.9104 Paf3 502.448883 Heatmap 167.77771 Total loss 1699.3147\n",
      "Epoch 296 Step 2350/5304 Paf1 596.231262 Paf2 548.567505 Paf3 536.258545 Heatmap 169.925751 Total loss 1850.98315\n",
      "Epoch 296 Step 2360/5304 Paf1 458.333557 Paf2 435.481628 Paf3 422.707703 Heatmap 169.144531 Total loss 1485.66748\n",
      "Epoch 296 Step 2370/5304 Paf1 660.358 Paf2 605.084534 Paf3 618.775513 Heatmap 189.37619 Total loss 2073.59424\n",
      "Epoch 296 Step 2380/5304 Paf1 561.984497 Paf2 523.838745 Paf3 532.57666 Heatmap 214.38382 Total loss 1832.78369\n",
      "Epoch 296 Step 2390/5304 Paf1 808.175598 Paf2 750.885559 Paf3 744.754211 Heatmap 251.622192 Total loss 2555.4375\n",
      "Epoch 296 Step 2400/5304 Paf1 650.773071 Paf2 608.285339 Paf3 604.775208 Heatmap 186.99707 Total loss 2050.83057\n",
      "Epoch 296 Step 2410/5304 Paf1 479.881409 Paf2 462.101746 Paf3 458.486633 Heatmap 160.024734 Total loss 1560.49451\n",
      "Epoch 296 Step 2420/5304 Paf1 464.447723 Paf2 424.617096 Paf3 441.114105 Heatmap 157.790802 Total loss 1487.96973\n",
      "Epoch 296 Step 2430/5304 Paf1 475.88797 Paf2 420.118042 Paf3 433.463257 Heatmap 144.923309 Total loss 1474.39258\n",
      "Epoch 296 Step 2440/5304 Paf1 775.265259 Paf2 738.108948 Paf3 739.964966 Heatmap 213.223587 Total loss 2466.56274\n",
      "Epoch 296 Step 2450/5304 Paf1 750.31012 Paf2 697.907471 Paf3 713.204224 Heatmap 260.14563 Total loss 2421.56738\n",
      "Epoch 296 Step 2460/5304 Paf1 422.517334 Paf2 402.308807 Paf3 395.656128 Heatmap 117.625061 Total loss 1338.10742\n",
      "Epoch 296 Step 2470/5304 Paf1 718.144653 Paf2 685.685303 Paf3 680.511841 Heatmap 210.725372 Total loss 2295.06714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296 Step 2480/5304 Paf1 531.61261 Paf2 516.083069 Paf3 506.234833 Heatmap 163.727463 Total loss 1717.65796\n",
      "Epoch 296 Step 2490/5304 Paf1 764.278198 Paf2 691.050171 Paf3 722.648438 Heatmap 219.420349 Total loss 2397.39722\n",
      "Epoch 296 Step 2500/5304 Paf1 430.847168 Paf2 388.969849 Paf3 392.139587 Heatmap 107.462585 Total loss 1319.41919\n",
      "Epoch 296 Step 2510/5304 Paf1 647.407349 Paf2 600.926453 Paf3 606.187 Heatmap 184.763336 Total loss 2039.28406\n",
      "Epoch 296 Step 2520/5304 Paf1 446.358643 Paf2 437.456482 Paf3 429.917206 Heatmap 133.1996 Total loss 1446.93188\n",
      "Epoch 296 Step 2530/5304 Paf1 601.575684 Paf2 566.860413 Paf3 568.663635 Heatmap 203.345413 Total loss 1940.44507\n",
      "Epoch 296 Step 2540/5304 Paf1 803.931641 Paf2 743.029663 Paf3 737.931335 Heatmap 288.738647 Total loss 2573.63135\n",
      "Epoch 296 Step 2550/5304 Paf1 537.017334 Paf2 500.794159 Paf3 496.450256 Heatmap 148.855972 Total loss 1683.11768\n",
      "Epoch 296 Step 2560/5304 Paf1 523.748291 Paf2 508.683899 Paf3 511.946686 Heatmap 152.039551 Total loss 1696.41833\n",
      "Epoch 296 Step 2570/5304 Paf1 996.804688 Paf2 947.199585 Paf3 952.253052 Heatmap 295.666595 Total loss 3191.92383\n",
      "Epoch 296 Step 2580/5304 Paf1 718.227173 Paf2 671.087769 Paf3 668.775757 Heatmap 195.187927 Total loss 2253.27856\n",
      "Epoch 296 Step 2590/5304 Paf1 672.773804 Paf2 611.748291 Paf3 632.783508 Heatmap 191.438477 Total loss 2108.74414\n",
      "Epoch 296 Step 2600/5304 Paf1 805.624268 Paf2 753.452209 Paf3 749.636353 Heatmap 225.300323 Total loss 2534.01318\n",
      "Epoch 296 Step 2610/5304 Paf1 625.747559 Paf2 609.251465 Paf3 608.999695 Heatmap 200.943939 Total loss 2044.94263\n",
      "Epoch 296 Step 2620/5304 Paf1 481.719971 Paf2 449.660431 Paf3 434.528748 Heatmap 130.988876 Total loss 1496.89795\n",
      "Epoch 296 Step 2630/5304 Paf1 823.324585 Paf2 773.673889 Paf3 764.492371 Heatmap 259.799194 Total loss 2621.29\n",
      "Epoch 296 Step 2640/5304 Paf1 654.111206 Paf2 606.311157 Paf3 599.484314 Heatmap 179.606949 Total loss 2039.51367\n",
      "Epoch 296 Step 2650/5304 Paf1 386.863098 Paf2 359.709839 Paf3 337.118164 Heatmap 127.434593 Total loss 1211.12573\n",
      "Epoch 296 Step 2660/5304 Paf1 882.241272 Paf2 844.889282 Paf3 842.946228 Heatmap 276.160706 Total loss 2846.23755\n",
      "Epoch 296 Step 2670/5304 Paf1 715.644409 Paf2 683.526733 Paf3 686.644165 Heatmap 230.47728 Total loss 2316.29248\n",
      "Epoch 296 Step 2680/5304 Paf1 630.635254 Paf2 576.132629 Paf3 600.483643 Heatmap 164.093048 Total loss 1971.34448\n",
      "Epoch 296 Step 2690/5304 Paf1 743.777 Paf2 703.842163 Paf3 695.541687 Heatmap 267.727142 Total loss 2410.88794\n",
      "Epoch 296 Step 2700/5304 Paf1 884.898193 Paf2 852.835205 Paf3 843.900574 Heatmap 284.22522 Total loss 2865.85913\n",
      "Epoch 296 Step 2710/5304 Paf1 632.829346 Paf2 596.487427 Paf3 587.891479 Heatmap 217.093079 Total loss 2034.30127\n",
      "Epoch 296 Step 2720/5304 Paf1 761.769043 Paf2 721.852722 Paf3 726.255798 Heatmap 231.479 Total loss 2441.35669\n",
      "Epoch 296 Step 2730/5304 Paf1 683.854065 Paf2 649.175415 Paf3 646.816223 Heatmap 181.045593 Total loss 2160.89136\n",
      "Epoch 296 Step 2740/5304 Paf1 720.390686 Paf2 670.776489 Paf3 660.48877 Heatmap 193.091492 Total loss 2244.74756\n",
      "Epoch 296 Step 2750/5304 Paf1 823.105042 Paf2 793.205444 Paf3 795.094482 Heatmap 264.938263 Total loss 2676.34326\n",
      "Epoch 296 Step 2760/5304 Paf1 838.370544 Paf2 813.333 Paf3 801.190735 Heatmap 280.756073 Total loss 2733.65039\n",
      "Epoch 296 Step 2770/5304 Paf1 689.202209 Paf2 685.917847 Paf3 674.028809 Heatmap 182.549011 Total loss 2231.698\n",
      "Epoch 296 Step 2780/5304 Paf1 557.085083 Paf2 530.621521 Paf3 546.667664 Heatmap 181.254486 Total loss 1815.62866\n",
      "Epoch 296 Step 2790/5304 Paf1 532.705 Paf2 502.414215 Paf3 499.734863 Heatmap 159.276245 Total loss 1694.13037\n",
      "Epoch 296 Step 2800/5304 Paf1 717.762207 Paf2 662.406433 Paf3 663.927063 Heatmap 208.559296 Total loss 2252.65503\n",
      "Epoch 296 Step 2810/5304 Paf1 679.626099 Paf2 642.554504 Paf3 630.823608 Heatmap 205.512421 Total loss 2158.5166\n",
      "Epoch 296 Step 2820/5304 Paf1 1038.13733 Paf2 988.709351 Paf3 1007.5462 Heatmap 338.777527 Total loss 3373.17041\n",
      "Epoch 296 Step 2830/5304 Paf1 761.26947 Paf2 732.837219 Paf3 724.038696 Heatmap 210.338287 Total loss 2428.48364\n",
      "Epoch 296 Step 2840/5304 Paf1 768.486084 Paf2 700.700134 Paf3 710.495361 Heatmap 200.874985 Total loss 2380.55664\n",
      "Epoch 296 Step 2850/5304 Paf1 602.479553 Paf2 559.200378 Paf3 555.282043 Heatmap 164.58757 Total loss 1881.54956\n",
      "Epoch 296 Step 2860/5304 Paf1 880.957764 Paf2 855.069031 Paf3 865.100525 Heatmap 232.972839 Total loss 2834.1\n",
      "Epoch 296 Step 2870/5304 Paf1 870.669128 Paf2 819.741455 Paf3 815.763428 Heatmap 252.938202 Total loss 2759.1123\n",
      "Epoch 296 Step 2880/5304 Paf1 1095.7616 Paf2 1012.36292 Paf3 1016.27765 Heatmap 400.87677 Total loss 3525.27881\n",
      "Epoch 296 Step 2890/5304 Paf1 439.047485 Paf2 412.679108 Paf3 399.2052 Heatmap 125.140495 Total loss 1376.07227\n",
      "Epoch 296 Step 2900/5304 Paf1 811.777 Paf2 789.875549 Paf3 797.884949 Heatmap 303.454895 Total loss 2702.99243\n",
      "Epoch 296 Step 2910/5304 Paf1 753.14032 Paf2 691.946 Paf3 706.215637 Heatmap 218.458038 Total loss 2369.76\n",
      "Epoch 296 Step 2920/5304 Paf1 575.287659 Paf2 596.696045 Paf3 594.081604 Heatmap 177.413116 Total loss 1943.47839\n",
      "Epoch 296 Step 2930/5304 Paf1 779.939453 Paf2 718.383484 Paf3 708.43 Heatmap 239.932632 Total loss 2446.68555\n",
      "Epoch 296 Step 2940/5304 Paf1 386.859558 Paf2 359.805145 Paf3 359.685028 Heatmap 109.468315 Total loss 1215.81799\n",
      "Epoch 296 Step 2950/5304 Paf1 551.24054 Paf2 521.322632 Paf3 506.575348 Heatmap 153.4384 Total loss 1732.5769\n",
      "Epoch 296 Step 2960/5304 Paf1 825.666504 Paf2 774.142334 Paf3 749.021912 Heatmap 241.067535 Total loss 2589.89844\n",
      "Epoch 296 Step 2970/5304 Paf1 779.388245 Paf2 747.398132 Paf3 741.29187 Heatmap 225.388458 Total loss 2493.4668\n",
      "Epoch 296 Step 2980/5304 Paf1 678.83667 Paf2 627.137268 Paf3 634.680176 Heatmap 235.29541 Total loss 2175.94946\n",
      "Epoch 296 Step 2990/5304 Paf1 600.355713 Paf2 600.037 Paf3 570.330444 Heatmap 160.514923 Total loss 1931.23804\n",
      "Epoch 296 Step 3000/5304 Paf1 635.0047 Paf2 594.84082 Paf3 592.172119 Heatmap 216.686432 Total loss 2038.70398\n",
      "Saved checkpoint for step 3000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1773\n",
      "Epoch 296 Step 3010/5304 Paf1 417.012115 Paf2 387.72403 Paf3 375.51825 Heatmap 122.277855 Total loss 1302.53223\n",
      "Epoch 296 Step 3020/5304 Paf1 867.348511 Paf2 823.059265 Paf3 831.551941 Heatmap 293.280212 Total loss 2815.23975\n",
      "Epoch 296 Step 3030/5304 Paf1 631.326538 Paf2 587.189514 Paf3 586.04541 Heatmap 167.55249 Total loss 1972.11401\n",
      "Epoch 296 Step 3040/5304 Paf1 451.092194 Paf2 421.268036 Paf3 411.541565 Heatmap 126.143379 Total loss 1410.04517\n",
      "Epoch 296 Step 3050/5304 Paf1 1078.61938 Paf2 1017.23645 Paf3 1020.42841 Heatmap 316.692169 Total loss 3432.97656\n",
      "Epoch 296 Step 3060/5304 Paf1 585.234497 Paf2 550.207214 Paf3 542.974915 Heatmap 210.974747 Total loss 1889.39136\n",
      "Epoch 296 Step 3070/5304 Paf1 509.198059 Paf2 505.829163 Paf3 502.495789 Heatmap 165.326874 Total loss 1682.84985\n",
      "Epoch 296 Step 3080/5304 Paf1 464.009094 Paf2 426.634399 Paf3 432.069824 Heatmap 126.320267 Total loss 1449.03357\n",
      "Epoch 296 Step 3090/5304 Paf1 756.810181 Paf2 712.770447 Paf3 712.680725 Heatmap 272.431519 Total loss 2454.69287\n",
      "Epoch 296 Step 3100/5304 Paf1 673.861694 Paf2 637.415344 Paf3 647.970459 Heatmap 188.096817 Total loss 2147.34424\n",
      "Epoch 296 Step 3110/5304 Paf1 663.503296 Paf2 591.841614 Paf3 603.694824 Heatmap 217.352386 Total loss 2076.39209\n",
      "Epoch 296 Step 3120/5304 Paf1 719.744 Paf2 677.987305 Paf3 682.281 Heatmap 242.946182 Total loss 2322.9585\n",
      "Epoch 296 Step 3130/5304 Paf1 717.342834 Paf2 673.646484 Paf3 684.69342 Heatmap 214.540634 Total loss 2290.22339\n",
      "Epoch 296 Step 3140/5304 Paf1 879.598145 Paf2 831.314514 Paf3 812.449036 Heatmap 283.762451 Total loss 2807.12402\n",
      "Epoch 296 Step 3150/5304 Paf1 441.46405 Paf2 430.956665 Paf3 421.546509 Heatmap 160.883759 Total loss 1454.85107\n",
      "Epoch 296 Step 3160/5304 Paf1 759.694641 Paf2 744.269775 Paf3 755.7453 Heatmap 282.610352 Total loss 2542.32\n",
      "Epoch 296 Step 3170/5304 Paf1 723.878296 Paf2 645.014832 Paf3 639.895142 Heatmap 205.893982 Total loss 2214.68213\n",
      "Epoch 296 Step 3180/5304 Paf1 758.727051 Paf2 723.081116 Paf3 719.029053 Heatmap 234.637665 Total loss 2435.47485\n",
      "Epoch 296 Step 3190/5304 Paf1 963.250183 Paf2 888.687622 Paf3 868.559326 Heatmap 281.30246 Total loss 3001.79956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296 Step 3200/5304 Paf1 686.083 Paf2 662.553833 Paf3 663.240845 Heatmap 215.361908 Total loss 2227.23975\n",
      "Epoch 296 Step 3210/5304 Paf1 599.01239 Paf2 538.974487 Paf3 557.536194 Heatmap 187.545822 Total loss 1883.06885\n",
      "Epoch 296 Step 3220/5304 Paf1 1037.49646 Paf2 957.282471 Paf3 963.404541 Heatmap 350.26886 Total loss 3308.45215\n",
      "Epoch 296 Step 3230/5304 Paf1 825.724426 Paf2 759.328247 Paf3 773.885132 Heatmap 282.478271 Total loss 2641.41602\n",
      "Epoch 296 Step 3240/5304 Paf1 743.358 Paf2 706.293457 Paf3 716.677795 Heatmap 248.73938 Total loss 2415.0686\n",
      "Epoch 296 Step 3250/5304 Paf1 808.767273 Paf2 747.773193 Paf3 730.156067 Heatmap 226.535187 Total loss 2513.23193\n",
      "Epoch 296 Step 3260/5304 Paf1 751.556213 Paf2 716.079468 Paf3 717.724365 Heatmap 235.932953 Total loss 2421.29297\n",
      "Epoch 296 Step 3270/5304 Paf1 806.663818 Paf2 765.53772 Paf3 755.842041 Heatmap 248.673477 Total loss 2576.71704\n",
      "Epoch 296 Step 3280/5304 Paf1 586.204834 Paf2 542.173 Paf3 541.691406 Heatmap 168.443405 Total loss 1838.5127\n",
      "Epoch 296 Step 3290/5304 Paf1 786.631836 Paf2 725.406128 Paf3 720.128967 Heatmap 262.076965 Total loss 2494.2439\n",
      "Epoch 296 Step 3300/5304 Paf1 575.805054 Paf2 516.061646 Paf3 525.98761 Heatmap 147.705872 Total loss 1765.56018\n",
      "Epoch 296 Step 3310/5304 Paf1 568.489807 Paf2 536.0802 Paf3 531.526367 Heatmap 163.952332 Total loss 1800.04883\n",
      "Epoch 296 Step 3320/5304 Paf1 788.854736 Paf2 749.881531 Paf3 744.298462 Heatmap 247.668762 Total loss 2530.70361\n",
      "Epoch 296 Step 3330/5304 Paf1 585.05835 Paf2 548.161499 Paf3 551.119568 Heatmap 171.045578 Total loss 1855.38501\n",
      "Epoch 296 Step 3340/5304 Paf1 684.751709 Paf2 661.768433 Paf3 667.960815 Heatmap 231.034637 Total loss 2245.51562\n",
      "Epoch 296 Step 3350/5304 Paf1 776.455322 Paf2 701.360718 Paf3 716.461792 Heatmap 252.437317 Total loss 2446.71509\n",
      "Epoch 296 Step 3360/5304 Paf1 747.319031 Paf2 706.584717 Paf3 705.530884 Heatmap 236.661636 Total loss 2396.09619\n",
      "Epoch 296 Step 3370/5304 Paf1 683.0849 Paf2 669.615417 Paf3 661.855164 Heatmap 226.594421 Total loss 2241.15\n",
      "Epoch 296 Step 3380/5304 Paf1 558.773743 Paf2 522.329 Paf3 527.364868 Heatmap 161.861847 Total loss 1770.32947\n",
      "Epoch 296 Step 3390/5304 Paf1 646.81134 Paf2 611.329041 Paf3 620.420959 Heatmap 247.416031 Total loss 2125.97729\n",
      "Epoch 296 Step 3400/5304 Paf1 886.684937 Paf2 801.030518 Paf3 791.030518 Heatmap 251.753265 Total loss 2730.49927\n",
      "Epoch 296 Step 3410/5304 Paf1 671.694153 Paf2 601.787903 Paf3 619.975098 Heatmap 190.47876 Total loss 2083.93604\n",
      "Epoch 296 Step 3420/5304 Paf1 658.471313 Paf2 619.907715 Paf3 621.670471 Heatmap 212.578888 Total loss 2112.62842\n",
      "Epoch 296 Step 3430/5304 Paf1 1029.03528 Paf2 966.601562 Paf3 954.803223 Heatmap 331.663574 Total loss 3282.10352\n",
      "Epoch 296 Step 3440/5304 Paf1 734.071655 Paf2 694.447144 Paf3 689.732544 Heatmap 293.418701 Total loss 2411.67\n",
      "Epoch 296 Step 3450/5304 Paf1 639.406555 Paf2 582.347168 Paf3 568.261353 Heatmap 165.141327 Total loss 1955.15637\n",
      "Epoch 296 Step 3460/5304 Paf1 742.274231 Paf2 686.62207 Paf3 698.07373 Heatmap 222.199799 Total loss 2349.17\n",
      "Epoch 296 Step 3470/5304 Paf1 625.304443 Paf2 576.797913 Paf3 575.691833 Heatmap 169.83374 Total loss 1947.62793\n",
      "Epoch 296 Step 3480/5304 Paf1 547.267334 Paf2 488.088562 Paf3 522.560547 Heatmap 144.450226 Total loss 1702.3667\n",
      "Epoch 296 Step 3490/5304 Paf1 601.222717 Paf2 547.975098 Paf3 556.605286 Heatmap 194.325043 Total loss 1900.12805\n",
      "Epoch 296 Step 3500/5304 Paf1 497.227 Paf2 449.868683 Paf3 456.809357 Heatmap 123.096855 Total loss 1527.00195\n",
      "Epoch 296 Step 3510/5304 Paf1 525.956848 Paf2 506.675415 Paf3 506.542358 Heatmap 172.482 Total loss 1711.65674\n",
      "Epoch 296 Step 3520/5304 Paf1 629.031921 Paf2 582.600708 Paf3 581.916 Heatmap 169.793396 Total loss 1963.34204\n",
      "Epoch 296 Step 3530/5304 Paf1 677.882385 Paf2 606.798523 Paf3 628.167969 Heatmap 185.592529 Total loss 2098.44141\n",
      "Epoch 296 Step 3540/5304 Paf1 541.219604 Paf2 501.864258 Paf3 528.767212 Heatmap 202.264618 Total loss 1774.11572\n",
      "Epoch 296 Step 3550/5304 Paf1 537.769775 Paf2 478.682251 Paf3 485.785461 Heatmap 148.260254 Total loss 1650.4978\n",
      "Epoch 296 Step 3560/5304 Paf1 748.155518 Paf2 694.625244 Paf3 683.481445 Heatmap 212.887268 Total loss 2339.14941\n",
      "Epoch 296 Step 3570/5304 Paf1 907.568115 Paf2 836.882202 Paf3 846.189575 Heatmap 246.101181 Total loss 2836.74121\n",
      "Epoch 296 Step 3580/5304 Paf1 780.357422 Paf2 739.809814 Paf3 725.358154 Heatmap 258.05304 Total loss 2503.57837\n",
      "Epoch 296 Step 3590/5304 Paf1 643.218567 Paf2 588.609192 Paf3 595.567749 Heatmap 188.787949 Total loss 2016.18347\n",
      "Epoch 296 Step 3600/5304 Paf1 583.054138 Paf2 528.833191 Paf3 564.089539 Heatmap 184.187469 Total loss 1860.16431\n",
      "Epoch 296 Step 3610/5304 Paf1 827.587 Paf2 799.967041 Paf3 810.140259 Heatmap 287.598755 Total loss 2725.29297\n",
      "Epoch 296 Step 3620/5304 Paf1 821.404663 Paf2 769.610168 Paf3 762.063416 Heatmap 276.438934 Total loss 2629.51709\n",
      "Epoch 296 Step 3630/5304 Paf1 559.579651 Paf2 511.900604 Paf3 523.460205 Heatmap 158.586243 Total loss 1753.52661\n",
      "Epoch 296 Step 3640/5304 Paf1 914.161316 Paf2 875.776245 Paf3 878.962769 Heatmap 262.589478 Total loss 2931.48975\n",
      "Epoch 296 Step 3650/5304 Paf1 634.307617 Paf2 585.29541 Paf3 578.234314 Heatmap 182.096741 Total loss 1979.93408\n",
      "Epoch 296 Step 3660/5304 Paf1 644.395142 Paf2 584.28833 Paf3 599.973877 Heatmap 197.357758 Total loss 2026.01514\n",
      "Epoch 296 Step 3670/5304 Paf1 457.631439 Paf2 411.183777 Paf3 424.202057 Heatmap 129.59259 Total loss 1422.60986\n",
      "Epoch 296 Step 3680/5304 Paf1 589.786133 Paf2 547.322632 Paf3 551.80188 Heatmap 162.946976 Total loss 1851.85767\n",
      "Epoch 296 Step 3690/5304 Paf1 932.442688 Paf2 880.86792 Paf3 870.871887 Heatmap 273.140564 Total loss 2957.323\n",
      "Epoch 296 Step 3700/5304 Paf1 758.793457 Paf2 707.975037 Paf3 701.328613 Heatmap 241.289154 Total loss 2409.38623\n",
      "Epoch 296 Step 3710/5304 Paf1 388.973419 Paf2 355.948486 Paf3 363.064453 Heatmap 102.462509 Total loss 1210.44885\n",
      "Epoch 296 Step 3720/5304 Paf1 523.763855 Paf2 510.902 Paf3 507.585022 Heatmap 187.885986 Total loss 1730.13696\n",
      "Epoch 296 Step 3730/5304 Paf1 685.703491 Paf2 655.175293 Paf3 644.033447 Heatmap 190.954056 Total loss 2175.86621\n",
      "Epoch 296 Step 3740/5304 Paf1 1151.41785 Paf2 1083.35657 Paf3 1110.7196 Heatmap 432.67923 Total loss 3778.17334\n",
      "Epoch 296 Step 3750/5304 Paf1 723.898621 Paf2 683.183838 Paf3 659.239197 Heatmap 246.564758 Total loss 2312.88647\n",
      "Epoch 296 Step 3760/5304 Paf1 609.023438 Paf2 555.271118 Paf3 551.59082 Heatmap 177.320404 Total loss 1893.20581\n",
      "Epoch 296 Step 3770/5304 Paf1 808.921631 Paf2 762.998779 Paf3 743.17749 Heatmap 293.453491 Total loss 2608.55127\n",
      "Epoch 296 Step 3780/5304 Paf1 719.511536 Paf2 679.028259 Paf3 679.171875 Heatmap 205.587616 Total loss 2283.29932\n",
      "Epoch 296 Step 3790/5304 Paf1 660.983337 Paf2 612.538452 Paf3 613.069519 Heatmap 195.926544 Total loss 2082.51782\n",
      "Epoch 296 Step 3800/5304 Paf1 729.598938 Paf2 685.887756 Paf3 682.706604 Heatmap 198.081802 Total loss 2296.27515\n",
      "Epoch 296 Step 3810/5304 Paf1 722.724426 Paf2 691.980347 Paf3 691.003479 Heatmap 198.851913 Total loss 2304.5603\n",
      "Epoch 296 Step 3820/5304 Paf1 867.637695 Paf2 807.035889 Paf3 810.08429 Heatmap 291.382416 Total loss 2776.14014\n",
      "Epoch 296 Step 3830/5304 Paf1 560.020752 Paf2 507.483704 Paf3 516.621521 Heatmap 152.997833 Total loss 1737.12378\n",
      "Epoch 296 Step 3840/5304 Paf1 435.418091 Paf2 412.923706 Paf3 403.057129 Heatmap 156.097076 Total loss 1407.49597\n",
      "Epoch 296 Step 3850/5304 Paf1 1163.34839 Paf2 1095.48877 Paf3 1081.97839 Heatmap 343.481262 Total loss 3684.29688\n",
      "Epoch 296 Step 3860/5304 Paf1 667.682251 Paf2 604.343872 Paf3 630.11438 Heatmap 194.017 Total loss 2096.15747\n",
      "Epoch 296 Step 3870/5304 Paf1 725.934692 Paf2 662.366333 Paf3 687.780396 Heatmap 207.998199 Total loss 2284.07959\n",
      "Epoch 296 Step 3880/5304 Paf1 748.193665 Paf2 698.39 Paf3 709.69812 Heatmap 230.610535 Total loss 2386.89233\n",
      "Epoch 296 Step 3890/5304 Paf1 649.690796 Paf2 620.885742 Paf3 616.661255 Heatmap 230.454712 Total loss 2117.69238\n",
      "Epoch 296 Step 3900/5304 Paf1 590.980957 Paf2 547.888855 Paf3 556.387817 Heatmap 193.33075 Total loss 1888.58838\n",
      "Epoch 296 Step 3910/5304 Paf1 873.66394 Paf2 815.952393 Paf3 796.070435 Heatmap 283.161469 Total loss 2768.84814\n",
      "Epoch 296 Step 3920/5304 Paf1 597.514038 Paf2 565.817139 Paf3 559.274414 Heatmap 174.098572 Total loss 1896.7041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296 Step 3930/5304 Paf1 676.326782 Paf2 632.358582 Paf3 632.376221 Heatmap 247.631042 Total loss 2188.69263\n",
      "Epoch 296 Step 3940/5304 Paf1 677.52655 Paf2 625.103943 Paf3 622.67218 Heatmap 209.113159 Total loss 2134.41577\n",
      "Epoch 296 Step 3950/5304 Paf1 526.337646 Paf2 495.526733 Paf3 499.29184 Heatmap 164.520874 Total loss 1685.67712\n",
      "Epoch 296 Step 3960/5304 Paf1 804.657 Paf2 767.085815 Paf3 752.521484 Heatmap 276.502411 Total loss 2600.7666\n",
      "Epoch 296 Step 3970/5304 Paf1 546.609924 Paf2 506.576385 Paf3 494.648407 Heatmap 156.691467 Total loss 1704.52612\n",
      "Epoch 296 Step 3980/5304 Paf1 704.019836 Paf2 679.421082 Paf3 670.255432 Heatmap 247.974152 Total loss 2301.67041\n",
      "Epoch 296 Step 3990/5304 Paf1 797.526611 Paf2 734.460205 Paf3 737.711182 Heatmap 264.693329 Total loss 2534.39136\n",
      "Epoch 296 Step 4000/5304 Paf1 597.732117 Paf2 570.785889 Paf3 582.967224 Heatmap 176.260376 Total loss 1927.74561\n",
      "Saved checkpoint for step 4000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1774\n",
      "Epoch 296 Step 4010/5304 Paf1 641.152588 Paf2 600.61969 Paf3 596.664612 Heatmap 182.744141 Total loss 2021.18091\n",
      "Epoch 296 Step 4020/5304 Paf1 490.837646 Paf2 456.164154 Paf3 455.703857 Heatmap 137.93042 Total loss 1540.63611\n",
      "Epoch 296 Step 4030/5304 Paf1 801.339844 Paf2 767.1026 Paf3 765.485474 Heatmap 206.8078 Total loss 2540.7356\n",
      "Epoch 296 Step 4040/5304 Paf1 955.692078 Paf2 899.866943 Paf3 906.891724 Heatmap 313.961304 Total loss 3076.41211\n",
      "Epoch 296 Step 4050/5304 Paf1 882.139221 Paf2 876.489685 Paf3 844.126 Heatmap 264.758179 Total loss 2867.51318\n",
      "Epoch 296 Step 4060/5304 Paf1 953.851868 Paf2 895.501282 Paf3 878.17688 Heatmap 256.414886 Total loss 2983.94482\n",
      "Epoch 296 Step 4070/5304 Paf1 875.88855 Paf2 824.32019 Paf3 810.776123 Heatmap 254.600113 Total loss 2765.58496\n",
      "Epoch 296 Step 4080/5304 Paf1 825.315247 Paf2 757.751 Paf3 764.879395 Heatmap 269.306305 Total loss 2617.25195\n",
      "Epoch 296 Step 4090/5304 Paf1 741.745728 Paf2 682.337646 Paf3 687.307495 Heatmap 197.158417 Total loss 2308.54932\n",
      "Epoch 296 Step 4100/5304 Paf1 481.77356 Paf2 442.297485 Paf3 453.590698 Heatmap 132.345749 Total loss 1510.00757\n",
      "Epoch 296 Step 4110/5304 Paf1 697.610107 Paf2 642.570679 Paf3 650.36908 Heatmap 222.757965 Total loss 2213.30786\n",
      "Epoch 296 Step 4120/5304 Paf1 861.21051 Paf2 817.18457 Paf3 809.388733 Heatmap 261.706055 Total loss 2749.48975\n",
      "Epoch 296 Step 4130/5304 Paf1 978.327759 Paf2 929.22229 Paf3 943.522705 Heatmap 357.279236 Total loss 3208.35205\n",
      "Epoch 296 Step 4140/5304 Paf1 699.205322 Paf2 649.054932 Paf3 653.333801 Heatmap 185.4944 Total loss 2187.08838\n",
      "Epoch 296 Step 4150/5304 Paf1 713.393372 Paf2 691.835815 Paf3 670.088867 Heatmap 210.57338 Total loss 2285.8916\n",
      "Epoch 296 Step 4160/5304 Paf1 593.321472 Paf2 559.432678 Paf3 544.574 Heatmap 151.073532 Total loss 1848.40161\n",
      "Epoch 296 Step 4170/5304 Paf1 503.932617 Paf2 492.927643 Paf3 479.444427 Heatmap 135.642822 Total loss 1611.94751\n",
      "Epoch 296 Step 4180/5304 Paf1 427.82196 Paf2 399.741608 Paf3 400.815979 Heatmap 139.897507 Total loss 1368.2771\n",
      "Epoch 296 Step 4190/5304 Paf1 784.783081 Paf2 746.574219 Paf3 747.616089 Heatmap 276.175537 Total loss 2555.14893\n",
      "Epoch 296 Step 4200/5304 Paf1 491.560699 Paf2 450.320801 Paf3 442.123962 Heatmap 168.137238 Total loss 1552.1427\n",
      "Epoch 296 Step 4210/5304 Paf1 526.716736 Paf2 478.849609 Paf3 463.101654 Heatmap 164.867691 Total loss 1633.53564\n",
      "Epoch 296 Step 4220/5304 Paf1 739.615295 Paf2 690.455078 Paf3 692.935303 Heatmap 208.273682 Total loss 2331.2793\n",
      "Epoch 296 Step 4230/5304 Paf1 1039.36926 Paf2 1004.06146 Paf3 981.47522 Heatmap 356.51593 Total loss 3381.42188\n",
      "Epoch 296 Step 4240/5304 Paf1 771.279907 Paf2 706.765869 Paf3 712.868103 Heatmap 249.989929 Total loss 2440.90381\n",
      "Epoch 296 Step 4250/5304 Paf1 943.662903 Paf2 898.697754 Paf3 879.887512 Heatmap 307.803284 Total loss 3030.05127\n",
      "Epoch 296 Step 4260/5304 Paf1 622.255493 Paf2 580.916931 Paf3 573.828186 Heatmap 174.026749 Total loss 1951.02734\n",
      "Epoch 296 Step 4270/5304 Paf1 738.200684 Paf2 696.607483 Paf3 687.196 Heatmap 244.740509 Total loss 2366.74463\n",
      "Epoch 296 Step 4280/5304 Paf1 493.64859 Paf2 469.829956 Paf3 469.879395 Heatmap 159.666626 Total loss 1593.02454\n",
      "Epoch 296 Step 4290/5304 Paf1 722.138367 Paf2 683.268677 Paf3 658.334473 Heatmap 208.832489 Total loss 2272.57397\n",
      "Epoch 296 Step 4300/5304 Paf1 623.678833 Paf2 586.931641 Paf3 578.672668 Heatmap 171.889801 Total loss 1961.17297\n",
      "Epoch 296 Step 4310/5304 Paf1 603.461914 Paf2 565.93634 Paf3 547.144592 Heatmap 150.829681 Total loss 1867.37244\n",
      "Epoch 296 Step 4320/5304 Paf1 673.73877 Paf2 631.152527 Paf3 644.99054 Heatmap 201.001221 Total loss 2150.88306\n",
      "Epoch 296 Step 4330/5304 Paf1 729.116455 Paf2 686.595581 Paf3 683.574158 Heatmap 228.233429 Total loss 2327.51953\n",
      "Epoch 296 Step 4340/5304 Paf1 699.184265 Paf2 658.159851 Paf3 655.371338 Heatmap 263.875488 Total loss 2276.59082\n",
      "Epoch 296 Step 4350/5304 Paf1 565.715576 Paf2 554.542297 Paf3 541.395691 Heatmap 201.219543 Total loss 1862.87305\n",
      "Epoch 296 Step 4360/5304 Paf1 855.925232 Paf2 806.18927 Paf3 807.185669 Heatmap 279.931641 Total loss 2749.23193\n",
      "Epoch 296 Step 4370/5304 Paf1 918.737427 Paf2 848.611816 Paf3 835.296753 Heatmap 287.523224 Total loss 2890.16919\n",
      "Epoch 296 Step 4380/5304 Paf1 793.904 Paf2 732.537964 Paf3 761.036194 Heatmap 254.316772 Total loss 2541.79492\n",
      "Epoch 296 Step 4390/5304 Paf1 932.188 Paf2 855.068481 Paf3 851.123779 Heatmap 297.642517 Total loss 2936.02295\n",
      "Epoch 296 Step 4400/5304 Paf1 524.295776 Paf2 457.842651 Paf3 469.654175 Heatmap 115.40834 Total loss 1567.20093\n",
      "Epoch 296 Step 4410/5304 Paf1 848.140442 Paf2 782.549866 Paf3 796.712891 Heatmap 278.925537 Total loss 2706.32861\n",
      "Epoch 296 Step 4420/5304 Paf1 928.486938 Paf2 870.824768 Paf3 880.975281 Heatmap 274.58194 Total loss 2954.86914\n",
      "Epoch 296 Step 4430/5304 Paf1 668.591553 Paf2 607.526855 Paf3 615.672 Heatmap 193.637283 Total loss 2085.42773\n",
      "Epoch 296 Step 4440/5304 Paf1 422.588226 Paf2 398.108612 Paf3 400.128357 Heatmap 133.074799 Total loss 1353.8999\n",
      "Epoch 296 Step 4450/5304 Paf1 783.753418 Paf2 698.000244 Paf3 701.195068 Heatmap 191.695618 Total loss 2374.64429\n",
      "Epoch 296 Step 4460/5304 Paf1 768.661865 Paf2 720.048706 Paf3 737.119263 Heatmap 208.501175 Total loss 2434.33105\n",
      "Epoch 296 Step 4470/5304 Paf1 625.407898 Paf2 608.96759 Paf3 601.134644 Heatmap 197.07341 Total loss 2032.5835\n",
      "Epoch 296 Step 4480/5304 Paf1 904.376587 Paf2 855.940125 Paf3 841.409302 Heatmap 272.921082 Total loss 2874.64697\n",
      "Epoch 296 Step 4490/5304 Paf1 1134.03296 Paf2 1075.39734 Paf3 1074.72217 Heatmap 313.994751 Total loss 3598.14697\n",
      "Epoch 296 Step 4500/5304 Paf1 434.623779 Paf2 389.468018 Paf3 389.647675 Heatmap 130.646896 Total loss 1344.38635\n",
      "Epoch 296 Step 4510/5304 Paf1 465.704926 Paf2 413.502136 Paf3 430.234253 Heatmap 141.532425 Total loss 1450.97363\n",
      "Epoch 296 Step 4520/5304 Paf1 726.377441 Paf2 678.803406 Paf3 677.329 Heatmap 217.046432 Total loss 2299.5564\n",
      "Epoch 296 Step 4530/5304 Paf1 830.005188 Paf2 770.492371 Paf3 773.086121 Heatmap 240.478668 Total loss 2614.0625\n",
      "Epoch 296 Step 4540/5304 Paf1 463.154724 Paf2 426.90564 Paf3 419.36908 Heatmap 131.527634 Total loss 1440.95703\n",
      "Epoch 296 Step 4550/5304 Paf1 1152.80566 Paf2 1060.5249 Paf3 1073.98987 Heatmap 322.447662 Total loss 3609.76807\n",
      "Epoch 296 Step 4560/5304 Paf1 427.344879 Paf2 403.300201 Paf3 403.184 Heatmap 106.273804 Total loss 1340.10291\n",
      "Epoch 296 Step 4570/5304 Paf1 1199.33228 Paf2 1110.08289 Paf3 1113.45837 Heatmap 343.806366 Total loss 3766.67969\n",
      "Epoch 296 Step 4580/5304 Paf1 934.63916 Paf2 902.897705 Paf3 898.646301 Heatmap 309.265381 Total loss 3045.44849\n",
      "Epoch 296 Step 4590/5304 Paf1 736.675415 Paf2 703.316895 Paf3 711.017334 Heatmap 226.778397 Total loss 2377.78809\n",
      "Epoch 296 Step 4600/5304 Paf1 621.76062 Paf2 581.489563 Paf3 569.482788 Heatmap 161.580673 Total loss 1934.31372\n",
      "Epoch 296 Step 4610/5304 Paf1 505.344757 Paf2 481.587189 Paf3 484.73175 Heatmap 139.552795 Total loss 1611.21655\n",
      "Epoch 296 Step 4620/5304 Paf1 744.440186 Paf2 712.713684 Paf3 714.660156 Heatmap 244.90065 Total loss 2416.7146\n",
      "Epoch 296 Step 4630/5304 Paf1 979.586 Paf2 970.463806 Paf3 955.493 Heatmap 335.242889 Total loss 3240.78564\n",
      "Epoch 296 Step 4640/5304 Paf1 1036.32153 Paf2 998.970581 Paf3 992.559326 Heatmap 340.917419 Total loss 3368.76904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 296 Step 4650/5304 Paf1 602.131409 Paf2 585.822144 Paf3 578.633728 Heatmap 197.74939 Total loss 1964.33667\n",
      "Epoch 296 Step 4660/5304 Paf1 786.667175 Paf2 731.941528 Paf3 727.078491 Heatmap 237.028229 Total loss 2482.71533\n",
      "Epoch 296 Step 4670/5304 Paf1 762.235657 Paf2 719.890381 Paf3 717.639954 Heatmap 260.213867 Total loss 2459.97974\n",
      "Epoch 296 Step 4680/5304 Paf1 456.09613 Paf2 420.010345 Paf3 432.648865 Heatmap 141.031021 Total loss 1449.78638\n",
      "Epoch 296 Step 4690/5304 Paf1 474.663635 Paf2 441.259155 Paf3 448.288177 Heatmap 111.230186 Total loss 1475.44116\n",
      "Epoch 296 Step 4700/5304 Paf1 869.65918 Paf2 828.30011 Paf3 841.754517 Heatmap 252.927231 Total loss 2792.64111\n",
      "Epoch 296 Step 4710/5304 Paf1 834.809509 Paf2 780.327576 Paf3 767.326843 Heatmap 260.340881 Total loss 2642.80469\n",
      "Epoch 296 Step 4720/5304 Paf1 820.827881 Paf2 779.893311 Paf3 782.43042 Heatmap 237.511353 Total loss 2620.66309\n",
      "Epoch 296 Step 4730/5304 Paf1 733.639954 Paf2 677.71 Paf3 701.832642 Heatmap 215.620743 Total loss 2328.80322\n",
      "Epoch 296 Step 4740/5304 Paf1 569.724548 Paf2 502.853516 Paf3 513.019409 Heatmap 177.005432 Total loss 1762.60303\n",
      "Epoch 296 Step 4750/5304 Paf1 594.593689 Paf2 556.731812 Paf3 559.966125 Heatmap 211.226822 Total loss 1922.51831\n",
      "Epoch 296 Step 4760/5304 Paf1 553.066345 Paf2 521.723877 Paf3 518.033752 Heatmap 174.483261 Total loss 1767.30737\n",
      "Epoch 296 Step 4770/5304 Paf1 524.795166 Paf2 477.835754 Paf3 483.890472 Heatmap 156.064972 Total loss 1642.58643\n",
      "Epoch 296 Step 4780/5304 Paf1 863.342773 Paf2 784.897217 Paf3 776.294739 Heatmap 254.545731 Total loss 2679.08057\n",
      "Epoch 296 Step 4790/5304 Paf1 755.218384 Paf2 720.831787 Paf3 703.518616 Heatmap 217.62645 Total loss 2397.19531\n",
      "Epoch 296 Step 4800/5304 Paf1 761.948 Paf2 706.598145 Paf3 700.863647 Heatmap 241.476288 Total loss 2410.88623\n",
      "Epoch 296 Step 4810/5304 Paf1 779.007141 Paf2 736.695801 Paf3 736.38269 Heatmap 243.981445 Total loss 2496.06689\n",
      "Epoch 296 Step 4820/5304 Paf1 403.991943 Paf2 369.03949 Paf3 377.727234 Heatmap 118.031845 Total loss 1268.79053\n",
      "Epoch 296 Step 4830/5304 Paf1 415.313049 Paf2 386.294495 Paf3 384.294525 Heatmap 101.969284 Total loss 1287.87134\n",
      "Epoch 296 Step 4840/5304 Paf1 956.28772 Paf2 889.851624 Paf3 889.77478 Heatmap 299.050171 Total loss 3034.96436\n",
      "Epoch 296 Step 4850/5304 Paf1 722.835083 Paf2 667.754211 Paf3 674.990662 Heatmap 252.788467 Total loss 2318.36841\n",
      "Epoch 296 Step 4860/5304 Paf1 767.019043 Paf2 718.293884 Paf3 737.91449 Heatmap 200.674652 Total loss 2423.9021\n",
      "Epoch 296 Step 4870/5304 Paf1 680.385437 Paf2 648.319092 Paf3 648.469727 Heatmap 234.392807 Total loss 2211.56714\n",
      "Epoch 296 Step 4880/5304 Paf1 828.236206 Paf2 822.192383 Paf3 787.401367 Heatmap 275.384064 Total loss 2713.21387\n",
      "Epoch 296 Step 4890/5304 Paf1 671.559265 Paf2 619.199402 Paf3 614.313293 Heatmap 229.15387 Total loss 2134.22583\n",
      "Epoch 296 Step 4900/5304 Paf1 776.888489 Paf2 734.044495 Paf3 735.029785 Heatmap 275.166 Total loss 2521.12891\n",
      "Epoch 296 Step 4910/5304 Paf1 468.874695 Paf2 448.382111 Paf3 446.881958 Heatmap 124.146317 Total loss 1488.28516\n",
      "Epoch 296 Step 4920/5304 Paf1 615.520264 Paf2 578.916 Paf3 583.196899 Heatmap 168.267456 Total loss 1945.90063\n",
      "Epoch 296 Step 4930/5304 Paf1 531.93689 Paf2 503.716431 Paf3 498.970459 Heatmap 133.69664 Total loss 1668.32043\n",
      "Epoch 296 Step 4940/5304 Paf1 873.275574 Paf2 808.715698 Paf3 792.176758 Heatmap 271.114349 Total loss 2745.28223\n",
      "Epoch 296 Step 4950/5304 Paf1 846.986572 Paf2 778.836304 Paf3 775.499634 Heatmap 235.617645 Total loss 2636.94019\n",
      "Epoch 296 Step 4960/5304 Paf1 377.160278 Paf2 344.318176 Paf3 341.196259 Heatmap 109.904572 Total loss 1172.57935\n",
      "Epoch 296 Step 4970/5304 Paf1 682.608582 Paf2 618.26825 Paf3 622.783691 Heatmap 174.239243 Total loss 2097.9\n",
      "Epoch 296 Step 4980/5304 Paf1 598.475 Paf2 566.421509 Paf3 567.58728 Heatmap 146.146896 Total loss 1878.63062\n",
      "Epoch 296 Step 4990/5304 Paf1 838.928467 Paf2 771.13562 Paf3 784.422363 Heatmap 252.625824 Total loss 2647.1123\n",
      "Epoch 296 Step 5000/5304 Paf1 594.223206 Paf2 585.543762 Paf3 576.172119 Heatmap 205.342499 Total loss 1961.28162\n",
      "Saved checkpoint for step 5000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1775\n",
      "Epoch 296 Step 5010/5304 Paf1 747.842224 Paf2 677.145447 Paf3 680.374573 Heatmap 198.712372 Total loss 2304.07471\n",
      "Epoch 296 Step 5020/5304 Paf1 681.103943 Paf2 631.231323 Paf3 636.120544 Heatmap 214.600204 Total loss 2163.05591\n",
      "Epoch 296 Step 5030/5304 Paf1 805.259033 Paf2 759.503296 Paf3 755.560913 Heatmap 243.372375 Total loss 2563.69556\n",
      "Epoch 296 Step 5040/5304 Paf1 695.65033 Paf2 619.43634 Paf3 611.402 Heatmap 197.334778 Total loss 2123.82349\n",
      "Epoch 296 Step 5050/5304 Paf1 738.234741 Paf2 718.130859 Paf3 711.11145 Heatmap 261.605469 Total loss 2429.08252\n",
      "Epoch 296 Step 5060/5304 Paf1 664.896912 Paf2 623.936523 Paf3 627.266418 Heatmap 215.254578 Total loss 2131.35449\n",
      "Epoch 296 Step 5070/5304 Paf1 622.171265 Paf2 602.194397 Paf3 592.589905 Heatmap 178.065 Total loss 1995.02063\n",
      "Epoch 296 Step 5080/5304 Paf1 429.518646 Paf2 401.571136 Paf3 404.467438 Heatmap 121.397293 Total loss 1356.95459\n",
      "Epoch 296 Step 5090/5304 Paf1 844.278625 Paf2 812.339 Paf3 798.822571 Heatmap 283.97406 Total loss 2739.41431\n",
      "Epoch 296 Step 5100/5304 Paf1 945.248291 Paf2 920.844299 Paf3 938.155884 Heatmap 337.420959 Total loss 3141.66943\n",
      "Epoch 296 Step 5110/5304 Paf1 508.963226 Paf2 463.990967 Paf3 471.706 Heatmap 162.149139 Total loss 1606.80933\n",
      "Epoch 296 Step 5120/5304 Paf1 1012.60858 Paf2 941.420227 Paf3 942.801392 Heatmap 302.353058 Total loss 3199.18311\n",
      "Epoch 296 Step 5130/5304 Paf1 734.284546 Paf2 701.826599 Paf3 676.256653 Heatmap 223.300232 Total loss 2335.66797\n",
      "Epoch 296 Step 5140/5304 Paf1 707.877319 Paf2 663.814392 Paf3 671.477051 Heatmap 247.910904 Total loss 2291.07959\n",
      "Epoch 296 Step 5150/5304 Paf1 633.672 Paf2 570.418152 Paf3 583.418 Heatmap 172.40097 Total loss 1959.90906\n",
      "Epoch 296 Step 5160/5304 Paf1 935.77063 Paf2 890.369873 Paf3 887.950073 Heatmap 302.542175 Total loss 3016.63281\n",
      "Epoch 296 Step 5170/5304 Paf1 876.87384 Paf2 841.928162 Paf3 843.667786 Heatmap 325.507111 Total loss 2887.97705\n",
      "Epoch 296 Step 5180/5304 Paf1 502.531982 Paf2 458.599182 Paf3 455.347839 Heatmap 128.83017 Total loss 1545.30908\n",
      "Epoch 296 Step 5190/5304 Paf1 502.182617 Paf2 458.773163 Paf3 455.42807 Heatmap 140.63356 Total loss 1557.01746\n",
      "Epoch 296 Step 5200/5304 Paf1 437.213867 Paf2 405.276886 Paf3 408.049744 Heatmap 157.882385 Total loss 1408.42285\n",
      "Epoch 296 Step 5210/5304 Paf1 483.32193 Paf2 440.888824 Paf3 434.829559 Heatmap 124.759727 Total loss 1483.8\n",
      "Epoch 296 Step 5220/5304 Paf1 581.963745 Paf2 532.681274 Paf3 530.091248 Heatmap 171.564606 Total loss 1816.3009\n",
      "Epoch 296 Step 5230/5304 Paf1 473.077667 Paf2 442.825531 Paf3 441.871521 Heatmap 141.739731 Total loss 1499.5144\n",
      "Epoch 296 Step 5240/5304 Paf1 627.856323 Paf2 596.387817 Paf3 588.651733 Heatmap 200.357071 Total loss 2013.25293\n",
      "Epoch 296 Step 5250/5304 Paf1 737.628052 Paf2 684.146729 Paf3 704.598755 Heatmap 246.899109 Total loss 2373.27271\n",
      "Epoch 296 Step 5260/5304 Paf1 579.484 Paf2 541.973389 Paf3 543.449158 Heatmap 159.030548 Total loss 1823.93713\n",
      "Epoch 296 Step 5270/5304 Paf1 787.249695 Paf2 737.180908 Paf3 733.153076 Heatmap 232.870453 Total loss 2490.4541\n",
      "Epoch 296 Step 5280/5304 Paf1 533.167175 Paf2 501.578308 Paf3 510.93576 Heatmap 133.198303 Total loss 1678.87952\n",
      "Epoch 296 Step 5290/5304 Paf1 685.709229 Paf2 627.624 Paf3 637.442688 Heatmap 213.82872 Total loss 2164.60474\n",
      "Epoch 296 Step 5300/5304 Paf1 551.036072 Paf2 510.6315 Paf3 484.012177 Heatmap 139.465912 Total loss 1685.14575\n",
      "Completed epoch 296. Saving weights...\n",
      "Epoch training time: 0:15:45.828665\n",
      "Calculating validation losses...\n",
      "Validation step 0 ...\n",
      "Validation losses for epoch: 296 : Loss paf 685.8330688476562, Loss heatmap 225.9134979248047, Total loss 2323.886474609375\n",
      "Start processing epoch 297\n",
      "Epoch 297 Step 10/5304 Paf1 479.879822 Paf2 447.716125 Paf3 461.549377 Heatmap 124.418137 Total loss 1513.56348\n",
      "Epoch 297 Step 20/5304 Paf1 589.270752 Paf2 528.175476 Paf3 549.654541 Heatmap 159.255432 Total loss 1826.3562\n",
      "Epoch 297 Step 30/5304 Paf1 534.391602 Paf2 509.213257 Paf3 500.188293 Heatmap 180.258484 Total loss 1724.05164\n",
      "Epoch 297 Step 40/5304 Paf1 611.901123 Paf2 564.986206 Paf3 560.278442 Heatmap 190.991882 Total loss 1928.15771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297 Step 50/5304 Paf1 649.747 Paf2 604.637451 Paf3 607.405579 Heatmap 192.480804 Total loss 2054.271\n",
      "Epoch 297 Step 60/5304 Paf1 552.988281 Paf2 534.040833 Paf3 519.551758 Heatmap 191.418457 Total loss 1797.99927\n",
      "Epoch 297 Step 70/5304 Paf1 683.775208 Paf2 642.246216 Paf3 647.725708 Heatmap 179.576691 Total loss 2153.32373\n",
      "Epoch 297 Step 80/5304 Paf1 550.736084 Paf2 504.175537 Paf3 506.095581 Heatmap 170.310928 Total loss 1731.31812\n",
      "Epoch 297 Step 90/5304 Paf1 498.217804 Paf2 450.786194 Paf3 455.283 Heatmap 122.581238 Total loss 1526.86829\n",
      "Epoch 297 Step 100/5304 Paf1 912.266357 Paf2 861.922058 Paf3 855.424194 Heatmap 287.604 Total loss 2917.2168\n",
      "Epoch 297 Step 110/5304 Paf1 744.077515 Paf2 678.364746 Paf3 706.95166 Heatmap 202.448914 Total loss 2331.84277\n",
      "Epoch 297 Step 120/5304 Paf1 599.282776 Paf2 567.489502 Paf3 555.747 Heatmap 202.089386 Total loss 1924.60864\n",
      "Epoch 297 Step 130/5304 Paf1 633.250488 Paf2 609.101318 Paf3 600.925415 Heatmap 212.077972 Total loss 2055.35522\n",
      "Epoch 297 Step 140/5304 Paf1 429.953766 Paf2 386.250031 Paf3 386.939575 Heatmap 132.098953 Total loss 1335.24231\n",
      "Epoch 297 Step 150/5304 Paf1 653.846741 Paf2 609.009 Paf3 618.183167 Heatmap 196.341064 Total loss 2077.38\n",
      "Epoch 297 Step 160/5304 Paf1 828.250549 Paf2 770.238831 Paf3 758.086121 Heatmap 227.180542 Total loss 2583.7561\n",
      "Epoch 297 Step 170/5304 Paf1 791.672363 Paf2 739.122864 Paf3 760.498779 Heatmap 253.689804 Total loss 2544.98389\n",
      "Epoch 297 Step 180/5304 Paf1 731.66394 Paf2 696.043762 Paf3 696.078918 Heatmap 220.437256 Total loss 2344.22388\n",
      "Epoch 297 Step 190/5304 Paf1 956.40979 Paf2 909.598572 Paf3 914.912048 Heatmap 342.355957 Total loss 3123.27637\n",
      "Epoch 297 Step 200/5304 Paf1 811.575867 Paf2 749.447449 Paf3 752.887268 Heatmap 228.339417 Total loss 2542.25\n",
      "Epoch 297 Step 210/5304 Paf1 845.191162 Paf2 814.315491 Paf3 818.87 Heatmap 262.656799 Total loss 2741.03345\n",
      "Epoch 297 Step 220/5304 Paf1 463.823364 Paf2 426.492859 Paf3 414.705078 Heatmap 105.330933 Total loss 1410.35229\n",
      "Epoch 297 Step 230/5304 Paf1 550.133728 Paf2 488.200104 Paf3 476.648102 Heatmap 140.736603 Total loss 1655.71851\n",
      "Epoch 297 Step 240/5304 Paf1 564.602234 Paf2 531.285339 Paf3 540.886475 Heatmap 184.4552 Total loss 1821.22925\n",
      "Epoch 297 Step 250/5304 Paf1 572.207336 Paf2 536.468201 Paf3 536.09668 Heatmap 193.555893 Total loss 1838.32812\n",
      "Epoch 297 Step 260/5304 Paf1 616.409424 Paf2 565.868958 Paf3 553.318665 Heatmap 185.265564 Total loss 1920.86255\n",
      "Epoch 297 Step 270/5304 Paf1 1149.32678 Paf2 1105.71838 Paf3 1098.25684 Heatmap 380.484375 Total loss 3733.78638\n",
      "Epoch 297 Step 280/5304 Paf1 771.445 Paf2 712.683044 Paf3 725.33 Heatmap 220.727356 Total loss 2430.18555\n",
      "Epoch 297 Step 290/5304 Paf1 442.597046 Paf2 404.812378 Paf3 403.55249 Heatmap 119.264336 Total loss 1370.22632\n",
      "Epoch 297 Step 300/5304 Paf1 851.071106 Paf2 837.603943 Paf3 831.017944 Heatmap 336.497467 Total loss 2856.19043\n",
      "Epoch 297 Step 310/5304 Paf1 565.44751 Paf2 530.27356 Paf3 541.562 Heatmap 187.254211 Total loss 1824.53735\n",
      "Epoch 297 Step 320/5304 Paf1 593.289246 Paf2 566.391785 Paf3 569.198181 Heatmap 230.677963 Total loss 1959.55713\n",
      "Epoch 297 Step 330/5304 Paf1 1001.72412 Paf2 918.544678 Paf3 914.458374 Heatmap 304.312 Total loss 3139.03906\n",
      "Epoch 297 Step 340/5304 Paf1 709.188477 Paf2 678.254089 Paf3 679.69696 Heatmap 225.128418 Total loss 2292.26807\n",
      "Epoch 297 Step 350/5304 Paf1 684.332458 Paf2 654.205383 Paf3 655.412231 Heatmap 203.910019 Total loss 2197.86\n",
      "Epoch 297 Step 360/5304 Paf1 1005.73718 Paf2 957.871521 Paf3 977.807739 Heatmap 305.040283 Total loss 3246.45654\n",
      "Epoch 297 Step 370/5304 Paf1 431.946106 Paf2 391.794189 Paf3 381.834381 Heatmap 117.308044 Total loss 1322.88269\n",
      "Epoch 297 Step 380/5304 Paf1 666.099487 Paf2 620.029663 Paf3 616.759827 Heatmap 186.614868 Total loss 2089.50391\n",
      "Epoch 297 Step 390/5304 Paf1 547.523804 Paf2 534.404236 Paf3 518.893433 Heatmap 188.28537 Total loss 1789.10681\n",
      "Epoch 297 Step 400/5304 Paf1 746.085571 Paf2 719.003662 Paf3 706.1026 Heatmap 231.564697 Total loss 2402.75659\n",
      "Epoch 297 Step 410/5304 Paf1 776.568298 Paf2 747.226318 Paf3 738.565063 Heatmap 252.405823 Total loss 2514.76562\n",
      "Epoch 297 Step 420/5304 Paf1 847.085938 Paf2 776.005615 Paf3 781.427856 Heatmap 264.722046 Total loss 2669.24146\n",
      "Epoch 297 Step 430/5304 Paf1 581.930176 Paf2 542.198608 Paf3 542.626343 Heatmap 156.47789 Total loss 1823.23303\n",
      "Epoch 297 Step 440/5304 Paf1 568.680969 Paf2 540.032 Paf3 545.068237 Heatmap 162.984894 Total loss 1816.76599\n",
      "Epoch 297 Step 450/5304 Paf1 674.141541 Paf2 633.887573 Paf3 637.478516 Heatmap 243.963226 Total loss 2189.4707\n",
      "Epoch 297 Step 460/5304 Paf1 780.69165 Paf2 723.108154 Paf3 730.788879 Heatmap 249.523163 Total loss 2484.11182\n",
      "Epoch 297 Step 470/5304 Paf1 1336.87427 Paf2 1251.45544 Paf3 1241.87732 Heatmap 426.681488 Total loss 4256.88867\n",
      "Epoch 297 Step 480/5304 Paf1 609.033752 Paf2 579.499207 Paf3 584.830811 Heatmap 174.60556 Total loss 1947.96936\n",
      "Epoch 297 Step 490/5304 Paf1 1005.4519 Paf2 938.570679 Paf3 948.117126 Heatmap 345.073303 Total loss 3237.21289\n",
      "Epoch 297 Step 500/5304 Paf1 862.029297 Paf2 822.507 Paf3 814.181885 Heatmap 241.806244 Total loss 2740.52441\n",
      "Epoch 297 Step 510/5304 Paf1 426.889648 Paf2 423.543152 Paf3 410.152679 Heatmap 128.426346 Total loss 1389.01184\n",
      "Epoch 297 Step 520/5304 Paf1 671.277344 Paf2 613.814209 Paf3 631.128845 Heatmap 162.266388 Total loss 2078.48682\n",
      "Epoch 297 Step 530/5304 Paf1 624.713501 Paf2 593.347534 Paf3 587.997253 Heatmap 197.914337 Total loss 2003.97266\n",
      "Epoch 297 Step 540/5304 Paf1 833.757202 Paf2 789.930115 Paf3 768.924133 Heatmap 246.253891 Total loss 2638.86523\n",
      "Epoch 297 Step 550/5304 Paf1 445.034698 Paf2 410.62854 Paf3 407.74118 Heatmap 123.287 Total loss 1386.69141\n",
      "Epoch 297 Step 560/5304 Paf1 500.965454 Paf2 460.40741 Paf3 449.274261 Heatmap 134.520477 Total loss 1545.1676\n",
      "Epoch 297 Step 570/5304 Paf1 728.180786 Paf2 700.560425 Paf3 702.78595 Heatmap 236.512238 Total loss 2368.03955\n",
      "Epoch 297 Step 580/5304 Paf1 600.573608 Paf2 543.504761 Paf3 544.109192 Heatmap 161.217331 Total loss 1849.40491\n",
      "Epoch 297 Step 590/5304 Paf1 965.014465 Paf2 903.431396 Paf3 895.746887 Heatmap 307.928711 Total loss 3072.12134\n",
      "Epoch 297 Step 600/5304 Paf1 1069.90967 Paf2 1001.06384 Paf3 989.108 Heatmap 330.154114 Total loss 3390.23584\n",
      "Epoch 297 Step 610/5304 Paf1 769.723 Paf2 736.61908 Paf3 740.064819 Heatmap 245.881699 Total loss 2492.28857\n",
      "Epoch 297 Step 620/5304 Paf1 849.551819 Paf2 753.275513 Paf3 775.382324 Heatmap 244.143356 Total loss 2622.35303\n",
      "Epoch 297 Step 630/5304 Paf1 930.621826 Paf2 878.07 Paf3 869.02948 Heatmap 283.885437 Total loss 2961.60693\n",
      "Epoch 297 Step 640/5304 Paf1 728.286194 Paf2 677.09845 Paf3 681.110352 Heatmap 249.811401 Total loss 2336.3064\n",
      "Epoch 297 Step 650/5304 Paf1 679.144043 Paf2 612.209229 Paf3 612.068 Heatmap 227.288376 Total loss 2130.70972\n",
      "Epoch 297 Step 660/5304 Paf1 447.419189 Paf2 412.22525 Paf3 407.409973 Heatmap 122.322113 Total loss 1389.37646\n",
      "Epoch 297 Step 670/5304 Paf1 990.694763 Paf2 945.321106 Paf3 962.063354 Heatmap 331.980225 Total loss 3230.05957\n",
      "Epoch 297 Step 680/5304 Paf1 680.767151 Paf2 603.550476 Paf3 628.991333 Heatmap 187.688583 Total loss 2100.99756\n",
      "Epoch 297 Step 690/5304 Paf1 823.599548 Paf2 776.534729 Paf3 761.578796 Heatmap 275.294647 Total loss 2637.00781\n",
      "Epoch 297 Step 700/5304 Paf1 683.435181 Paf2 647.885925 Paf3 665.007812 Heatmap 229.779327 Total loss 2226.10815\n",
      "Epoch 297 Step 710/5304 Paf1 991.720764 Paf2 942.402466 Paf3 947.031189 Heatmap 335.67569 Total loss 3216.83\n",
      "Epoch 297 Step 720/5304 Paf1 719.443604 Paf2 677.615479 Paf3 669.608582 Heatmap 223.148117 Total loss 2289.81592\n",
      "Epoch 297 Step 730/5304 Paf1 906.331299 Paf2 861.627075 Paf3 858.994812 Heatmap 316.538269 Total loss 2943.49146\n",
      "Epoch 297 Step 740/5304 Paf1 735.295898 Paf2 689.418274 Paf3 685.25415 Heatmap 195.475098 Total loss 2305.44336\n",
      "Epoch 297 Step 750/5304 Paf1 573.639099 Paf2 525.318787 Paf3 538.843567 Heatmap 162.612854 Total loss 1800.41431\n",
      "Epoch 297 Step 760/5304 Paf1 691.567871 Paf2 682.685425 Paf3 668.311829 Heatmap 239.16655 Total loss 2281.73169\n",
      "Epoch 297 Step 770/5304 Paf1 506.423798 Paf2 451.340576 Paf3 458.052124 Heatmap 172.108963 Total loss 1587.92554\n",
      "Epoch 297 Step 780/5304 Paf1 818.140381 Paf2 777.094482 Paf3 766.362305 Heatmap 247.653702 Total loss 2609.25098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297 Step 790/5304 Paf1 707.377808 Paf2 639.237732 Paf3 656.322205 Heatmap 231.698181 Total loss 2234.63574\n",
      "Epoch 297 Step 800/5304 Paf1 550.786255 Paf2 518.435181 Paf3 503.412 Heatmap 216.512238 Total loss 1789.14563\n",
      "Epoch 297 Step 810/5304 Paf1 1028.59326 Paf2 978.210144 Paf3 967.776123 Heatmap 328.624695 Total loss 3303.20435\n",
      "Epoch 297 Step 820/5304 Paf1 482.401855 Paf2 448.897095 Paf3 456.524384 Heatmap 138.946442 Total loss 1526.76978\n",
      "Epoch 297 Step 830/5304 Paf1 672.702698 Paf2 617.542419 Paf3 628.980774 Heatmap 215.239334 Total loss 2134.46533\n",
      "Epoch 297 Step 840/5304 Paf1 594.157715 Paf2 558.855408 Paf3 556.532349 Heatmap 204.037186 Total loss 1913.58276\n",
      "Epoch 297 Step 850/5304 Paf1 854.563904 Paf2 787.567627 Paf3 796.05188 Heatmap 246.643219 Total loss 2684.82666\n",
      "Epoch 297 Step 860/5304 Paf1 896.107483 Paf2 853.657471 Paf3 843.764526 Heatmap 265.938171 Total loss 2859.46753\n",
      "Epoch 297 Step 870/5304 Paf1 815.757446 Paf2 743.31311 Paf3 753.497375 Heatmap 235.231598 Total loss 2547.79956\n",
      "Epoch 297 Step 880/5304 Paf1 762.36438 Paf2 732.134521 Paf3 728.949829 Heatmap 236.57608 Total loss 2460.0249\n",
      "Epoch 297 Step 890/5304 Paf1 601.270386 Paf2 559.638916 Paf3 563.767151 Heatmap 155.367279 Total loss 1880.0437\n",
      "Epoch 297 Step 900/5304 Paf1 558.414185 Paf2 506.616913 Paf3 507.661652 Heatmap 169.235016 Total loss 1741.92773\n",
      "Epoch 297 Step 910/5304 Paf1 710.186646 Paf2 672.367432 Paf3 678.269287 Heatmap 231.49353 Total loss 2292.31689\n",
      "Epoch 297 Step 920/5304 Paf1 597.85553 Paf2 578.996277 Paf3 544.393127 Heatmap 190.006363 Total loss 1911.25122\n",
      "Epoch 297 Step 930/5304 Paf1 662.243591 Paf2 611.443298 Paf3 603.143066 Heatmap 191.898697 Total loss 2068.72852\n",
      "Epoch 297 Step 940/5304 Paf1 980.780762 Paf2 932.97113 Paf3 914.04657 Heatmap 292.397888 Total loss 3120.19629\n",
      "Epoch 297 Step 950/5304 Paf1 509.746643 Paf2 475.147705 Paf3 461.078888 Heatmap 169.648743 Total loss 1615.62207\n",
      "Epoch 297 Step 960/5304 Paf1 533.104858 Paf2 506.774658 Paf3 500.335297 Heatmap 127.044022 Total loss 1667.25879\n",
      "Epoch 297 Step 970/5304 Paf1 931.213379 Paf2 882.983521 Paf3 860.344421 Heatmap 290.179657 Total loss 2964.72095\n",
      "Epoch 297 Step 980/5304 Paf1 584.758667 Paf2 529.419495 Paf3 537.623291 Heatmap 196.039398 Total loss 1847.84094\n",
      "Epoch 297 Step 990/5304 Paf1 745.17749 Paf2 702.559448 Paf3 699.673401 Heatmap 225.966949 Total loss 2373.37744\n",
      "Epoch 297 Step 1000/5304 Paf1 400.956299 Paf2 374.204 Paf3 373.754303 Heatmap 114.616562 Total loss 1263.53113\n",
      "Saved checkpoint for step 1000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1777\n",
      "Epoch 297 Step 1010/5304 Paf1 648.746765 Paf2 623.127 Paf3 608.393433 Heatmap 236.910782 Total loss 2117.17798\n",
      "Epoch 297 Step 1020/5304 Paf1 678.496887 Paf2 636.140137 Paf3 627.633545 Heatmap 192.036713 Total loss 2134.30713\n",
      "Epoch 297 Step 1030/5304 Paf1 732.200134 Paf2 707.240234 Paf3 705.074341 Heatmap 225.561615 Total loss 2370.07642\n",
      "Epoch 297 Step 1040/5304 Paf1 674.607422 Paf2 611.673462 Paf3 595.966309 Heatmap 201.432953 Total loss 2083.68018\n",
      "Epoch 297 Step 1050/5304 Paf1 659.746399 Paf2 607.468079 Paf3 616.447327 Heatmap 182.890198 Total loss 2066.552\n",
      "Epoch 297 Step 1060/5304 Paf1 670.048828 Paf2 630.009216 Paf3 629.534302 Heatmap 206.679489 Total loss 2136.27197\n",
      "Epoch 297 Step 1070/5304 Paf1 700.987 Paf2 659.213623 Paf3 649.153076 Heatmap 173.329559 Total loss 2182.68335\n",
      "Epoch 297 Step 1080/5304 Paf1 628.284912 Paf2 594.315552 Paf3 577.167908 Heatmap 196.786 Total loss 1996.55444\n",
      "Epoch 297 Step 1090/5304 Paf1 1068.86694 Paf2 1030.44128 Paf3 1009.73206 Heatmap 341.309143 Total loss 3450.34937\n",
      "Epoch 297 Step 1100/5304 Paf1 785.055847 Paf2 725.912292 Paf3 722.088501 Heatmap 232.614502 Total loss 2465.67114\n",
      "Epoch 297 Step 1110/5304 Paf1 828.082092 Paf2 773.08728 Paf3 780.383789 Heatmap 235.921692 Total loss 2617.47485\n",
      "Epoch 297 Step 1120/5304 Paf1 497.164825 Paf2 444.992584 Paf3 460.336975 Heatmap 133.936066 Total loss 1536.43042\n",
      "Epoch 297 Step 1130/5304 Paf1 688.597168 Paf2 624.372681 Paf3 616.305603 Heatmap 219.109543 Total loss 2148.38501\n",
      "Epoch 297 Step 1140/5304 Paf1 842.721924 Paf2 800.731 Paf3 796.17688 Heatmap 286.330505 Total loss 2725.96021\n",
      "Epoch 297 Step 1150/5304 Paf1 830.549 Paf2 792.177795 Paf3 784.481567 Heatmap 229.131378 Total loss 2636.33984\n",
      "Epoch 297 Step 1160/5304 Paf1 636.381958 Paf2 586.236328 Paf3 574.503967 Heatmap 188.719635 Total loss 1985.84192\n",
      "Epoch 297 Step 1170/5304 Paf1 593.693726 Paf2 544.063965 Paf3 556.906799 Heatmap 197.061844 Total loss 1891.72632\n",
      "Epoch 297 Step 1180/5304 Paf1 702.24707 Paf2 659.685669 Paf3 645.105103 Heatmap 207.017059 Total loss 2214.05493\n",
      "Epoch 297 Step 1190/5304 Paf1 538.518 Paf2 514.50824 Paf3 504.407776 Heatmap 174.935333 Total loss 1732.36938\n",
      "Epoch 297 Step 1200/5304 Paf1 889.897461 Paf2 820.105957 Paf3 860.449524 Heatmap 279.554565 Total loss 2850.00757\n",
      "Epoch 297 Step 1210/5304 Paf1 957.322693 Paf2 918.591675 Paf3 905.566162 Heatmap 310.887543 Total loss 3092.36816\n",
      "Epoch 297 Step 1220/5304 Paf1 835.114258 Paf2 806.690613 Paf3 806.010681 Heatmap 293.871155 Total loss 2741.68677\n",
      "Epoch 297 Step 1230/5304 Paf1 771.65094 Paf2 728.496826 Paf3 720.275208 Heatmap 251.935837 Total loss 2472.35889\n",
      "Epoch 297 Step 1240/5304 Paf1 523.718445 Paf2 472.874298 Paf3 485.060944 Heatmap 161.820618 Total loss 1643.47437\n",
      "Epoch 297 Step 1250/5304 Paf1 697.279907 Paf2 637.514648 Paf3 649.666565 Heatmap 217.598938 Total loss 2202.06\n",
      "Epoch 297 Step 1260/5304 Paf1 837.918213 Paf2 781.801819 Paf3 789.352112 Heatmap 247.561279 Total loss 2656.6333\n",
      "Epoch 297 Step 1270/5304 Paf1 786.523254 Paf2 705.909668 Paf3 715.430359 Heatmap 214.24704 Total loss 2422.11035\n",
      "Epoch 297 Step 1280/5304 Paf1 840.13208 Paf2 769.161499 Paf3 744.069092 Heatmap 248.191711 Total loss 2601.55444\n",
      "Epoch 297 Step 1290/5304 Paf1 815.008911 Paf2 787.92627 Paf3 796.301758 Heatmap 271.581024 Total loss 2670.81787\n",
      "Epoch 297 Step 1300/5304 Paf1 484.65506 Paf2 448.086029 Paf3 443.173279 Heatmap 138.693085 Total loss 1514.60742\n",
      "Epoch 297 Step 1310/5304 Paf1 815.235046 Paf2 778.056335 Paf3 775.925171 Heatmap 275.927673 Total loss 2645.14404\n",
      "Epoch 297 Step 1320/5304 Paf1 1098.77063 Paf2 1061.05249 Paf3 1048.97253 Heatmap 366.644653 Total loss 3575.44043\n",
      "Epoch 297 Step 1330/5304 Paf1 414.5896 Paf2 380.322296 Paf3 380.099518 Heatmap 92.8349 Total loss 1267.84631\n",
      "Epoch 297 Step 1340/5304 Paf1 680.660522 Paf2 630.978455 Paf3 630.758789 Heatmap 232.524841 Total loss 2174.92261\n",
      "Epoch 297 Step 1350/5304 Paf1 774.645 Paf2 702.837708 Paf3 721.057312 Heatmap 232.439682 Total loss 2430.97974\n",
      "Epoch 297 Step 1360/5304 Paf1 532.806702 Paf2 507.135223 Paf3 511.600098 Heatmap 143.779846 Total loss 1695.32178\n",
      "Epoch 297 Step 1370/5304 Paf1 592.591125 Paf2 565.607605 Paf3 553.636047 Heatmap 149.842209 Total loss 1861.677\n",
      "Epoch 297 Step 1380/5304 Paf1 416.710144 Paf2 382.884705 Paf3 372.844727 Heatmap 134.292053 Total loss 1306.73169\n",
      "Epoch 297 Step 1390/5304 Paf1 765.425354 Paf2 721.601929 Paf3 704.681396 Heatmap 251.374542 Total loss 2443.08325\n",
      "Epoch 297 Step 1400/5304 Paf1 657.993042 Paf2 609.524048 Paf3 610.270874 Heatmap 201.493546 Total loss 2079.28149\n",
      "Epoch 297 Step 1410/5304 Paf1 792.348938 Paf2 723.813965 Paf3 721.974121 Heatmap 271.260803 Total loss 2509.39771\n",
      "Epoch 297 Step 1420/5304 Paf1 599.505737 Paf2 562.832458 Paf3 550.615356 Heatmap 171.945343 Total loss 1884.8988\n",
      "Epoch 297 Step 1430/5304 Paf1 813.206909 Paf2 757.231201 Paf3 771.223633 Heatmap 252.339722 Total loss 2594.00146\n",
      "Epoch 297 Step 1440/5304 Paf1 751.878357 Paf2 698.682434 Paf3 699.048889 Heatmap 208.434372 Total loss 2358.04395\n",
      "Epoch 297 Step 1450/5304 Paf1 566.786438 Paf2 527.191772 Paf3 530.342896 Heatmap 176.396027 Total loss 1800.71716\n",
      "Epoch 297 Step 1460/5304 Paf1 725.600403 Paf2 669.353821 Paf3 674.509827 Heatmap 229.339691 Total loss 2298.80371\n",
      "Epoch 297 Step 1470/5304 Paf1 470.775452 Paf2 429.811951 Paf3 437.776337 Heatmap 137.824554 Total loss 1476.18823\n",
      "Epoch 297 Step 1480/5304 Paf1 783.989502 Paf2 753.771 Paf3 750.227783 Heatmap 308.241516 Total loss 2596.22974\n",
      "Epoch 297 Step 1490/5304 Paf1 722.905396 Paf2 686.222046 Paf3 677.501099 Heatmap 229.95015 Total loss 2316.57861\n",
      "Epoch 297 Step 1500/5304 Paf1 1092.94116 Paf2 1056.18591 Paf3 1057.13025 Heatmap 384.0737 Total loss 3590.33105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297 Step 1510/5304 Paf1 639.260559 Paf2 601.018799 Paf3 604.461914 Heatmap 227.074646 Total loss 2071.81592\n",
      "Epoch 297 Step 1520/5304 Paf1 622.672363 Paf2 577.971619 Paf3 579.437378 Heatmap 163.047623 Total loss 1943.12903\n",
      "Epoch 297 Step 1530/5304 Paf1 805.897217 Paf2 770.154724 Paf3 766.299 Heatmap 251.54509 Total loss 2593.896\n",
      "Epoch 297 Step 1540/5304 Paf1 599.725159 Paf2 528.336426 Paf3 534.756775 Heatmap 157.036362 Total loss 1819.85474\n",
      "Epoch 297 Step 1550/5304 Paf1 538.388306 Paf2 488.328552 Paf3 479.798279 Heatmap 163.63562 Total loss 1670.15063\n",
      "Epoch 297 Step 1560/5304 Paf1 857.368652 Paf2 782.898071 Paf3 782.778503 Heatmap 252.391357 Total loss 2675.43652\n",
      "Epoch 297 Step 1570/5304 Paf1 909.072937 Paf2 871.842285 Paf3 884.128723 Heatmap 377.10733 Total loss 3042.15137\n",
      "Epoch 297 Step 1580/5304 Paf1 477.677979 Paf2 432.445862 Paf3 426.798309 Heatmap 154.387512 Total loss 1491.30957\n",
      "Epoch 297 Step 1590/5304 Paf1 593.659424 Paf2 547.477478 Paf3 527.633545 Heatmap 204.779022 Total loss 1873.54956\n",
      "Epoch 297 Step 1600/5304 Paf1 688.724915 Paf2 626.236389 Paf3 643.235596 Heatmap 216.926727 Total loss 2175.12354\n",
      "Epoch 297 Step 1610/5304 Paf1 833.098877 Paf2 792.859253 Paf3 800.26062 Heatmap 278.274353 Total loss 2704.49316\n",
      "Epoch 297 Step 1620/5304 Paf1 580.989746 Paf2 548.088257 Paf3 558.103 Heatmap 197.228195 Total loss 1884.40918\n",
      "Epoch 297 Step 1630/5304 Paf1 611.855835 Paf2 575.443481 Paf3 571.351318 Heatmap 180.956223 Total loss 1939.60693\n",
      "Epoch 297 Step 1640/5304 Paf1 763.850952 Paf2 721.473267 Paf3 729.632324 Heatmap 260.415741 Total loss 2475.37231\n",
      "Epoch 297 Step 1650/5304 Paf1 786.857361 Paf2 737.21936 Paf3 720.61853 Heatmap 250.231705 Total loss 2494.92676\n",
      "Epoch 297 Step 1660/5304 Paf1 993.263916 Paf2 930.286194 Paf3 929.984497 Heatmap 307.053528 Total loss 3160.58813\n",
      "Epoch 297 Step 1670/5304 Paf1 545.123657 Paf2 512.179504 Paf3 527.114624 Heatmap 164.828842 Total loss 1749.2467\n",
      "Epoch 297 Step 1680/5304 Paf1 325.252197 Paf2 304.008057 Paf3 297.20047 Heatmap 89.7117081 Total loss 1016.17242\n",
      "Epoch 297 Step 1690/5304 Paf1 794.633911 Paf2 753.287354 Paf3 729.470764 Heatmap 241.831329 Total loss 2519.22339\n",
      "Epoch 297 Step 1700/5304 Paf1 597.498657 Paf2 562.895 Paf3 564.791138 Heatmap 179.203979 Total loss 1904.38879\n",
      "Epoch 297 Step 1710/5304 Paf1 774.698792 Paf2 732.874878 Paf3 742.348328 Heatmap 225.65065 Total loss 2475.57275\n",
      "Epoch 297 Step 1720/5304 Paf1 574.130432 Paf2 547.387329 Paf3 545.333801 Heatmap 181.045258 Total loss 1847.89685\n",
      "Epoch 297 Step 1730/5304 Paf1 562.910889 Paf2 518.035828 Paf3 516.155396 Heatmap 170.922089 Total loss 1768.02429\n",
      "Epoch 297 Step 1740/5304 Paf1 550.800537 Paf2 519.818604 Paf3 523.188904 Heatmap 175.611725 Total loss 1769.4198\n",
      "Epoch 297 Step 1750/5304 Paf1 529.642761 Paf2 487.235229 Paf3 491.146454 Heatmap 158.214081 Total loss 1666.23853\n",
      "Epoch 297 Step 1760/5304 Paf1 423.800262 Paf2 401.105042 Paf3 398.049713 Heatmap 150.253815 Total loss 1373.20874\n",
      "Epoch 297 Step 1770/5304 Paf1 523.715881 Paf2 481.615 Paf3 495.722229 Heatmap 131.147949 Total loss 1632.20105\n",
      "Epoch 297 Step 1780/5304 Paf1 704.697815 Paf2 652.592834 Paf3 646.643555 Heatmap 200.425827 Total loss 2204.36\n",
      "Epoch 297 Step 1790/5304 Paf1 874.348755 Paf2 832.28 Paf3 814.475098 Heatmap 263.751221 Total loss 2784.85498\n",
      "Epoch 297 Step 1800/5304 Paf1 800.934265 Paf2 775.870605 Paf3 770.694641 Heatmap 254.841751 Total loss 2602.34131\n",
      "Epoch 297 Step 1810/5304 Paf1 758.583191 Paf2 716.692505 Paf3 725.139954 Heatmap 257.936584 Total loss 2458.35205\n",
      "Epoch 297 Step 1820/5304 Paf1 732.347839 Paf2 727.56488 Paf3 719.127197 Heatmap 244.285278 Total loss 2423.3252\n",
      "Epoch 297 Step 1830/5304 Paf1 670.866272 Paf2 636.907898 Paf3 617.234619 Heatmap 197.45929 Total loss 2122.46802\n",
      "Epoch 297 Step 1840/5304 Paf1 676.611389 Paf2 658.46991 Paf3 669.550659 Heatmap 216.931671 Total loss 2221.56348\n",
      "Epoch 297 Step 1850/5304 Paf1 733.26532 Paf2 689.06 Paf3 676.109253 Heatmap 223.890289 Total loss 2322.32471\n",
      "Epoch 297 Step 1860/5304 Paf1 900.470459 Paf2 867.59375 Paf3 866.79364 Heatmap 276.852722 Total loss 2911.71045\n",
      "Epoch 297 Step 1870/5304 Paf1 479.001648 Paf2 460.824097 Paf3 467.006714 Heatmap 128.208801 Total loss 1535.04126\n",
      "Epoch 297 Step 1880/5304 Paf1 653.706787 Paf2 617.369263 Paf3 603.71875 Heatmap 198.583603 Total loss 2073.37842\n",
      "Epoch 297 Step 1890/5304 Paf1 744.118042 Paf2 693.222656 Paf3 704.171 Heatmap 271.927368 Total loss 2413.43896\n",
      "Epoch 297 Step 1900/5304 Paf1 581.777 Paf2 548.666626 Paf3 533.75 Heatmap 174.704529 Total loss 1838.89819\n",
      "Epoch 297 Step 1910/5304 Paf1 825.089478 Paf2 811.60614 Paf3 795.625244 Heatmap 252.008041 Total loss 2684.32886\n",
      "Epoch 297 Step 1920/5304 Paf1 538.404297 Paf2 523.174805 Paf3 521.762817 Heatmap 202.403198 Total loss 1785.74512\n",
      "Epoch 297 Step 1930/5304 Paf1 484.638245 Paf2 454.502655 Paf3 451.118835 Heatmap 141.806671 Total loss 1532.06641\n",
      "Epoch 297 Step 1940/5304 Paf1 978.74353 Paf2 939.758606 Paf3 924.746826 Heatmap 287.145874 Total loss 3130.39502\n",
      "Epoch 297 Step 1950/5304 Paf1 806.125916 Paf2 772.399292 Paf3 766.80011 Heatmap 232.12207 Total loss 2577.44727\n",
      "Epoch 297 Step 1960/5304 Paf1 720.089905 Paf2 668.04187 Paf3 667.189575 Heatmap 201.335327 Total loss 2256.65674\n",
      "Epoch 297 Step 1970/5304 Paf1 693.669434 Paf2 660.999451 Paf3 670.609192 Heatmap 199.836731 Total loss 2225.11475\n",
      "Epoch 297 Step 1980/5304 Paf1 722.486206 Paf2 677.614197 Paf3 675.460938 Heatmap 210.599 Total loss 2286.16016\n",
      "Epoch 297 Step 1990/5304 Paf1 1192.45166 Paf2 1157.25781 Paf3 1130.02637 Heatmap 418.748322 Total loss 3898.48413\n",
      "Epoch 297 Step 2000/5304 Paf1 586.361 Paf2 535.236267 Paf3 524.098572 Heatmap 166.732925 Total loss 1812.42871\n",
      "Saved checkpoint for step 2000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1778\n",
      "Epoch 297 Step 2010/5304 Paf1 722.335876 Paf2 730.086304 Paf3 715.551575 Heatmap 263.192017 Total loss 2431.16577\n",
      "Epoch 297 Step 2020/5304 Paf1 681.535095 Paf2 640.003601 Paf3 631.674255 Heatmap 217.395554 Total loss 2170.6084\n",
      "Epoch 297 Step 2030/5304 Paf1 593.631958 Paf2 526.694153 Paf3 538.431213 Heatmap 187.593262 Total loss 1846.35059\n",
      "Epoch 297 Step 2040/5304 Paf1 804.02356 Paf2 770.942749 Paf3 746.100708 Heatmap 266.721039 Total loss 2587.78809\n",
      "Epoch 297 Step 2050/5304 Paf1 508.042603 Paf2 499.525391 Paf3 488.486816 Heatmap 173.556763 Total loss 1669.61157\n",
      "Epoch 297 Step 2060/5304 Paf1 861.325317 Paf2 784.188721 Paf3 797.185181 Heatmap 255.213593 Total loss 2697.91284\n",
      "Epoch 297 Step 2070/5304 Paf1 558.975647 Paf2 531.219055 Paf3 514.592957 Heatmap 159.608368 Total loss 1764.396\n",
      "Epoch 297 Step 2080/5304 Paf1 637.68811 Paf2 615.511292 Paf3 614.146851 Heatmap 193.338013 Total loss 2060.68433\n",
      "Epoch 297 Step 2090/5304 Paf1 613.494141 Paf2 562.838074 Paf3 554.938 Heatmap 198.51535 Total loss 1929.78564\n",
      "Epoch 297 Step 2100/5304 Paf1 780.749146 Paf2 734.123596 Paf3 720.470276 Heatmap 217.839569 Total loss 2453.18262\n",
      "Epoch 297 Step 2110/5304 Paf1 767.49762 Paf2 720.025 Paf3 705.808167 Heatmap 252.987534 Total loss 2446.31836\n",
      "Epoch 297 Step 2120/5304 Paf1 527.768555 Paf2 501.374939 Paf3 492.054932 Heatmap 163.095581 Total loss 1684.29407\n",
      "Epoch 297 Step 2130/5304 Paf1 786.571899 Paf2 754.872498 Paf3 760.909912 Heatmap 324.363708 Total loss 2626.71802\n",
      "Epoch 297 Step 2140/5304 Paf1 997.945923 Paf2 937.856445 Paf3 951.722046 Heatmap 357.275452 Total loss 3244.7998\n",
      "Epoch 297 Step 2150/5304 Paf1 838.243958 Paf2 808.735962 Paf3 775.425 Heatmap 265.343231 Total loss 2687.74805\n",
      "Epoch 297 Step 2160/5304 Paf1 488.039124 Paf2 459.080475 Paf3 475.393829 Heatmap 137.042664 Total loss 1559.55615\n",
      "Epoch 297 Step 2170/5304 Paf1 848.694397 Paf2 807.614502 Paf3 810.095 Heatmap 257.54718 Total loss 2723.95093\n",
      "Epoch 297 Step 2180/5304 Paf1 1272.33984 Paf2 1185.68152 Paf3 1195.53369 Heatmap 452.402954 Total loss 4105.95801\n",
      "Epoch 297 Step 2190/5304 Paf1 497.665161 Paf2 469.19046 Paf3 458.029724 Heatmap 148.275757 Total loss 1573.16113\n",
      "Epoch 297 Step 2200/5304 Paf1 888.459839 Paf2 859.788452 Paf3 868.195374 Heatmap 273.165039 Total loss 2889.60864\n",
      "Epoch 297 Step 2210/5304 Paf1 729.098389 Paf2 698.606 Paf3 699.429443 Heatmap 210.704498 Total loss 2337.83838\n",
      "Epoch 297 Step 2220/5304 Paf1 889.256226 Paf2 831.851074 Paf3 850.146729 Heatmap 295.357513 Total loss 2866.61157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297 Step 2230/5304 Paf1 1008.60095 Paf2 953.109253 Paf3 944.185913 Heatmap 307.336975 Total loss 3213.23315\n",
      "Epoch 297 Step 2240/5304 Paf1 473.105713 Paf2 457.888489 Paf3 462.577728 Heatmap 188.625992 Total loss 1582.198\n",
      "Epoch 297 Step 2250/5304 Paf1 1314.34912 Paf2 1221.87158 Paf3 1239.82214 Heatmap 415.236145 Total loss 4191.2793\n",
      "Epoch 297 Step 2260/5304 Paf1 582.105652 Paf2 553.656555 Paf3 558.07666 Heatmap 179.535126 Total loss 1873.37402\n",
      "Epoch 297 Step 2270/5304 Paf1 503.140656 Paf2 464.197632 Paf3 456.597473 Heatmap 134.757935 Total loss 1558.6936\n",
      "Epoch 297 Step 2280/5304 Paf1 811.681824 Paf2 768.642456 Paf3 747.628174 Heatmap 263.030029 Total loss 2590.98242\n",
      "Epoch 297 Step 2290/5304 Paf1 419.162384 Paf2 382.98822 Paf3 379.085785 Heatmap 138.223328 Total loss 1319.45972\n",
      "Epoch 297 Step 2300/5304 Paf1 522.509888 Paf2 495.953247 Paf3 486.222595 Heatmap 152.154175 Total loss 1656.83984\n",
      "Epoch 297 Step 2310/5304 Paf1 740.374146 Paf2 691.880737 Paf3 692.211548 Heatmap 240.966354 Total loss 2365.43286\n",
      "Epoch 297 Step 2320/5304 Paf1 679.523621 Paf2 656.150452 Paf3 643.121399 Heatmap 233.95636 Total loss 2212.75195\n",
      "Epoch 297 Step 2330/5304 Paf1 515.405518 Paf2 490.564819 Paf3 492.877502 Heatmap 176.917542 Total loss 1675.76538\n",
      "Epoch 297 Step 2340/5304 Paf1 812.055298 Paf2 777.910583 Paf3 770.61615 Heatmap 291.358887 Total loss 2651.94092\n",
      "Epoch 297 Step 2350/5304 Paf1 988.203674 Paf2 928.830078 Paf3 925.684814 Heatmap 345.825867 Total loss 3188.54443\n",
      "Epoch 297 Step 2360/5304 Paf1 713.569092 Paf2 673.66449 Paf3 662.112183 Heatmap 214.809 Total loss 2264.15479\n",
      "Epoch 297 Step 2370/5304 Paf1 559.749817 Paf2 513.914673 Paf3 504.172241 Heatmap 150.630432 Total loss 1728.46729\n",
      "Epoch 297 Step 2380/5304 Paf1 534.308533 Paf2 487.500854 Paf3 497.559174 Heatmap 189.08197 Total loss 1708.45044\n",
      "Epoch 297 Step 2390/5304 Paf1 688.800476 Paf2 643.571533 Paf3 635.942444 Heatmap 207.9758 Total loss 2176.29028\n",
      "Epoch 297 Step 2400/5304 Paf1 700.470947 Paf2 662.914917 Paf3 650.549866 Heatmap 240.773743 Total loss 2254.70947\n",
      "Epoch 297 Step 2410/5304 Paf1 610.653 Paf2 597.156311 Paf3 591.999084 Heatmap 286.003906 Total loss 2085.81226\n",
      "Epoch 297 Step 2420/5304 Paf1 525.40509 Paf2 497.518829 Paf3 482.848816 Heatmap 132.578415 Total loss 1638.3512\n",
      "Epoch 297 Step 2430/5304 Paf1 739.922485 Paf2 731.199768 Paf3 743.689453 Heatmap 248.239777 Total loss 2463.05151\n",
      "Epoch 297 Step 2440/5304 Paf1 907.680786 Paf2 822.031555 Paf3 826.481 Heatmap 289.985596 Total loss 2846.17896\n",
      "Epoch 297 Step 2450/5304 Paf1 576.816406 Paf2 537.028259 Paf3 535.228 Heatmap 180.383057 Total loss 1829.45581\n",
      "Epoch 297 Step 2460/5304 Paf1 483.181213 Paf2 436.201294 Paf3 435.044 Heatmap 115.587379 Total loss 1470.01392\n",
      "Epoch 297 Step 2470/5304 Paf1 626.354187 Paf2 592.724792 Paf3 591.539062 Heatmap 233.145401 Total loss 2043.76343\n",
      "Epoch 297 Step 2480/5304 Paf1 745.147339 Paf2 693.916443 Paf3 693.256042 Heatmap 225.834869 Total loss 2358.15479\n",
      "Epoch 297 Step 2490/5304 Paf1 599.710083 Paf2 548.220032 Paf3 544.639587 Heatmap 165.63707 Total loss 1858.20679\n",
      "Epoch 297 Step 2500/5304 Paf1 649.243713 Paf2 634.603088 Paf3 637.394 Heatmap 170.807678 Total loss 2092.04834\n",
      "Epoch 297 Step 2510/5304 Paf1 733.248657 Paf2 708.317627 Paf3 699.075684 Heatmap 263.786743 Total loss 2404.42871\n",
      "Epoch 297 Step 2520/5304 Paf1 1054.66223 Paf2 976.144775 Paf3 969.040527 Heatmap 333.533173 Total loss 3333.38086\n",
      "Epoch 297 Step 2530/5304 Paf1 634.351318 Paf2 593.376221 Paf3 595.653625 Heatmap 190.930771 Total loss 2014.31201\n",
      "Epoch 297 Step 2540/5304 Paf1 816.912231 Paf2 761.270081 Paf3 753.442322 Heatmap 262.000488 Total loss 2593.62524\n",
      "Epoch 297 Step 2550/5304 Paf1 654.04248 Paf2 609.825073 Paf3 623.747314 Heatmap 208.777405 Total loss 2096.39233\n",
      "Epoch 297 Step 2560/5304 Paf1 680.369507 Paf2 667.532715 Paf3 647.961914 Heatmap 199.827209 Total loss 2195.69141\n",
      "Epoch 297 Step 2570/5304 Paf1 745.932739 Paf2 700.727539 Paf3 670.990723 Heatmap 228.419189 Total loss 2346.07031\n",
      "Epoch 297 Step 2580/5304 Paf1 438.39325 Paf2 410.140198 Paf3 405.789856 Heatmap 140.449539 Total loss 1394.77283\n",
      "Epoch 297 Step 2590/5304 Paf1 632.89856 Paf2 576.446533 Paf3 591.53009 Heatmap 191.079865 Total loss 1991.95508\n",
      "Epoch 297 Step 2600/5304 Paf1 699.571655 Paf2 646.444885 Paf3 634.466797 Heatmap 177.762909 Total loss 2158.24634\n",
      "Epoch 297 Step 2610/5304 Paf1 645.253235 Paf2 610.109253 Paf3 606.191772 Heatmap 172.259659 Total loss 2033.81396\n",
      "Epoch 297 Step 2620/5304 Paf1 725.784363 Paf2 666.575806 Paf3 657.263672 Heatmap 223.503113 Total loss 2273.12695\n",
      "Epoch 297 Step 2630/5304 Paf1 813.74292 Paf2 784.411072 Paf3 778.129211 Heatmap 263.790863 Total loss 2640.07422\n",
      "Epoch 297 Step 2640/5304 Paf1 549.487915 Paf2 533.050049 Paf3 534.153931 Heatmap 189.522812 Total loss 1806.21472\n",
      "Epoch 297 Step 2650/5304 Paf1 779.678955 Paf2 746.140808 Paf3 732.170654 Heatmap 247.018494 Total loss 2505.00903\n",
      "Epoch 297 Step 2660/5304 Paf1 886.433105 Paf2 818.389832 Paf3 826.886719 Heatmap 255.773773 Total loss 2787.4834\n",
      "Epoch 297 Step 2670/5304 Paf1 801.240601 Paf2 744.365234 Paf3 750.730591 Heatmap 241.793793 Total loss 2538.13037\n",
      "Epoch 297 Step 2680/5304 Paf1 564.991211 Paf2 543.758667 Paf3 525.071411 Heatmap 168.467529 Total loss 1802.28882\n",
      "Epoch 297 Step 2690/5304 Paf1 620.192566 Paf2 580.726135 Paf3 579.42572 Heatmap 177.404282 Total loss 1957.74878\n",
      "Epoch 297 Step 2700/5304 Paf1 732.719421 Paf2 682.396057 Paf3 676.124329 Heatmap 234.745758 Total loss 2325.9856\n",
      "Epoch 297 Step 2710/5304 Paf1 727.269 Paf2 663.308411 Paf3 656.516296 Heatmap 188.426102 Total loss 2235.51978\n",
      "Epoch 297 Step 2720/5304 Paf1 814.8172 Paf2 763.812073 Paf3 749.001404 Heatmap 243.450745 Total loss 2571.08154\n",
      "Epoch 297 Step 2730/5304 Paf1 710.155273 Paf2 654.038208 Paf3 668.7948 Heatmap 255.898041 Total loss 2288.88623\n",
      "Epoch 297 Step 2740/5304 Paf1 795.414795 Paf2 739.702576 Paf3 740.916687 Heatmap 249.971588 Total loss 2526.00586\n",
      "Epoch 297 Step 2750/5304 Paf1 559.494385 Paf2 527.761292 Paf3 536.345581 Heatmap 195.154495 Total loss 1818.75562\n",
      "Epoch 297 Step 2760/5304 Paf1 527.980713 Paf2 465.91983 Paf3 461.03653 Heatmap 146.000259 Total loss 1600.93726\n",
      "Epoch 297 Step 2770/5304 Paf1 380.164856 Paf2 347.160309 Paf3 350.330688 Heatmap 99.1022644 Total loss 1176.75818\n",
      "Epoch 297 Step 2780/5304 Paf1 1161.44531 Paf2 1126.21313 Paf3 1097.50513 Heatmap 393.266052 Total loss 3778.42969\n",
      "Epoch 297 Step 2790/5304 Paf1 861.909607 Paf2 783.741638 Paf3 793.025635 Heatmap 266.429169 Total loss 2705.10596\n",
      "Epoch 297 Step 2800/5304 Paf1 954.47821 Paf2 883.243042 Paf3 885.503 Heatmap 310.740021 Total loss 3033.96436\n",
      "Epoch 297 Step 2810/5304 Paf1 673.12 Paf2 653.837769 Paf3 646.565186 Heatmap 212.17952 Total loss 2185.70239\n",
      "Epoch 297 Step 2820/5304 Paf1 710.433655 Paf2 675.180603 Paf3 662.507874 Heatmap 246.041931 Total loss 2294.16406\n",
      "Epoch 297 Step 2830/5304 Paf1 617.452 Paf2 598.367065 Paf3 591.282776 Heatmap 212.994965 Total loss 2020.0968\n",
      "Epoch 297 Step 2840/5304 Paf1 567.435059 Paf2 506.032104 Paf3 511.28595 Heatmap 177.572754 Total loss 1762.32593\n",
      "Epoch 297 Step 2850/5304 Paf1 526.520752 Paf2 480.476379 Paf3 477.310974 Heatmap 142.786865 Total loss 1627.09497\n",
      "Epoch 297 Step 2860/5304 Paf1 494.296478 Paf2 452.258514 Paf3 450.551208 Heatmap 168.664703 Total loss 1565.771\n",
      "Epoch 297 Step 2870/5304 Paf1 589.653625 Paf2 537.279785 Paf3 538.614807 Heatmap 147.442291 Total loss 1812.99048\n",
      "Epoch 297 Step 2880/5304 Paf1 644.280762 Paf2 606.153625 Paf3 598.227905 Heatmap 186.894531 Total loss 2035.55676\n",
      "Epoch 297 Step 2890/5304 Paf1 599.984924 Paf2 583.221069 Paf3 576.517517 Heatmap 221.082092 Total loss 1980.80566\n",
      "Epoch 297 Step 2900/5304 Paf1 778.251831 Paf2 760.182617 Paf3 740.170288 Heatmap 257.255 Total loss 2535.85986\n",
      "Epoch 297 Step 2910/5304 Paf1 796.549133 Paf2 727.24646 Paf3 729.045715 Heatmap 201.100281 Total loss 2453.94165\n",
      "Epoch 297 Step 2920/5304 Paf1 907.058716 Paf2 844.365356 Paf3 857.576416 Heatmap 291.81427 Total loss 2900.8147\n",
      "Epoch 297 Step 2930/5304 Paf1 491.715 Paf2 463.815063 Paf3 450.46 Heatmap 132.812241 Total loss 1538.80225\n",
      "Epoch 297 Step 2940/5304 Paf1 993.636597 Paf2 961.07 Paf3 933.540588 Heatmap 307.819946 Total loss 3196.06714\n",
      "Epoch 297 Step 2950/5304 Paf1 766.397278 Paf2 646.457642 Paf3 656.428467 Heatmap 178.906586 Total loss 2248.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297 Step 2960/5304 Paf1 899.954285 Paf2 850.902222 Paf3 856.768738 Heatmap 304.070374 Total loss 2911.69556\n",
      "Epoch 297 Step 2970/5304 Paf1 650.617615 Paf2 614.375366 Paf3 623.888794 Heatmap 198.990799 Total loss 2087.87256\n",
      "Epoch 297 Step 2980/5304 Paf1 962.464783 Paf2 893.724365 Paf3 902.219604 Heatmap 381.314331 Total loss 3139.72314\n",
      "Epoch 297 Step 2990/5304 Paf1 990.120605 Paf2 957.463928 Paf3 976.775696 Heatmap 324.105377 Total loss 3248.46558\n",
      "Epoch 297 Step 3000/5304 Paf1 509.527069 Paf2 477.448273 Paf3 483.094421 Heatmap 163.181015 Total loss 1633.25073\n",
      "Saved checkpoint for step 3000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1779\n",
      "Epoch 297 Step 3010/5304 Paf1 536.212097 Paf2 504.132111 Paf3 508.785431 Heatmap 162.614822 Total loss 1711.74451\n",
      "Epoch 297 Step 3020/5304 Paf1 517.511719 Paf2 491.443176 Paf3 475.300293 Heatmap 140.016052 Total loss 1624.27124\n",
      "Epoch 297 Step 3030/5304 Paf1 749.51825 Paf2 675.798462 Paf3 684.402771 Heatmap 212.211212 Total loss 2321.93066\n",
      "Epoch 297 Step 3040/5304 Paf1 684.594849 Paf2 631.216797 Paf3 640.010559 Heatmap 196.526978 Total loss 2152.34912\n",
      "Epoch 297 Step 3050/5304 Paf1 929.815918 Paf2 859.657288 Paf3 882.454712 Heatmap 269.004272 Total loss 2940.93213\n",
      "Epoch 297 Step 3060/5304 Paf1 687.071838 Paf2 642.962646 Paf3 648.206055 Heatmap 207.739349 Total loss 2185.98\n",
      "Epoch 297 Step 3070/5304 Paf1 810.529785 Paf2 773.988281 Paf3 763.726379 Heatmap 226.564606 Total loss 2574.80908\n",
      "Epoch 297 Step 3080/5304 Paf1 671.377869 Paf2 620.559814 Paf3 630.68335 Heatmap 218.099976 Total loss 2140.72119\n",
      "Epoch 297 Step 3090/5304 Paf1 600.412598 Paf2 550.626831 Paf3 542.822754 Heatmap 152.000549 Total loss 1845.86279\n",
      "Epoch 297 Step 3100/5304 Paf1 711.163208 Paf2 700.778259 Paf3 697.779236 Heatmap 233.135223 Total loss 2342.85596\n",
      "Epoch 297 Step 3110/5304 Paf1 679.912964 Paf2 623.084412 Paf3 636.278931 Heatmap 194.697388 Total loss 2133.97363\n",
      "Epoch 297 Step 3120/5304 Paf1 678.718506 Paf2 640.306641 Paf3 622.812439 Heatmap 210.499542 Total loss 2152.33716\n",
      "Epoch 297 Step 3130/5304 Paf1 666.262634 Paf2 590.115295 Paf3 579.438721 Heatmap 173.267609 Total loss 2009.08423\n",
      "Epoch 297 Step 3140/5304 Paf1 826.067627 Paf2 804.31958 Paf3 783.029236 Heatmap 227.524811 Total loss 2640.94141\n",
      "Epoch 297 Step 3150/5304 Paf1 626.531616 Paf2 584.418213 Paf3 591.967651 Heatmap 183.449585 Total loss 1986.36707\n",
      "Epoch 297 Step 3160/5304 Paf1 580.601868 Paf2 534.939941 Paf3 528.539 Heatmap 167.96019 Total loss 1812.04102\n",
      "Epoch 297 Step 3170/5304 Paf1 763.790039 Paf2 730.710327 Paf3 726.150085 Heatmap 240.994797 Total loss 2461.64526\n",
      "Epoch 297 Step 3180/5304 Paf1 678.718262 Paf2 659.856079 Paf3 669.928711 Heatmap 194.730164 Total loss 2203.23315\n",
      "Epoch 297 Step 3190/5304 Paf1 729.459106 Paf2 673.780273 Paf3 686.770752 Heatmap 210.461456 Total loss 2300.47168\n",
      "Epoch 297 Step 3200/5304 Paf1 670.090515 Paf2 626.002563 Paf3 632.921387 Heatmap 193.705292 Total loss 2122.71973\n",
      "Epoch 297 Step 3210/5304 Paf1 750.351624 Paf2 720.106689 Paf3 720.25885 Heatmap 239.602173 Total loss 2430.31934\n",
      "Epoch 297 Step 3220/5304 Paf1 612.633 Paf2 569.406 Paf3 590.727417 Heatmap 199.493011 Total loss 1972.25952\n",
      "Epoch 297 Step 3230/5304 Paf1 813.453613 Paf2 756.279236 Paf3 738.212097 Heatmap 245.629807 Total loss 2553.57471\n",
      "Epoch 297 Step 3240/5304 Paf1 533.096497 Paf2 504.276764 Paf3 493.111908 Heatmap 179.281494 Total loss 1709.76672\n",
      "Epoch 297 Step 3250/5304 Paf1 553.658203 Paf2 534.296631 Paf3 531.165833 Heatmap 172.245453 Total loss 1791.36609\n",
      "Epoch 297 Step 3260/5304 Paf1 625.338745 Paf2 568.912781 Paf3 576.629639 Heatmap 180.411041 Total loss 1951.29211\n",
      "Epoch 297 Step 3270/5304 Paf1 717.11261 Paf2 664.658569 Paf3 672.86908 Heatmap 206.761917 Total loss 2261.40234\n",
      "Epoch 297 Step 3280/5304 Paf1 427.391602 Paf2 395.605774 Paf3 405.480255 Heatmap 117.025391 Total loss 1345.50293\n",
      "Epoch 297 Step 3290/5304 Paf1 543.930664 Paf2 515.960938 Paf3 511.056152 Heatmap 168.786514 Total loss 1739.73425\n",
      "Epoch 297 Step 3300/5304 Paf1 563.98877 Paf2 527.374817 Paf3 520.427795 Heatmap 152.110504 Total loss 1763.90186\n",
      "Epoch 297 Step 3310/5304 Paf1 778.225281 Paf2 734.334534 Paf3 730.893127 Heatmap 243.666748 Total loss 2487.11963\n",
      "Epoch 297 Step 3320/5304 Paf1 770.287598 Paf2 713.623169 Paf3 728.019897 Heatmap 222.167786 Total loss 2434.09839\n",
      "Epoch 297 Step 3330/5304 Paf1 821.397461 Paf2 774.016296 Paf3 766.443 Heatmap 244.744308 Total loss 2606.60107\n",
      "Epoch 297 Step 3340/5304 Paf1 621.406494 Paf2 566.20575 Paf3 570.130432 Heatmap 176.646622 Total loss 1934.3894\n",
      "Epoch 297 Step 3350/5304 Paf1 803.457947 Paf2 765.651489 Paf3 761.044312 Heatmap 245.739548 Total loss 2575.89331\n",
      "Epoch 297 Step 3360/5304 Paf1 805.735046 Paf2 749.602905 Paf3 759.638123 Heatmap 234.426514 Total loss 2549.40259\n",
      "Epoch 297 Step 3370/5304 Paf1 721.467285 Paf2 676.736938 Paf3 663.296204 Heatmap 210.927399 Total loss 2272.42773\n",
      "Epoch 297 Step 3380/5304 Paf1 581.250732 Paf2 537.964 Paf3 536.513245 Heatmap 212.617355 Total loss 1868.34534\n",
      "Epoch 297 Step 3390/5304 Paf1 627.344604 Paf2 609.537781 Paf3 605.80127 Heatmap 189.78981 Total loss 2032.47339\n",
      "Epoch 297 Step 3400/5304 Paf1 453.416138 Paf2 410.204742 Paf3 432.451172 Heatmap 111.463013 Total loss 1407.53503\n",
      "Epoch 297 Step 3410/5304 Paf1 746.295 Paf2 711.841431 Paf3 718.333679 Heatmap 263.741638 Total loss 2440.21191\n",
      "Epoch 297 Step 3420/5304 Paf1 816.452332 Paf2 749.488586 Paf3 762.628723 Heatmap 248.453 Total loss 2577.02271\n",
      "Epoch 297 Step 3430/5304 Paf1 682.325928 Paf2 664.48 Paf3 645.51416 Heatmap 222.477905 Total loss 2214.79785\n",
      "Epoch 297 Step 3440/5304 Paf1 649.847412 Paf2 613.529236 Paf3 599.812683 Heatmap 184.036713 Total loss 2047.22607\n",
      "Epoch 297 Step 3450/5304 Paf1 776.573608 Paf2 738.325195 Paf3 740.457214 Heatmap 262.95694 Total loss 2518.31299\n",
      "Epoch 297 Step 3460/5304 Paf1 604.120605 Paf2 550.005371 Paf3 542.887268 Heatmap 169.207184 Total loss 1866.22046\n",
      "Epoch 297 Step 3470/5304 Paf1 644.820801 Paf2 580.013367 Paf3 589.868 Heatmap 169.271301 Total loss 1983.97351\n",
      "Epoch 297 Step 3480/5304 Paf1 706.504089 Paf2 662.47 Paf3 665.119751 Heatmap 184.074554 Total loss 2218.16846\n",
      "Epoch 297 Step 3490/5304 Paf1 689.783081 Paf2 639.739929 Paf3 637.965454 Heatmap 242.82045 Total loss 2210.30884\n",
      "Epoch 297 Step 3500/5304 Paf1 635.147522 Paf2 583.088196 Paf3 584.728882 Heatmap 212.853622 Total loss 2015.81824\n",
      "Epoch 297 Step 3510/5304 Paf1 572.42688 Paf2 538.346497 Paf3 539.432129 Heatmap 155.685944 Total loss 1805.89148\n",
      "Epoch 297 Step 3520/5304 Paf1 750.159302 Paf2 667.090881 Paf3 668.419312 Heatmap 206.705109 Total loss 2292.37451\n",
      "Epoch 297 Step 3530/5304 Paf1 625.158386 Paf2 597.293762 Paf3 594.958496 Heatmap 231.737198 Total loss 2049.14795\n",
      "Epoch 297 Step 3540/5304 Paf1 728.311279 Paf2 679.056641 Paf3 692.063782 Heatmap 204.602417 Total loss 2304.03418\n",
      "Epoch 297 Step 3550/5304 Paf1 512.478455 Paf2 481.070221 Paf3 481.846252 Heatmap 171.844 Total loss 1647.23901\n",
      "Epoch 297 Step 3560/5304 Paf1 732.971924 Paf2 688.84668 Paf3 691.759 Heatmap 239.946198 Total loss 2353.52393\n",
      "Epoch 297 Step 3570/5304 Paf1 560.290161 Paf2 508.267456 Paf3 504.59848 Heatmap 158.679016 Total loss 1731.83508\n",
      "Epoch 297 Step 3580/5304 Paf1 571.605469 Paf2 528.455444 Paf3 520.365906 Heatmap 160.08316 Total loss 1780.51\n",
      "Epoch 297 Step 3590/5304 Paf1 817.954895 Paf2 788.138184 Paf3 787.996521 Heatmap 301.600647 Total loss 2695.69019\n",
      "Epoch 297 Step 3600/5304 Paf1 515.38 Paf2 476.116211 Paf3 477.58316 Heatmap 134.389709 Total loss 1603.46912\n",
      "Epoch 297 Step 3610/5304 Paf1 791.490112 Paf2 699.169678 Paf3 705.63 Heatmap 255.384918 Total loss 2451.6748\n",
      "Epoch 297 Step 3620/5304 Paf1 640.453796 Paf2 591.888428 Paf3 595.108826 Heatmap 220.761429 Total loss 2048.2124\n",
      "Epoch 297 Step 3630/5304 Paf1 610.121704 Paf2 592.348511 Paf3 572.363403 Heatmap 201.689819 Total loss 1976.52344\n",
      "Epoch 297 Step 3640/5304 Paf1 506.578339 Paf2 482.29071 Paf3 470.648376 Heatmap 158.786377 Total loss 1618.30371\n",
      "Epoch 297 Step 3650/5304 Paf1 753.124146 Paf2 689.494263 Paf3 701.46228 Heatmap 241.182159 Total loss 2385.2627\n",
      "Epoch 297 Step 3660/5304 Paf1 708.983 Paf2 659.842896 Paf3 672.707825 Heatmap 200.875244 Total loss 2242.40894\n",
      "Epoch 297 Step 3670/5304 Paf1 723.288879 Paf2 683.758179 Paf3 699.207336 Heatmap 245.274445 Total loss 2351.52881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297 Step 3680/5304 Paf1 1045.26233 Paf2 998.328613 Paf3 1006.08899 Heatmap 325.165375 Total loss 3374.84521\n",
      "Epoch 297 Step 3690/5304 Paf1 915.074951 Paf2 869.625305 Paf3 868.465881 Heatmap 345.3461 Total loss 2998.51221\n",
      "Epoch 297 Step 3700/5304 Paf1 938.990356 Paf2 878.146057 Paf3 891.200195 Heatmap 291.889221 Total loss 3000.22583\n",
      "Epoch 297 Step 3710/5304 Paf1 562.627441 Paf2 506.15213 Paf3 502.132416 Heatmap 160.004517 Total loss 1730.9165\n",
      "Epoch 297 Step 3720/5304 Paf1 578.368591 Paf2 552.246826 Paf3 556.901611 Heatmap 152.801453 Total loss 1840.3186\n",
      "Epoch 297 Step 3730/5304 Paf1 618.45752 Paf2 599.538513 Paf3 585.439087 Heatmap 161.190399 Total loss 1964.62561\n",
      "Epoch 297 Step 3740/5304 Paf1 578.472839 Paf2 539.637 Paf3 531.803833 Heatmap 175.653717 Total loss 1825.56738\n",
      "Epoch 297 Step 3750/5304 Paf1 724.281372 Paf2 696.157593 Paf3 690.404 Heatmap 237.515137 Total loss 2348.35815\n",
      "Epoch 297 Step 3760/5304 Paf1 525.783325 Paf2 489.299591 Paf3 474.892395 Heatmap 173.193054 Total loss 1663.16833\n",
      "Epoch 297 Step 3770/5304 Paf1 760.041443 Paf2 686.805481 Paf3 686.403076 Heatmap 193.41507 Total loss 2326.66504\n",
      "Epoch 297 Step 3780/5304 Paf1 1133.59241 Paf2 1075.93945 Paf3 1071.99268 Heatmap 346.489 Total loss 3628.01343\n",
      "Epoch 297 Step 3790/5304 Paf1 499.16156 Paf2 473.38681 Paf3 466.742126 Heatmap 128.009155 Total loss 1567.29956\n",
      "Epoch 297 Step 3800/5304 Paf1 667.503174 Paf2 625.580688 Paf3 619.820312 Heatmap 199.183472 Total loss 2112.08765\n",
      "Epoch 297 Step 3810/5304 Paf1 815.598694 Paf2 784.497925 Paf3 784.479065 Heatmap 257.019653 Total loss 2641.59546\n",
      "Epoch 297 Step 3820/5304 Paf1 901.943848 Paf2 862.537048 Paf3 861.236389 Heatmap 345.463867 Total loss 2971.18115\n",
      "Epoch 297 Step 3830/5304 Paf1 705.94104 Paf2 646.021667 Paf3 647.660461 Heatmap 207.298111 Total loss 2206.92114\n",
      "Epoch 297 Step 3840/5304 Paf1 647.858154 Paf2 598.820618 Paf3 599.653625 Heatmap 155.705109 Total loss 2002.03748\n",
      "Epoch 297 Step 3850/5304 Paf1 923.415283 Paf2 852.880432 Paf3 852.257874 Heatmap 264.607056 Total loss 2893.16064\n",
      "Epoch 297 Step 3860/5304 Paf1 546.908936 Paf2 511.08667 Paf3 516.498901 Heatmap 168.768234 Total loss 1743.2627\n",
      "Epoch 297 Step 3870/5304 Paf1 765.587646 Paf2 742.822144 Paf3 730.124634 Heatmap 222.180023 Total loss 2460.71436\n",
      "Epoch 297 Step 3880/5304 Paf1 789.922424 Paf2 771.192261 Paf3 746.68988 Heatmap 265.730103 Total loss 2573.53467\n",
      "Epoch 297 Step 3890/5304 Paf1 535.618 Paf2 474.999023 Paf3 477.265747 Heatmap 151.419601 Total loss 1639.30237\n",
      "Epoch 297 Step 3900/5304 Paf1 793.160645 Paf2 735.431458 Paf3 745.062744 Heatmap 235.814178 Total loss 2509.46899\n",
      "Epoch 297 Step 3910/5304 Paf1 859.112427 Paf2 786.716797 Paf3 788.85553 Heatmap 279.392487 Total loss 2714.07715\n",
      "Epoch 297 Step 3920/5304 Paf1 565.453796 Paf2 539.880188 Paf3 523.83667 Heatmap 163.607788 Total loss 1792.77844\n",
      "Epoch 297 Step 3930/5304 Paf1 680.132812 Paf2 647.051636 Paf3 664.469604 Heatmap 256.4375 Total loss 2248.09155\n",
      "Epoch 297 Step 3940/5304 Paf1 740.305847 Paf2 711.455322 Paf3 711.312134 Heatmap 206.968109 Total loss 2370.0415\n",
      "Epoch 297 Step 3950/5304 Paf1 601.029846 Paf2 576.312195 Paf3 577.86 Heatmap 174.924683 Total loss 1930.12671\n",
      "Epoch 297 Step 3960/5304 Paf1 876.771423 Paf2 824.316 Paf3 816.214844 Heatmap 299.722931 Total loss 2817.02515\n",
      "Epoch 297 Step 3970/5304 Paf1 709.249878 Paf2 656.337952 Paf3 665.913452 Heatmap 193.847427 Total loss 2225.34863\n",
      "Epoch 297 Step 3980/5304 Paf1 871.074829 Paf2 843.924744 Paf3 842.959 Heatmap 283.452 Total loss 2841.41064\n",
      "Epoch 297 Step 3990/5304 Paf1 423.728729 Paf2 382.566193 Paf3 380.56897 Heatmap 116.982529 Total loss 1303.84644\n",
      "Epoch 297 Step 4000/5304 Paf1 907.248352 Paf2 846.99115 Paf3 851.598755 Heatmap 365.541809 Total loss 2971.38013\n",
      "Saved checkpoint for step 4000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1780\n",
      "Epoch 297 Step 4010/5304 Paf1 708.701172 Paf2 644.066101 Paf3 640.560913 Heatmap 199.59816 Total loss 2192.92627\n",
      "Epoch 297 Step 4020/5304 Paf1 546.217773 Paf2 500.180145 Paf3 500.784668 Heatmap 183.389221 Total loss 1730.57178\n",
      "Epoch 297 Step 4030/5304 Paf1 505.973511 Paf2 474.1987 Paf3 478.171021 Heatmap 130.59816 Total loss 1588.94141\n",
      "Epoch 297 Step 4040/5304 Paf1 817.772766 Paf2 754.469604 Paf3 756.167969 Heatmap 230.098404 Total loss 2558.50879\n",
      "Epoch 297 Step 4050/5304 Paf1 642.546936 Paf2 605.336243 Paf3 602.416138 Heatmap 193.183426 Total loss 2043.48267\n",
      "Epoch 297 Step 4060/5304 Paf1 464.699036 Paf2 427.617126 Paf3 418.899872 Heatmap 143.356628 Total loss 1454.57263\n",
      "Epoch 297 Step 4070/5304 Paf1 891.474731 Paf2 852.404663 Paf3 837.1604 Heatmap 308.875458 Total loss 2889.91528\n",
      "Epoch 297 Step 4080/5304 Paf1 645.404785 Paf2 599.626953 Paf3 588.73468 Heatmap 201.334915 Total loss 2035.10132\n",
      "Epoch 297 Step 4090/5304 Paf1 568.981873 Paf2 546.17395 Paf3 543.181213 Heatmap 172.937317 Total loss 1831.27429\n",
      "Epoch 297 Step 4100/5304 Paf1 498.34906 Paf2 452.183685 Paf3 449.846313 Heatmap 146.423737 Total loss 1546.80273\n",
      "Epoch 297 Step 4110/5304 Paf1 694.744385 Paf2 664.263794 Paf3 660.428955 Heatmap 233.167267 Total loss 2252.60449\n",
      "Epoch 297 Step 4120/5304 Paf1 755.426514 Paf2 707.904236 Paf3 713.582642 Heatmap 267.265411 Total loss 2444.17871\n",
      "Epoch 297 Step 4130/5304 Paf1 899.475281 Paf2 856.762085 Paf3 855.080872 Heatmap 277.070618 Total loss 2888.38867\n",
      "Epoch 297 Step 4140/5304 Paf1 596.371033 Paf2 553.99707 Paf3 559.967773 Heatmap 170.588547 Total loss 1880.92456\n",
      "Epoch 297 Step 4150/5304 Paf1 551.039185 Paf2 508.195251 Paf3 510.904053 Heatmap 161.365891 Total loss 1731.50439\n",
      "Epoch 297 Step 4160/5304 Paf1 968.17926 Paf2 894.636353 Paf3 916.419373 Heatmap 254.882935 Total loss 3034.11792\n",
      "Epoch 297 Step 4170/5304 Paf1 767.319519 Paf2 725.707825 Paf3 716.484619 Heatmap 259.379333 Total loss 2468.89136\n",
      "Epoch 297 Step 4180/5304 Paf1 659.520142 Paf2 634.446899 Paf3 626.308044 Heatmap 236.952148 Total loss 2157.22729\n",
      "Epoch 297 Step 4190/5304 Paf1 826.323547 Paf2 788.992615 Paf3 794.653931 Heatmap 236.476273 Total loss 2646.44629\n",
      "Epoch 297 Step 4200/5304 Paf1 672.735046 Paf2 633.15509 Paf3 625.452637 Heatmap 197.259186 Total loss 2128.60205\n",
      "Epoch 297 Step 4210/5304 Paf1 714.607788 Paf2 669.42395 Paf3 678.434814 Heatmap 222.617813 Total loss 2285.08447\n",
      "Epoch 297 Step 4220/5304 Paf1 721.443 Paf2 654.294312 Paf3 667.595337 Heatmap 215.202881 Total loss 2258.53564\n",
      "Epoch 297 Step 4230/5304 Paf1 747.583679 Paf2 724.34668 Paf3 734.105225 Heatmap 228.193146 Total loss 2434.22876\n",
      "Epoch 297 Step 4240/5304 Paf1 791.121521 Paf2 728.721436 Paf3 744.14563 Heatmap 248.818756 Total loss 2512.80737\n",
      "Epoch 297 Step 4250/5304 Paf1 674.060059 Paf2 637.794312 Paf3 642.591 Heatmap 210.83844 Total loss 2165.28369\n",
      "Epoch 297 Step 4260/5304 Paf1 728.138855 Paf2 660.752441 Paf3 684.630371 Heatmap 219.32431 Total loss 2292.84619\n",
      "Epoch 297 Step 4270/5304 Paf1 836.674622 Paf2 775.672241 Paf3 803.115417 Heatmap 255.461777 Total loss 2670.92407\n",
      "Epoch 297 Step 4280/5304 Paf1 499.43222 Paf2 475.975555 Paf3 475.922974 Heatmap 147.652252 Total loss 1598.98291\n",
      "Epoch 297 Step 4290/5304 Paf1 672.295471 Paf2 648.636108 Paf3 631.690125 Heatmap 211.491226 Total loss 2164.11304\n",
      "Epoch 297 Step 4300/5304 Paf1 650.953613 Paf2 597.502197 Paf3 579.925781 Heatmap 212.447235 Total loss 2040.82886\n",
      "Epoch 297 Step 4310/5304 Paf1 512.973206 Paf2 500.432465 Paf3 495.981476 Heatmap 155.500763 Total loss 1664.88794\n",
      "Epoch 297 Step 4320/5304 Paf1 728.25592 Paf2 702.127563 Paf3 704.78833 Heatmap 232.669037 Total loss 2367.84082\n",
      "Epoch 297 Step 4330/5304 Paf1 537.552368 Paf2 508.752258 Paf3 510.126129 Heatmap 162.797134 Total loss 1719.22803\n",
      "Epoch 297 Step 4340/5304 Paf1 1000.79773 Paf2 914.216797 Paf3 916.556335 Heatmap 322.992889 Total loss 3154.56372\n",
      "Epoch 297 Step 4350/5304 Paf1 532.150391 Paf2 509.87326 Paf3 508.715759 Heatmap 136.841644 Total loss 1687.58105\n",
      "Epoch 297 Step 4360/5304 Paf1 915.995605 Paf2 859.48 Paf3 855.977417 Heatmap 286.402771 Total loss 2917.85571\n",
      "Epoch 297 Step 4370/5304 Paf1 507.62854 Paf2 470.40921 Paf3 472.028931 Heatmap 148.414948 Total loss 1598.48157\n",
      "Epoch 297 Step 4380/5304 Paf1 605.725342 Paf2 573.579468 Paf3 575.889465 Heatmap 198.323975 Total loss 1953.51831\n",
      "Epoch 297 Step 4390/5304 Paf1 537.431 Paf2 529.067749 Paf3 495.770844 Heatmap 182.182861 Total loss 1744.45251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297 Step 4400/5304 Paf1 528.065125 Paf2 500.708496 Paf3 491.311859 Heatmap 178.270859 Total loss 1698.35645\n",
      "Epoch 297 Step 4410/5304 Paf1 977.60791 Paf2 887.363647 Paf3 902.962036 Heatmap 383.903442 Total loss 3151.83691\n",
      "Epoch 297 Step 4420/5304 Paf1 888.718506 Paf2 860.177246 Paf3 860.186157 Heatmap 334.519836 Total loss 2943.60181\n",
      "Epoch 297 Step 4430/5304 Paf1 449.395874 Paf2 416.877228 Paf3 416.212494 Heatmap 136.824585 Total loss 1419.31018\n",
      "Epoch 297 Step 4440/5304 Paf1 803.325623 Paf2 758.65 Paf3 744.537354 Heatmap 250.04715 Total loss 2556.56\n",
      "Epoch 297 Step 4450/5304 Paf1 770.871094 Paf2 745.433838 Paf3 750.683594 Heatmap 242.455658 Total loss 2509.44434\n",
      "Epoch 297 Step 4460/5304 Paf1 730.075806 Paf2 672.649902 Paf3 676.511169 Heatmap 236.775269 Total loss 2316.01221\n",
      "Epoch 297 Step 4470/5304 Paf1 706.794861 Paf2 665.886902 Paf3 655.043579 Heatmap 207.621094 Total loss 2235.34644\n",
      "Epoch 297 Step 4480/5304 Paf1 512.333 Paf2 463.057587 Paf3 475.121368 Heatmap 125.787025 Total loss 1576.29907\n",
      "Epoch 297 Step 4490/5304 Paf1 546.715 Paf2 511.311981 Paf3 518.959717 Heatmap 153.274597 Total loss 1730.26123\n",
      "Epoch 297 Step 4500/5304 Paf1 615.695801 Paf2 602.415955 Paf3 580.4245 Heatmap 191.582153 Total loss 1990.11841\n",
      "Epoch 297 Step 4510/5304 Paf1 856.132568 Paf2 838.690491 Paf3 814.9729 Heatmap 307.531555 Total loss 2817.32739\n",
      "Epoch 297 Step 4520/5304 Paf1 465.764252 Paf2 463.900574 Paf3 440.281799 Heatmap 127.775986 Total loss 1497.72266\n",
      "Epoch 297 Step 4530/5304 Paf1 779.34259 Paf2 751.603394 Paf3 745.40332 Heatmap 269.476807 Total loss 2545.82617\n",
      "Epoch 297 Step 4540/5304 Paf1 825.142578 Paf2 773.777527 Paf3 769.795349 Heatmap 274.382904 Total loss 2643.09839\n",
      "Epoch 297 Step 4550/5304 Paf1 758.383789 Paf2 700.022949 Paf3 711.76178 Heatmap 244.361053 Total loss 2414.52954\n",
      "Epoch 297 Step 4560/5304 Paf1 769.325806 Paf2 705.151855 Paf3 704.360046 Heatmap 252.925629 Total loss 2431.76318\n",
      "Epoch 297 Step 4570/5304 Paf1 622.523743 Paf2 570.573608 Paf3 576.483215 Heatmap 178.662781 Total loss 1948.24341\n",
      "Epoch 297 Step 4580/5304 Paf1 754.819824 Paf2 707.733032 Paf3 699.987488 Heatmap 226.986847 Total loss 2389.52734\n",
      "Epoch 297 Step 4590/5304 Paf1 471.139404 Paf2 435.019958 Paf3 434.597931 Heatmap 124.431305 Total loss 1465.1886\n",
      "Epoch 297 Step 4600/5304 Paf1 615.836487 Paf2 561.863281 Paf3 557.997437 Heatmap 169.001968 Total loss 1904.6991\n",
      "Epoch 297 Step 4610/5304 Paf1 757.839355 Paf2 716.651123 Paf3 711.334167 Heatmap 212.395599 Total loss 2398.22021\n",
      "Epoch 297 Step 4620/5304 Paf1 546.32428 Paf2 512.235229 Paf3 514.256409 Heatmap 149.081039 Total loss 1721.89697\n",
      "Epoch 297 Step 4630/5304 Paf1 711.37677 Paf2 735.996 Paf3 708.324951 Heatmap 241.509094 Total loss 2397.20679\n",
      "Epoch 297 Step 4640/5304 Paf1 734.946899 Paf2 701.576538 Paf3 699.459 Heatmap 251.062134 Total loss 2387.04443\n",
      "Epoch 297 Step 4650/5304 Paf1 618.110901 Paf2 588.451355 Paf3 609.597534 Heatmap 199.230194 Total loss 2015.39\n",
      "Epoch 297 Step 4660/5304 Paf1 645.417969 Paf2 614.635071 Paf3 610.647217 Heatmap 246.899338 Total loss 2117.59961\n",
      "Epoch 297 Step 4670/5304 Paf1 563.222168 Paf2 530.698547 Paf3 532.02594 Heatmap 169.971329 Total loss 1795.91797\n",
      "Epoch 297 Step 4680/5304 Paf1 615.836792 Paf2 555.453064 Paf3 563.708496 Heatmap 191.547791 Total loss 1926.54614\n",
      "Epoch 297 Step 4690/5304 Paf1 677.670898 Paf2 632.699158 Paf3 631.629761 Heatmap 204.565338 Total loss 2146.56519\n",
      "Epoch 297 Step 4700/5304 Paf1 811.921631 Paf2 748.983398 Paf3 743.833862 Heatmap 264.516174 Total loss 2569.25513\n",
      "Epoch 297 Step 4710/5304 Paf1 475.753815 Paf2 442.908783 Paf3 453.491272 Heatmap 133.041641 Total loss 1505.19556\n",
      "Epoch 297 Step 4720/5304 Paf1 609.125671 Paf2 548.452454 Paf3 550.710815 Heatmap 174.911621 Total loss 1883.20056\n",
      "Epoch 297 Step 4730/5304 Paf1 478.421387 Paf2 442.326447 Paf3 452.321655 Heatmap 161.683167 Total loss 1534.75269\n",
      "Epoch 297 Step 4740/5304 Paf1 959.820557 Paf2 909.001404 Paf3 894.849854 Heatmap 296.857056 Total loss 3060.52881\n",
      "Epoch 297 Step 4750/5304 Paf1 769.423035 Paf2 725.8349 Paf3 724.966309 Heatmap 247.886139 Total loss 2468.11035\n",
      "Epoch 297 Step 4760/5304 Paf1 482.538025 Paf2 450.872253 Paf3 457.845612 Heatmap 153.903915 Total loss 1545.15979\n",
      "Epoch 297 Step 4770/5304 Paf1 692.832031 Paf2 686.928894 Paf3 699.946533 Heatmap 261.427856 Total loss 2341.13525\n",
      "Epoch 297 Step 4780/5304 Paf1 541.256653 Paf2 515.690674 Paf3 535.956665 Heatmap 153.561737 Total loss 1746.4657\n",
      "Epoch 297 Step 4790/5304 Paf1 544.465393 Paf2 511.062 Paf3 502.426941 Heatmap 159.938553 Total loss 1717.89282\n",
      "Epoch 297 Step 4800/5304 Paf1 798.279175 Paf2 729.188782 Paf3 745.278259 Heatmap 216.200897 Total loss 2488.94727\n",
      "Epoch 297 Step 4810/5304 Paf1 662.371887 Paf2 599.547668 Paf3 590.480225 Heatmap 172.968582 Total loss 2025.36841\n",
      "Epoch 297 Step 4820/5304 Paf1 616.388 Paf2 575.829956 Paf3 574.886841 Heatmap 190.842209 Total loss 1957.94702\n",
      "Epoch 297 Step 4830/5304 Paf1 571.164185 Paf2 521.548462 Paf3 529.115417 Heatmap 183.06897 Total loss 1804.89697\n",
      "Epoch 297 Step 4840/5304 Paf1 575.165466 Paf2 555.404541 Paf3 565.768677 Heatmap 179.236618 Total loss 1875.57544\n",
      "Epoch 297 Step 4850/5304 Paf1 673.597168 Paf2 621.485901 Paf3 630.159058 Heatmap 207.683456 Total loss 2132.92554\n",
      "Epoch 297 Step 4860/5304 Paf1 609.299683 Paf2 596.546631 Paf3 596.655 Heatmap 250.233368 Total loss 2052.73486\n",
      "Epoch 297 Step 4870/5304 Paf1 766.424683 Paf2 721.505737 Paf3 715.605 Heatmap 248.584122 Total loss 2452.11963\n",
      "Epoch 297 Step 4880/5304 Paf1 937.123779 Paf2 872.892517 Paf3 872.670776 Heatmap 320.101257 Total loss 3002.78833\n",
      "Epoch 297 Step 4890/5304 Paf1 597.33606 Paf2 580.727295 Paf3 568.602234 Heatmap 190.263535 Total loss 1936.9292\n",
      "Epoch 297 Step 4900/5304 Paf1 639.198303 Paf2 610.083923 Paf3 609.218384 Heatmap 189.5755 Total loss 2048.07617\n",
      "Epoch 297 Step 4910/5304 Paf1 392.865448 Paf2 355.647949 Paf3 357.298889 Heatmap 125.562744 Total loss 1231.375\n",
      "Epoch 297 Step 4920/5304 Paf1 609.084167 Paf2 571.704102 Paf3 554.684143 Heatmap 185.879837 Total loss 1921.35229\n",
      "Epoch 297 Step 4930/5304 Paf1 694.093872 Paf2 648.178406 Paf3 639.305054 Heatmap 228.861023 Total loss 2210.43823\n",
      "Epoch 297 Step 4940/5304 Paf1 808.559631 Paf2 766.982666 Paf3 768.070679 Heatmap 256.140045 Total loss 2599.75293\n",
      "Epoch 297 Step 4950/5304 Paf1 755.717651 Paf2 696.608826 Paf3 699.904114 Heatmap 231.453491 Total loss 2383.68408\n",
      "Epoch 297 Step 4960/5304 Paf1 806.364868 Paf2 770.204346 Paf3 773.574097 Heatmap 253.126282 Total loss 2603.26953\n",
      "Epoch 297 Step 4970/5304 Paf1 747.00415 Paf2 696.80072 Paf3 709.965088 Heatmap 237.804626 Total loss 2391.57471\n",
      "Epoch 297 Step 4980/5304 Paf1 648.044 Paf2 616.86377 Paf3 611.685669 Heatmap 207.205154 Total loss 2083.79858\n",
      "Epoch 297 Step 4990/5304 Paf1 547.122437 Paf2 513.587463 Paf3 513.033875 Heatmap 160.72818 Total loss 1734.47205\n",
      "Epoch 297 Step 5000/5304 Paf1 639.516174 Paf2 602.834045 Paf3 596.677246 Heatmap 203.000259 Total loss 2042.02771\n",
      "Saved checkpoint for step 5000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1781\n",
      "Epoch 297 Step 5010/5304 Paf1 851.863647 Paf2 761.304688 Paf3 763.26416 Heatmap 228.3815 Total loss 2604.81396\n",
      "Epoch 297 Step 5020/5304 Paf1 566.775269 Paf2 509.84552 Paf3 504.520813 Heatmap 165.044083 Total loss 1746.18579\n",
      "Epoch 297 Step 5030/5304 Paf1 1055.84314 Paf2 995.752686 Paf3 980.686157 Heatmap 417.726685 Total loss 3450.00854\n",
      "Epoch 297 Step 5040/5304 Paf1 633.066406 Paf2 586.488159 Paf3 584.058411 Heatmap 178.622467 Total loss 1982.23547\n",
      "Epoch 297 Step 5050/5304 Paf1 467.129028 Paf2 438.37 Paf3 449.090942 Heatmap 127.274673 Total loss 1481.86462\n",
      "Epoch 297 Step 5060/5304 Paf1 727.66 Paf2 698.349 Paf3 690.592407 Heatmap 234.655365 Total loss 2351.25684\n",
      "Epoch 297 Step 5070/5304 Paf1 1052.2019 Paf2 1048.04675 Paf3 1022.24255 Heatmap 334.899109 Total loss 3457.39014\n",
      "Epoch 297 Step 5080/5304 Paf1 980.553955 Paf2 931.20813 Paf3 935.685181 Heatmap 342.858459 Total loss 3190.30566\n",
      "Epoch 297 Step 5090/5304 Paf1 907.338867 Paf2 868.625427 Paf3 839.16748 Heatmap 257.047729 Total loss 2872.17969\n",
      "Epoch 297 Step 5100/5304 Paf1 750.658203 Paf2 681.083496 Paf3 697.213562 Heatmap 238.364868 Total loss 2367.32\n",
      "Epoch 297 Step 5110/5304 Paf1 646.469177 Paf2 619.508423 Paf3 618.634094 Heatmap 219.225906 Total loss 2103.8374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297 Step 5120/5304 Paf1 736.528503 Paf2 683.231445 Paf3 685.016 Heatmap 213.818848 Total loss 2318.59473\n",
      "Epoch 297 Step 5130/5304 Paf1 461.835144 Paf2 431.29187 Paf3 434.148102 Heatmap 116.977966 Total loss 1444.25317\n",
      "Epoch 297 Step 5140/5304 Paf1 728.516724 Paf2 679.553772 Paf3 691.577332 Heatmap 246.029114 Total loss 2345.677\n",
      "Epoch 297 Step 5150/5304 Paf1 535.879517 Paf2 520.681335 Paf3 517.605225 Heatmap 193.508118 Total loss 1767.67407\n",
      "Epoch 297 Step 5160/5304 Paf1 618.339111 Paf2 585.11792 Paf3 581.007751 Heatmap 155.796722 Total loss 1940.26147\n",
      "Epoch 297 Step 5170/5304 Paf1 741.351562 Paf2 694.025208 Paf3 700.540283 Heatmap 232.401306 Total loss 2368.31836\n",
      "Epoch 297 Step 5180/5304 Paf1 482.469055 Paf2 475.138611 Paf3 473.63382 Heatmap 141.868469 Total loss 1573.11\n",
      "Epoch 297 Step 5190/5304 Paf1 699.888184 Paf2 674.51123 Paf3 681.736572 Heatmap 234.346283 Total loss 2290.48242\n",
      "Epoch 297 Step 5200/5304 Paf1 909.390686 Paf2 842.546509 Paf3 849.58075 Heatmap 320.208466 Total loss 2921.72656\n",
      "Epoch 297 Step 5210/5304 Paf1 1062.18469 Paf2 1007.25128 Paf3 1016.75696 Heatmap 368.711823 Total loss 3454.90479\n",
      "Epoch 297 Step 5220/5304 Paf1 718.664795 Paf2 683.482788 Paf3 689.596558 Heatmap 259.562195 Total loss 2351.3064\n",
      "Epoch 297 Step 5230/5304 Paf1 415.354858 Paf2 369.746948 Paf3 373.110535 Heatmap 103.093887 Total loss 1261.30615\n",
      "Epoch 297 Step 5240/5304 Paf1 640.593933 Paf2 553.452393 Paf3 536.709839 Heatmap 191.202118 Total loss 1921.95837\n",
      "Epoch 297 Step 5250/5304 Paf1 711.493713 Paf2 653.458618 Paf3 660.173279 Heatmap 210.891296 Total loss 2236.01709\n",
      "Epoch 297 Step 5260/5304 Paf1 642.010315 Paf2 603.005737 Paf3 608.519 Heatmap 196.9 Total loss 2050.43506\n",
      "Epoch 297 Step 5270/5304 Paf1 812.59375 Paf2 791.623596 Paf3 770.90863 Heatmap 242.760483 Total loss 2617.88647\n",
      "Epoch 297 Step 5280/5304 Paf1 801.066956 Paf2 775.576782 Paf3 769.703 Heatmap 248.35994 Total loss 2594.70679\n",
      "Epoch 297 Step 5290/5304 Paf1 577.460327 Paf2 523.243469 Paf3 526.695801 Heatmap 132.90744 Total loss 1760.30713\n",
      "Epoch 297 Step 5300/5304 Paf1 681.949524 Paf2 621.917908 Paf3 600.436279 Heatmap 156.055267 Total loss 2060.35889\n",
      "Completed epoch 297. Saving weights...\n",
      "Epoch training time: 0:15:49.526831\n",
      "Calculating validation losses...\n",
      "Validation step 0 ...\n",
      "Validation losses for epoch: 297 : Loss paf 689.7125854492188, Loss heatmap 229.55816650390625, Total loss 2340.27197265625\n",
      "Start processing epoch 298\n",
      "Epoch 298 Step 10/5304 Paf1 756.38269 Paf2 702.844482 Paf3 731.357361 Heatmap 213.027939 Total loss 2403.61255\n",
      "Epoch 298 Step 20/5304 Paf1 469.805664 Paf2 427.775269 Paf3 433.11084 Heatmap 120.974037 Total loss 1451.66577\n",
      "Epoch 298 Step 30/5304 Paf1 793.61731 Paf2 752.364807 Paf3 748.087769 Heatmap 222.544525 Total loss 2516.6145\n",
      "Epoch 298 Step 40/5304 Paf1 577.956604 Paf2 568.378174 Paf3 550.749512 Heatmap 176.029907 Total loss 1873.11414\n",
      "Epoch 298 Step 50/5304 Paf1 543.410889 Paf2 516.043 Paf3 521.273254 Heatmap 177.30304 Total loss 1758.03015\n",
      "Epoch 298 Step 60/5304 Paf1 450.975891 Paf2 403.958496 Paf3 407.832611 Heatmap 118.253784 Total loss 1381.02075\n",
      "Epoch 298 Step 70/5304 Paf1 633.537231 Paf2 588.802307 Paf3 591.923218 Heatmap 170.163895 Total loss 1984.42676\n",
      "Epoch 298 Step 80/5304 Paf1 636.360229 Paf2 582.647034 Paf3 591.23938 Heatmap 172.943832 Total loss 1983.19055\n",
      "Epoch 298 Step 90/5304 Paf1 899.485779 Paf2 849.702881 Paf3 834.047546 Heatmap 321.457764 Total loss 2904.69409\n",
      "Epoch 298 Step 100/5304 Paf1 634.776306 Paf2 591.091553 Paf3 580.424866 Heatmap 179.782776 Total loss 1986.07556\n",
      "Epoch 298 Step 110/5304 Paf1 631.548096 Paf2 610.477051 Paf3 602.922607 Heatmap 196.307617 Total loss 2041.25537\n",
      "Epoch 298 Step 120/5304 Paf1 720.225708 Paf2 713.56427 Paf3 711.178589 Heatmap 287.20163 Total loss 2432.17041\n",
      "Epoch 298 Step 130/5304 Paf1 503.812347 Paf2 461.696808 Paf3 464.977478 Heatmap 150.972 Total loss 1581.45862\n",
      "Epoch 298 Step 140/5304 Paf1 874.463623 Paf2 830.483032 Paf3 837.294067 Heatmap 235.74881 Total loss 2777.9895\n",
      "Epoch 298 Step 150/5304 Paf1 617.95874 Paf2 607.749817 Paf3 593.423828 Heatmap 207.084534 Total loss 2026.2168\n",
      "Epoch 298 Step 160/5304 Paf1 529.284363 Paf2 499.656769 Paf3 503.517975 Heatmap 145.34198 Total loss 1677.80115\n",
      "Epoch 298 Step 170/5304 Paf1 805.027588 Paf2 748.179138 Paf3 740.266602 Heatmap 282.067932 Total loss 2575.54126\n",
      "Epoch 298 Step 180/5304 Paf1 668.818237 Paf2 616.647 Paf3 623.446472 Heatmap 212.876663 Total loss 2121.78833\n",
      "Epoch 298 Step 190/5304 Paf1 630.198853 Paf2 568.782959 Paf3 568.753845 Heatmap 208.192169 Total loss 1975.92786\n",
      "Epoch 298 Step 200/5304 Paf1 810.36969 Paf2 762.450928 Paf3 758.906128 Heatmap 231.985229 Total loss 2563.71191\n",
      "Epoch 298 Step 210/5304 Paf1 500.265411 Paf2 485.177521 Paf3 486.570953 Heatmap 154.062439 Total loss 1626.07642\n",
      "Epoch 298 Step 220/5304 Paf1 886.893494 Paf2 841.558228 Paf3 851.93689 Heatmap 228.586304 Total loss 2808.97485\n",
      "Epoch 298 Step 230/5304 Paf1 547.309631 Paf2 503.647766 Paf3 508.634644 Heatmap 176.958282 Total loss 1736.55029\n",
      "Epoch 298 Step 240/5304 Paf1 616.326294 Paf2 566.636475 Paf3 556.621826 Heatmap 197.877899 Total loss 1937.46252\n",
      "Epoch 298 Step 250/5304 Paf1 604.016968 Paf2 569.64447 Paf3 563.790283 Heatmap 176.001465 Total loss 1913.45312\n",
      "Epoch 298 Step 260/5304 Paf1 969.013428 Paf2 892.193115 Paf3 888.941284 Heatmap 290.714874 Total loss 3040.86279\n",
      "Epoch 298 Step 270/5304 Paf1 669.067505 Paf2 637.169434 Paf3 621.206 Heatmap 183.223557 Total loss 2110.6665\n",
      "Epoch 298 Step 280/5304 Paf1 767.908875 Paf2 727.499207 Paf3 731.540833 Heatmap 246.240631 Total loss 2473.18945\n",
      "Epoch 298 Step 290/5304 Paf1 564.154846 Paf2 534.934814 Paf3 523.277893 Heatmap 166.914642 Total loss 1789.2821\n",
      "Epoch 298 Step 300/5304 Paf1 534.343262 Paf2 489.38266 Paf3 488.190399 Heatmap 147.218323 Total loss 1659.13464\n",
      "Epoch 298 Step 310/5304 Paf1 700.74353 Paf2 657.026855 Paf3 652.097656 Heatmap 206.759857 Total loss 2216.62793\n",
      "Epoch 298 Step 320/5304 Paf1 685.982544 Paf2 636.78894 Paf3 651.357056 Heatmap 211.023315 Total loss 2185.15186\n",
      "Epoch 298 Step 330/5304 Paf1 556.176147 Paf2 509.798 Paf3 529.212952 Heatmap 185.461472 Total loss 1780.64856\n",
      "Epoch 298 Step 340/5304 Paf1 898.397705 Paf2 858.673767 Paf3 853.227966 Heatmap 284.228149 Total loss 2894.52759\n",
      "Epoch 298 Step 350/5304 Paf1 430.075653 Paf2 370.928101 Paf3 362.66272 Heatmap 89.4588699 Total loss 1253.12537\n",
      "Epoch 298 Step 360/5304 Paf1 715.32312 Paf2 677.769531 Paf3 684.090271 Heatmap 242.746826 Total loss 2319.92969\n",
      "Epoch 298 Step 370/5304 Paf1 769.263672 Paf2 714.86084 Paf3 723.311 Heatmap 233.88266 Total loss 2441.31812\n",
      "Epoch 298 Step 380/5304 Paf1 583.753174 Paf2 537.44104 Paf3 562.232178 Heatmap 161.18573 Total loss 1844.61206\n",
      "Epoch 298 Step 390/5304 Paf1 539.933167 Paf2 525.43158 Paf3 509.267487 Heatmap 200.586792 Total loss 1775.21899\n",
      "Epoch 298 Step 400/5304 Paf1 693.00885 Paf2 656.089355 Paf3 660.238403 Heatmap 242.650558 Total loss 2251.98706\n",
      "Epoch 298 Step 410/5304 Paf1 724.009521 Paf2 659.911194 Paf3 679.361328 Heatmap 226.894241 Total loss 2290.17627\n",
      "Epoch 298 Step 420/5304 Paf1 608.185852 Paf2 567.807922 Paf3 562.95636 Heatmap 175.310394 Total loss 1914.2605\n",
      "Epoch 298 Step 430/5304 Paf1 828.198792 Paf2 791.794556 Paf3 803.564087 Heatmap 246.46077 Total loss 2670.01831\n",
      "Epoch 298 Step 440/5304 Paf1 882.64563 Paf2 858.650391 Paf3 858.502136 Heatmap 284.618591 Total loss 2884.41675\n",
      "Epoch 298 Step 450/5304 Paf1 717.130554 Paf2 683.903564 Paf3 680.334106 Heatmap 204.079346 Total loss 2285.44775\n",
      "Epoch 298 Step 460/5304 Paf1 508.148651 Paf2 487.362762 Paf3 486.520874 Heatmap 142.472885 Total loss 1624.50513\n",
      "Epoch 298 Step 470/5304 Paf1 705.646484 Paf2 651.448242 Paf3 650.444641 Heatmap 237.236206 Total loss 2244.77563\n",
      "Epoch 298 Step 480/5304 Paf1 582.550415 Paf2 532.22876 Paf3 529.173218 Heatmap 143.711853 Total loss 1787.66431\n",
      "Epoch 298 Step 490/5304 Paf1 715.211914 Paf2 650.555481 Paf3 654.744324 Heatmap 197.756516 Total loss 2218.26807\n",
      "Epoch 298 Step 500/5304 Paf1 704.715576 Paf2 653.081238 Paf3 669.667236 Heatmap 246.827713 Total loss 2274.29175\n",
      "Epoch 298 Step 510/5304 Paf1 584.747803 Paf2 541.72937 Paf3 553.823303 Heatmap 172.277405 Total loss 1852.57788\n",
      "Epoch 298 Step 520/5304 Paf1 565.141724 Paf2 513.394775 Paf3 512.700928 Heatmap 175.206116 Total loss 1766.4436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 298 Step 530/5304 Paf1 809.474915 Paf2 769.725037 Paf3 739.512146 Heatmap 236.980637 Total loss 2555.69287\n",
      "Epoch 298 Step 540/5304 Paf1 824.296692 Paf2 774.822449 Paf3 772.71 Heatmap 268.047943 Total loss 2639.87695\n",
      "Epoch 298 Step 550/5304 Paf1 656.883606 Paf2 594.586304 Paf3 613.162598 Heatmap 176.509521 Total loss 2041.14209\n",
      "Epoch 298 Step 560/5304 Paf1 1190.46582 Paf2 1120.76099 Paf3 1119.05518 Heatmap 465.222931 Total loss 3895.50488\n",
      "Epoch 298 Step 570/5304 Paf1 958.939209 Paf2 948.805359 Paf3 901.887939 Heatmap 329.193359 Total loss 3138.82593\n",
      "Epoch 298 Step 580/5304 Paf1 927.356689 Paf2 896.465759 Paf3 899.257935 Heatmap 360.116455 Total loss 3083.19678\n",
      "Epoch 298 Step 590/5304 Paf1 730.340759 Paf2 693.724915 Paf3 688.544373 Heatmap 188.397369 Total loss 2301.00732\n",
      "Epoch 298 Step 600/5304 Paf1 523.553955 Paf2 493.50766 Paf3 486.644104 Heatmap 161.989822 Total loss 1665.69556\n",
      "Epoch 298 Step 610/5304 Paf1 512.600098 Paf2 476.090637 Paf3 473.006897 Heatmap 149.280716 Total loss 1610.97827\n",
      "Epoch 298 Step 620/5304 Paf1 606.23584 Paf2 578.774902 Paf3 585.442627 Heatmap 182.393845 Total loss 1952.84717\n",
      "Epoch 298 Step 630/5304 Paf1 918.861328 Paf2 858.104553 Paf3 862.409119 Heatmap 251.617065 Total loss 2890.99194\n",
      "Epoch 298 Step 640/5304 Paf1 536.652283 Paf2 510.275238 Paf3 493.699677 Heatmap 170.167557 Total loss 1710.79468\n",
      "Epoch 298 Step 650/5304 Paf1 843.951782 Paf2 785.77124 Paf3 788.694885 Heatmap 273.174 Total loss 2691.5918\n",
      "Epoch 298 Step 660/5304 Paf1 745.082397 Paf2 691.748108 Paf3 685.83374 Heatmap 210.113312 Total loss 2332.77759\n",
      "Epoch 298 Step 670/5304 Paf1 796.406067 Paf2 732.419617 Paf3 748.609741 Heatmap 280.700562 Total loss 2558.13599\n",
      "Epoch 298 Step 680/5304 Paf1 858.821228 Paf2 816.294678 Paf3 785.34906 Heatmap 286.322693 Total loss 2746.7876\n",
      "Epoch 298 Step 690/5304 Paf1 486.62204 Paf2 461.003479 Paf3 443.659607 Heatmap 161.800461 Total loss 1553.08557\n",
      "Epoch 298 Step 700/5304 Paf1 694.638733 Paf2 623.699646 Paf3 627.504395 Heatmap 209.572876 Total loss 2155.41553\n",
      "Epoch 298 Step 710/5304 Paf1 843.628906 Paf2 782.909912 Paf3 768.638184 Heatmap 234.430374 Total loss 2629.60742\n",
      "Epoch 298 Step 720/5304 Paf1 796.07666 Paf2 744.73053 Paf3 728.419 Heatmap 218.263275 Total loss 2487.48926\n",
      "Epoch 298 Step 730/5304 Paf1 632.09729 Paf2 604.877563 Paf3 592.963867 Heatmap 185.835358 Total loss 2015.77405\n",
      "Epoch 298 Step 740/5304 Paf1 503.53537 Paf2 473.465271 Paf3 465.836334 Heatmap 163.618225 Total loss 1606.4552\n",
      "Epoch 298 Step 750/5304 Paf1 843.617493 Paf2 787.834961 Paf3 778.354248 Heatmap 269.920837 Total loss 2679.72754\n",
      "Epoch 298 Step 760/5304 Paf1 497.671906 Paf2 460.765747 Paf3 453.618591 Heatmap 127.065384 Total loss 1539.12158\n",
      "Epoch 298 Step 770/5304 Paf1 664.045288 Paf2 608.969849 Paf3 595.606323 Heatmap 205.980408 Total loss 2074.60181\n",
      "Epoch 298 Step 780/5304 Paf1 621.820679 Paf2 572.917664 Paf3 589.803223 Heatmap 258.983612 Total loss 2043.52515\n",
      "Epoch 298 Step 790/5304 Paf1 568.464844 Paf2 546.929871 Paf3 553.969177 Heatmap 189.540588 Total loss 1858.90454\n",
      "Epoch 298 Step 800/5304 Paf1 612.674744 Paf2 577.179443 Paf3 580.46228 Heatmap 214.09967 Total loss 1984.41626\n",
      "Epoch 298 Step 810/5304 Paf1 702.724915 Paf2 630.31311 Paf3 645.953735 Heatmap 230.126251 Total loss 2209.11816\n",
      "Epoch 298 Step 820/5304 Paf1 914.752258 Paf2 861.877258 Paf3 843.790283 Heatmap 243.281235 Total loss 2863.70117\n",
      "Epoch 298 Step 830/5304 Paf1 821.438293 Paf2 772.87085 Paf3 777.58136 Heatmap 247.142899 Total loss 2619.0332\n",
      "Epoch 298 Step 840/5304 Paf1 513.022888 Paf2 461.324219 Paf3 459.237823 Heatmap 129.248367 Total loss 1562.83325\n",
      "Epoch 298 Step 850/5304 Paf1 519.418701 Paf2 477.967102 Paf3 490.82605 Heatmap 163.454376 Total loss 1651.66626\n",
      "Epoch 298 Step 860/5304 Paf1 970.362915 Paf2 955.20166 Paf3 938.020325 Heatmap 335.609528 Total loss 3199.19434\n",
      "Epoch 298 Step 870/5304 Paf1 1154.91333 Paf2 1072.68384 Paf3 1073.00562 Heatmap 361.111328 Total loss 3661.71411\n",
      "Epoch 298 Step 880/5304 Paf1 826.882874 Paf2 771.684692 Paf3 794.869202 Heatmap 235.26326 Total loss 2628.7002\n",
      "Epoch 298 Step 890/5304 Paf1 620.290894 Paf2 577.417603 Paf3 568.272949 Heatmap 187.996643 Total loss 1953.97803\n",
      "Epoch 298 Step 900/5304 Paf1 518.472656 Paf2 487.076111 Paf3 484.558411 Heatmap 150.033096 Total loss 1640.14026\n",
      "Epoch 298 Step 910/5304 Paf1 473.542786 Paf2 452.784607 Paf3 439.858154 Heatmap 144.863312 Total loss 1511.04883\n",
      "Epoch 298 Step 920/5304 Paf1 642.316895 Paf2 615.338745 Paf3 599.62207 Heatmap 226.145172 Total loss 2083.42285\n",
      "Epoch 298 Step 930/5304 Paf1 400.356934 Paf2 362.849762 Paf3 360.932861 Heatmap 102.583847 Total loss 1226.72339\n",
      "Epoch 298 Step 940/5304 Paf1 389.900452 Paf2 350.259979 Paf3 352.582611 Heatmap 119.638489 Total loss 1212.38147\n",
      "Epoch 298 Step 950/5304 Paf1 601.356445 Paf2 563.340515 Paf3 562.898865 Heatmap 168.95459 Total loss 1896.55054\n",
      "Epoch 298 Step 960/5304 Paf1 789.245483 Paf2 755.122192 Paf3 739.198181 Heatmap 222.413879 Total loss 2505.97974\n",
      "Epoch 298 Step 970/5304 Paf1 692.982666 Paf2 632.593201 Paf3 637.734802 Heatmap 214.150696 Total loss 2177.46143\n",
      "Epoch 298 Step 980/5304 Paf1 740.031799 Paf2 708.723938 Paf3 698.428223 Heatmap 230.537598 Total loss 2377.72168\n",
      "Epoch 298 Step 990/5304 Paf1 624.666199 Paf2 585.160095 Paf3 571.421143 Heatmap 178.937408 Total loss 1960.18481\n",
      "Epoch 298 Step 1000/5304 Paf1 598.789246 Paf2 548.82605 Paf3 536.747437 Heatmap 159.347534 Total loss 1843.71021\n",
      "Saved checkpoint for step 1000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1783\n",
      "Epoch 298 Step 1010/5304 Paf1 569.876831 Paf2 557.272217 Paf3 559.018738 Heatmap 216.451431 Total loss 1902.61914\n",
      "Epoch 298 Step 1020/5304 Paf1 669.737915 Paf2 622.6521 Paf3 622.31488 Heatmap 185.843475 Total loss 2100.54834\n",
      "Epoch 298 Step 1030/5304 Paf1 651.387512 Paf2 605.589478 Paf3 609.935303 Heatmap 217.220718 Total loss 2084.13306\n",
      "Epoch 298 Step 1040/5304 Paf1 578.345581 Paf2 520.764893 Paf3 521.77887 Heatmap 172.044342 Total loss 1792.93372\n",
      "Epoch 298 Step 1050/5304 Paf1 576.000488 Paf2 553.368958 Paf3 537.146362 Heatmap 155.848953 Total loss 1822.36475\n",
      "Epoch 298 Step 1060/5304 Paf1 534.374329 Paf2 492.37439 Paf3 484.581665 Heatmap 196.30835 Total loss 1707.63879\n",
      "Epoch 298 Step 1070/5304 Paf1 617.648804 Paf2 560.793152 Paf3 573.575806 Heatmap 167.074127 Total loss 1919.0918\n",
      "Epoch 298 Step 1080/5304 Paf1 595.87677 Paf2 564.489 Paf3 547.886841 Heatmap 185.670135 Total loss 1893.92273\n",
      "Epoch 298 Step 1090/5304 Paf1 654.164429 Paf2 609.525208 Paf3 608.420044 Heatmap 189.632324 Total loss 2061.74219\n",
      "Epoch 298 Step 1100/5304 Paf1 608.326721 Paf2 560.539124 Paf3 557.21051 Heatmap 145.779861 Total loss 1871.8562\n",
      "Epoch 298 Step 1110/5304 Paf1 555.164734 Paf2 515.027344 Paf3 507.947083 Heatmap 169.059814 Total loss 1747.19897\n",
      "Epoch 298 Step 1120/5304 Paf1 979.810242 Paf2 946.41 Paf3 938.755798 Heatmap 316.945129 Total loss 3181.92114\n",
      "Epoch 298 Step 1130/5304 Paf1 463.674255 Paf2 441.417084 Paf3 430.189972 Heatmap 137.642288 Total loss 1472.92358\n",
      "Epoch 298 Step 1140/5304 Paf1 522.802429 Paf2 492.410095 Paf3 488.322815 Heatmap 162.108765 Total loss 1665.64404\n",
      "Epoch 298 Step 1150/5304 Paf1 650.507568 Paf2 599.191772 Paf3 592.48053 Heatmap 191.780914 Total loss 2033.96082\n",
      "Epoch 298 Step 1160/5304 Paf1 674.1026 Paf2 644.699 Paf3 629.192627 Heatmap 206.540894 Total loss 2154.53516\n",
      "Epoch 298 Step 1170/5304 Paf1 492.53833 Paf2 461.876953 Paf3 458.731201 Heatmap 154.851425 Total loss 1567.99792\n",
      "Epoch 298 Step 1180/5304 Paf1 672.24646 Paf2 648.283508 Paf3 649.180298 Heatmap 223.398987 Total loss 2193.10938\n",
      "Epoch 298 Step 1190/5304 Paf1 674.458 Paf2 615.974 Paf3 622.833191 Heatmap 230.680756 Total loss 2143.9458\n",
      "Epoch 298 Step 1200/5304 Paf1 1051.43665 Paf2 994.448364 Paf3 992.023315 Heatmap 341.254028 Total loss 3379.16235\n",
      "Epoch 298 Step 1210/5304 Paf1 661.365662 Paf2 620.412292 Paf3 607.238647 Heatmap 176.842163 Total loss 2065.85889\n",
      "Epoch 298 Step 1220/5304 Paf1 718.16687 Paf2 663.519409 Paf3 679.83429 Heatmap 223.58873 Total loss 2285.10938\n",
      "Epoch 298 Step 1230/5304 Paf1 767.412354 Paf2 733.512573 Paf3 726.583 Heatmap 244.329041 Total loss 2471.83691\n",
      "Epoch 298 Step 1240/5304 Paf1 842.879822 Paf2 787.884766 Paf3 781.594543 Heatmap 242.840515 Total loss 2655.19971\n",
      "Epoch 298 Step 1250/5304 Paf1 678.708496 Paf2 627.305359 Paf3 631.283386 Heatmap 204.415222 Total loss 2141.7124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 298 Step 1260/5304 Paf1 799.511719 Paf2 722.461487 Paf3 738.26062 Heatmap 225.067505 Total loss 2485.30127\n",
      "Epoch 298 Step 1270/5304 Paf1 502.541656 Paf2 464.547424 Paf3 463.501801 Heatmap 158.160889 Total loss 1588.75183\n",
      "Epoch 298 Step 1280/5304 Paf1 595.397 Paf2 555.509033 Paf3 565.639465 Heatmap 168.517365 Total loss 1885.06287\n",
      "Epoch 298 Step 1290/5304 Paf1 945.84491 Paf2 867.182434 Paf3 887.437866 Heatmap 306.882385 Total loss 3007.34766\n",
      "Epoch 298 Step 1300/5304 Paf1 602.20813 Paf2 571.27417 Paf3 577.596252 Heatmap 220.101807 Total loss 1971.18042\n",
      "Epoch 298 Step 1310/5304 Paf1 777.03894 Paf2 726.234 Paf3 728.648193 Heatmap 255.868256 Total loss 2487.78955\n",
      "Epoch 298 Step 1320/5304 Paf1 574.209106 Paf2 553.092712 Paf3 542.741699 Heatmap 182.572021 Total loss 1852.61548\n",
      "Epoch 298 Step 1330/5304 Paf1 755.726685 Paf2 700.590942 Paf3 710.656738 Heatmap 245.959183 Total loss 2412.93359\n",
      "Epoch 298 Step 1340/5304 Paf1 888.864624 Paf2 846.844 Paf3 861.539307 Heatmap 252.544586 Total loss 2849.79248\n",
      "Epoch 298 Step 1350/5304 Paf1 827.218567 Paf2 763.906189 Paf3 772.487854 Heatmap 315.061157 Total loss 2678.67383\n",
      "Epoch 298 Step 1360/5304 Paf1 667.347717 Paf2 622.829468 Paf3 606.584167 Heatmap 216.702942 Total loss 2113.46436\n",
      "Epoch 298 Step 1370/5304 Paf1 660.682312 Paf2 609.517273 Paf3 616.979919 Heatmap 215.841858 Total loss 2103.02148\n",
      "Epoch 298 Step 1380/5304 Paf1 633.126953 Paf2 620.785645 Paf3 623.889771 Heatmap 242.791962 Total loss 2120.59424\n",
      "Epoch 298 Step 1390/5304 Paf1 607.165405 Paf2 569.300598 Paf3 585.029846 Heatmap 158.346863 Total loss 1919.84277\n",
      "Epoch 298 Step 1400/5304 Paf1 619.452515 Paf2 568.684387 Paf3 584.283936 Heatmap 188.802185 Total loss 1961.22314\n",
      "Epoch 298 Step 1410/5304 Paf1 895.357056 Paf2 857.250122 Paf3 846.827515 Heatmap 286.946869 Total loss 2886.38159\n",
      "Epoch 298 Step 1420/5304 Paf1 635.917847 Paf2 598.668091 Paf3 587.726379 Heatmap 164.652283 Total loss 1986.9646\n",
      "Epoch 298 Step 1430/5304 Paf1 587.850037 Paf2 528.579834 Paf3 553.056458 Heatmap 177.772095 Total loss 1847.25854\n",
      "Epoch 298 Step 1440/5304 Paf1 1074.70215 Paf2 981.95459 Paf3 996.702393 Heatmap 359.65979 Total loss 3413.01904\n",
      "Epoch 298 Step 1450/5304 Paf1 617.849304 Paf2 575.049805 Paf3 580.037842 Heatmap 192.361572 Total loss 1965.29858\n",
      "Epoch 298 Step 1460/5304 Paf1 1057.32153 Paf2 993.239197 Paf3 1009.86237 Heatmap 358.177063 Total loss 3418.6\n",
      "Epoch 298 Step 1470/5304 Paf1 816.137146 Paf2 784.232788 Paf3 789.889404 Heatmap 258.028076 Total loss 2648.28735\n",
      "Epoch 298 Step 1480/5304 Paf1 586.464417 Paf2 540.07782 Paf3 535.75885 Heatmap 158.511749 Total loss 1820.81287\n",
      "Epoch 298 Step 1490/5304 Paf1 481.986023 Paf2 457.700256 Paf3 461.682587 Heatmap 150.585602 Total loss 1551.95447\n",
      "Epoch 298 Step 1500/5304 Paf1 740.611206 Paf2 695.738892 Paf3 705.858154 Heatmap 214.255524 Total loss 2356.46387\n",
      "Epoch 298 Step 1510/5304 Paf1 781.857056 Paf2 722.025879 Paf3 723.043274 Heatmap 236.13591 Total loss 2463.06201\n",
      "Epoch 298 Step 1520/5304 Paf1 589.989502 Paf2 539.605164 Paf3 556.585632 Heatmap 203.130707 Total loss 1889.31104\n",
      "Epoch 298 Step 1530/5304 Paf1 774.539673 Paf2 689.679688 Paf3 709.07666 Heatmap 251.717133 Total loss 2425.01318\n",
      "Epoch 298 Step 1540/5304 Paf1 738.297485 Paf2 684.251 Paf3 702.384827 Heatmap 224.312561 Total loss 2349.24585\n",
      "Epoch 298 Step 1550/5304 Paf1 714.51062 Paf2 664.477234 Paf3 661.383545 Heatmap 230.552246 Total loss 2270.92358\n",
      "Epoch 298 Step 1560/5304 Paf1 668.549194 Paf2 620.350708 Paf3 623.686829 Heatmap 170.047455 Total loss 2082.63428\n",
      "Epoch 298 Step 1570/5304 Paf1 950.493408 Paf2 919.189087 Paf3 911.410339 Heatmap 322.149384 Total loss 3103.24219\n",
      "Epoch 298 Step 1580/5304 Paf1 894.230103 Paf2 844.302246 Paf3 848.746643 Heatmap 306.5495 Total loss 2893.82861\n",
      "Epoch 298 Step 1590/5304 Paf1 526.386902 Paf2 502.310669 Paf3 507.367371 Heatmap 205.413361 Total loss 1741.47827\n",
      "Epoch 298 Step 1600/5304 Paf1 632.396057 Paf2 584.952148 Paf3 591.940857 Heatmap 193.362823 Total loss 2002.65186\n",
      "Epoch 298 Step 1610/5304 Paf1 410.175293 Paf2 377.214386 Paf3 389.69635 Heatmap 111.260208 Total loss 1288.34619\n",
      "Epoch 298 Step 1620/5304 Paf1 504.976807 Paf2 471.668732 Paf3 465.795776 Heatmap 151.660736 Total loss 1594.10205\n",
      "Epoch 298 Step 1630/5304 Paf1 670.456 Paf2 611.813538 Paf3 615.68158 Heatmap 196.307312 Total loss 2094.2583\n",
      "Epoch 298 Step 1640/5304 Paf1 679.814636 Paf2 650.773621 Paf3 650.112305 Heatmap 207.420624 Total loss 2188.12109\n",
      "Epoch 298 Step 1650/5304 Paf1 828.018616 Paf2 767.769043 Paf3 761.344238 Heatmap 274.363586 Total loss 2631.49536\n",
      "Epoch 298 Step 1660/5304 Paf1 1042.38525 Paf2 991.259338 Paf3 987.649658 Heatmap 317.332886 Total loss 3338.62695\n",
      "Epoch 298 Step 1670/5304 Paf1 927.896729 Paf2 870.658325 Paf3 878.811035 Heatmap 326.727295 Total loss 3004.09326\n",
      "Epoch 298 Step 1680/5304 Paf1 612.744141 Paf2 571.318237 Paf3 568.845215 Heatmap 179.96698 Total loss 1932.87451\n",
      "Epoch 298 Step 1690/5304 Paf1 720.571838 Paf2 686.914368 Paf3 688.622498 Heatmap 224.378845 Total loss 2320.48755\n",
      "Epoch 298 Step 1700/5304 Paf1 604.43396 Paf2 563.945374 Paf3 565.159546 Heatmap 180.939056 Total loss 1914.47803\n",
      "Epoch 298 Step 1710/5304 Paf1 781.72821 Paf2 721.234924 Paf3 718.630432 Heatmap 247.234 Total loss 2468.82764\n",
      "Epoch 298 Step 1720/5304 Paf1 523.785828 Paf2 474.482727 Paf3 484.838379 Heatmap 161.490387 Total loss 1644.59729\n",
      "Epoch 298 Step 1730/5304 Paf1 733.95874 Paf2 682.475098 Paf3 700.105347 Heatmap 255.623383 Total loss 2372.1626\n",
      "Epoch 298 Step 1740/5304 Paf1 803.705933 Paf2 758.987854 Paf3 779.559 Heatmap 248.241394 Total loss 2590.49414\n",
      "Epoch 298 Step 1750/5304 Paf1 763.2229 Paf2 718.377319 Paf3 721.085449 Heatmap 254.333359 Total loss 2457.01904\n",
      "Epoch 298 Step 1760/5304 Paf1 620.586304 Paf2 562.545227 Paf3 565.768 Heatmap 183.507629 Total loss 1932.40723\n",
      "Epoch 298 Step 1770/5304 Paf1 609.212585 Paf2 566.053955 Paf3 548.212036 Heatmap 159.056915 Total loss 1882.53552\n",
      "Epoch 298 Step 1780/5304 Paf1 835.612244 Paf2 770.324585 Paf3 776.362305 Heatmap 279.561951 Total loss 2661.86108\n",
      "Epoch 298 Step 1790/5304 Paf1 681.880188 Paf2 608.62 Paf3 626.443726 Heatmap 210.125549 Total loss 2127.06958\n",
      "Epoch 298 Step 1800/5304 Paf1 817.495056 Paf2 778.242432 Paf3 777.684631 Heatmap 273.535797 Total loss 2646.95801\n",
      "Epoch 298 Step 1810/5304 Paf1 375.849304 Paf2 320.728241 Paf3 335.857819 Heatmap 103.482361 Total loss 1135.91772\n",
      "Epoch 298 Step 1820/5304 Paf1 557.911 Paf2 515.02594 Paf3 526.499878 Heatmap 165.984497 Total loss 1765.42139\n",
      "Epoch 298 Step 1830/5304 Paf1 627.851 Paf2 589.881409 Paf3 578.574 Heatmap 194.609665 Total loss 1990.91602\n",
      "Epoch 298 Step 1840/5304 Paf1 433.623352 Paf2 390.252563 Paf3 390.443237 Heatmap 102.611488 Total loss 1316.93066\n",
      "Epoch 298 Step 1850/5304 Paf1 588.82666 Paf2 547.256653 Paf3 551.012207 Heatmap 172.626556 Total loss 1859.72205\n",
      "Epoch 298 Step 1860/5304 Paf1 703.718079 Paf2 673.754761 Paf3 670.028 Heatmap 219.767059 Total loss 2267.26807\n",
      "Epoch 298 Step 1870/5304 Paf1 812.887939 Paf2 807.211 Paf3 794.160828 Heatmap 306.447083 Total loss 2720.70679\n",
      "Epoch 298 Step 1880/5304 Paf1 593.506714 Paf2 557.844482 Paf3 544.519287 Heatmap 199.958649 Total loss 1895.8291\n",
      "Epoch 298 Step 1890/5304 Paf1 805.606201 Paf2 767.528687 Paf3 767.310181 Heatmap 269.12085 Total loss 2609.56592\n",
      "Epoch 298 Step 1900/5304 Paf1 494.004242 Paf2 448.97113 Paf3 465.147461 Heatmap 146.255859 Total loss 1554.37866\n",
      "Epoch 298 Step 1910/5304 Paf1 942.17981 Paf2 903.959351 Paf3 901.115234 Heatmap 315.191833 Total loss 3062.44629\n",
      "Epoch 298 Step 1920/5304 Paf1 691.089294 Paf2 662.703796 Paf3 661.445251 Heatmap 237.268478 Total loss 2252.50684\n",
      "Epoch 298 Step 1930/5304 Paf1 682.118835 Paf2 653.336304 Paf3 646.06665 Heatmap 214.425323 Total loss 2195.94702\n",
      "Epoch 298 Step 1940/5304 Paf1 704.198853 Paf2 683.182617 Paf3 684.32312 Heatmap 280.642853 Total loss 2352.34741\n",
      "Epoch 298 Step 1950/5304 Paf1 724.100342 Paf2 668.063354 Paf3 675.203796 Heatmap 222.22229 Total loss 2289.58984\n",
      "Epoch 298 Step 1960/5304 Paf1 782.069214 Paf2 725.829224 Paf3 735.655273 Heatmap 253.204071 Total loss 2496.75781\n",
      "Epoch 298 Step 1970/5304 Paf1 508.290497 Paf2 474.733704 Paf3 461.598328 Heatmap 154.167801 Total loss 1598.79028\n",
      "Epoch 298 Step 1980/5304 Paf1 361.818298 Paf2 337.020599 Paf3 336.79425 Heatmap 104.579018 Total loss 1140.21216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 298 Step 1990/5304 Paf1 508.948212 Paf2 496.529114 Paf3 484.856628 Heatmap 151.421951 Total loss 1641.75586\n",
      "Epoch 298 Step 2000/5304 Paf1 541.282471 Paf2 505.014984 Paf3 503.563141 Heatmap 131.116898 Total loss 1680.97754\n",
      "Saved checkpoint for step 2000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1784\n",
      "Epoch 298 Step 2010/5304 Paf1 864.900757 Paf2 796.76709 Paf3 788.438049 Heatmap 266.705505 Total loss 2716.81152\n",
      "Epoch 298 Step 2020/5304 Paf1 915.246399 Paf2 877.63324 Paf3 876.243896 Heatmap 301.938965 Total loss 2971.0625\n",
      "Epoch 298 Step 2030/5304 Paf1 643.446106 Paf2 609.80011 Paf3 606.360413 Heatmap 168.008148 Total loss 2027.61475\n",
      "Epoch 298 Step 2040/5304 Paf1 758.600952 Paf2 714.27 Paf3 700.961548 Heatmap 190.824219 Total loss 2364.65674\n",
      "Epoch 298 Step 2050/5304 Paf1 503.631531 Paf2 464.243591 Paf3 451.356049 Heatmap 137.235199 Total loss 1556.46631\n",
      "Epoch 298 Step 2060/5304 Paf1 705.549744 Paf2 642.382263 Paf3 640.571472 Heatmap 209.840668 Total loss 2198.34424\n",
      "Epoch 298 Step 2070/5304 Paf1 560.932739 Paf2 515.210938 Paf3 511.259796 Heatmap 169.010773 Total loss 1756.41431\n",
      "Epoch 298 Step 2080/5304 Paf1 523.586182 Paf2 519.109314 Paf3 477.519501 Heatmap 147.027374 Total loss 1667.24243\n",
      "Epoch 298 Step 2090/5304 Paf1 403.837067 Paf2 392.121979 Paf3 379.86322 Heatmap 119.801277 Total loss 1295.62354\n",
      "Epoch 298 Step 2100/5304 Paf1 797.418091 Paf2 738.491394 Paf3 735.67749 Heatmap 254.926575 Total loss 2526.51343\n",
      "Epoch 298 Step 2110/5304 Paf1 486.930023 Paf2 458.464508 Paf3 455.934265 Heatmap 154.388977 Total loss 1555.71777\n",
      "Epoch 298 Step 2120/5304 Paf1 732.684387 Paf2 694.90625 Paf3 683.19928 Heatmap 207.950256 Total loss 2318.74023\n",
      "Epoch 298 Step 2130/5304 Paf1 469.442963 Paf2 421.808472 Paf3 433.72168 Heatmap 146.987289 Total loss 1471.96045\n",
      "Epoch 298 Step 2140/5304 Paf1 823.005249 Paf2 802.409668 Paf3 761.636 Heatmap 226.656769 Total loss 2613.70752\n",
      "Epoch 298 Step 2150/5304 Paf1 634.941 Paf2 612.829956 Paf3 597.050293 Heatmap 187.503265 Total loss 2032.32458\n",
      "Epoch 298 Step 2160/5304 Paf1 939.972656 Paf2 897.420166 Paf3 877.41925 Heatmap 256.013184 Total loss 2970.8252\n",
      "Epoch 298 Step 2170/5304 Paf1 749.69989 Paf2 717.646484 Paf3 709.958557 Heatmap 233.486771 Total loss 2410.79175\n",
      "Epoch 298 Step 2180/5304 Paf1 566.771606 Paf2 529.618225 Paf3 524.863098 Heatmap 139.459808 Total loss 1760.71277\n",
      "Epoch 298 Step 2190/5304 Paf1 730.791 Paf2 642.301575 Paf3 661.104065 Heatmap 184.727982 Total loss 2218.92456\n",
      "Epoch 298 Step 2200/5304 Paf1 778.117493 Paf2 749.760254 Paf3 755.707031 Heatmap 288.620544 Total loss 2572.20532\n",
      "Epoch 298 Step 2210/5304 Paf1 535.750488 Paf2 513.995667 Paf3 500.539429 Heatmap 180.184967 Total loss 1730.47046\n",
      "Epoch 298 Step 2220/5304 Paf1 727.961426 Paf2 687.523499 Paf3 694.201721 Heatmap 227.646851 Total loss 2337.3335\n",
      "Epoch 298 Step 2230/5304 Paf1 567.834351 Paf2 521.202393 Paf3 529.852539 Heatmap 145.403488 Total loss 1764.29272\n",
      "Epoch 298 Step 2240/5304 Paf1 650.569214 Paf2 594.28363 Paf3 592.919 Heatmap 184.100769 Total loss 2021.87256\n",
      "Epoch 298 Step 2250/5304 Paf1 743.814331 Paf2 722.481506 Paf3 721.806458 Heatmap 242.733429 Total loss 2430.83594\n",
      "Epoch 298 Step 2260/5304 Paf1 404.742706 Paf2 397.999237 Paf3 393.434723 Heatmap 113.369431 Total loss 1309.54614\n",
      "Epoch 298 Step 2270/5304 Paf1 431.28653 Paf2 425.393768 Paf3 411.049622 Heatmap 106.424515 Total loss 1374.15442\n",
      "Epoch 298 Step 2280/5304 Paf1 439.062408 Paf2 406.341614 Paf3 409.481934 Heatmap 117.146423 Total loss 1372.03247\n",
      "Epoch 298 Step 2290/5304 Paf1 740.659363 Paf2 680.343811 Paf3 678.335815 Heatmap 247.02211 Total loss 2346.36108\n",
      "Epoch 298 Step 2300/5304 Paf1 625.147095 Paf2 553.618591 Paf3 568.108215 Heatmap 184.479034 Total loss 1931.35291\n",
      "Epoch 298 Step 2310/5304 Paf1 638.014404 Paf2 594.771362 Paf3 578.267334 Heatmap 187.853134 Total loss 1998.90625\n",
      "Epoch 298 Step 2320/5304 Paf1 829.331665 Paf2 764.28125 Paf3 756.957397 Heatmap 250.06427 Total loss 2600.63452\n",
      "Epoch 298 Step 2330/5304 Paf1 481.098938 Paf2 451.698792 Paf3 462.470947 Heatmap 158.366989 Total loss 1553.63574\n",
      "Epoch 298 Step 2340/5304 Paf1 527.571899 Paf2 481.797974 Paf3 469.688446 Heatmap 141.454758 Total loss 1620.51306\n",
      "Epoch 298 Step 2350/5304 Paf1 489.887177 Paf2 458.700623 Paf3 445.641785 Heatmap 128.327057 Total loss 1522.55664\n",
      "Epoch 298 Step 2360/5304 Paf1 779.857666 Paf2 723.459045 Paf3 725.647034 Heatmap 220.744751 Total loss 2449.7085\n",
      "Epoch 298 Step 2370/5304 Paf1 397.564117 Paf2 376.845764 Paf3 360.185 Heatmap 112.777855 Total loss 1247.3728\n",
      "Epoch 298 Step 2380/5304 Paf1 476.979218 Paf2 434.04071 Paf3 432.077209 Heatmap 165.251846 Total loss 1508.34888\n",
      "Epoch 298 Step 2390/5304 Paf1 566.600342 Paf2 534.053589 Paf3 522.251831 Heatmap 148.91185 Total loss 1771.81763\n",
      "Epoch 298 Step 2400/5304 Paf1 853.990234 Paf2 799.017334 Paf3 805.879883 Heatmap 244.745789 Total loss 2703.6333\n",
      "Epoch 298 Step 2410/5304 Paf1 488.524414 Paf2 457.547668 Paf3 458.208557 Heatmap 135.935059 Total loss 1540.2157\n",
      "Epoch 298 Step 2420/5304 Paf1 653.404968 Paf2 602.009949 Paf3 598.379211 Heatmap 164.618896 Total loss 2018.41309\n",
      "Epoch 298 Step 2430/5304 Paf1 984.075195 Paf2 941.258057 Paf3 936.727661 Heatmap 307.174377 Total loss 3169.23535\n",
      "Epoch 298 Step 2440/5304 Paf1 583.6745 Paf2 542.980225 Paf3 537.82843 Heatmap 186.243591 Total loss 1850.72681\n",
      "Epoch 298 Step 2450/5304 Paf1 1107.13611 Paf2 1072.39563 Paf3 1043.02551 Heatmap 351.888702 Total loss 3574.4458\n",
      "Epoch 298 Step 2460/5304 Paf1 861.750244 Paf2 836.290466 Paf3 815.428101 Heatmap 310.986328 Total loss 2824.45508\n",
      "Epoch 298 Step 2470/5304 Paf1 639.703369 Paf2 610.688477 Paf3 608.873 Heatmap 229.188385 Total loss 2088.45312\n",
      "Epoch 298 Step 2480/5304 Paf1 903.923584 Paf2 844.472 Paf3 832.431641 Heatmap 303.164948 Total loss 2883.99219\n",
      "Epoch 298 Step 2490/5304 Paf1 553.614075 Paf2 493.12323 Paf3 506.634796 Heatmap 125.603378 Total loss 1678.97546\n",
      "Epoch 298 Step 2500/5304 Paf1 627.263245 Paf2 558.773804 Paf3 585.086792 Heatmap 174.950424 Total loss 1946.07434\n",
      "Epoch 298 Step 2510/5304 Paf1 730.965759 Paf2 678.805786 Paf3 688.684937 Heatmap 193.656067 Total loss 2292.11255\n",
      "Epoch 298 Step 2520/5304 Paf1 874.773132 Paf2 834.12262 Paf3 822.596191 Heatmap 310.486023 Total loss 2841.97803\n",
      "Epoch 298 Step 2530/5304 Paf1 891.457397 Paf2 826.067932 Paf3 840.457642 Heatmap 306.487671 Total loss 2864.4707\n",
      "Epoch 298 Step 2540/5304 Paf1 530.924133 Paf2 505.077423 Paf3 493.887695 Heatmap 172.374298 Total loss 1702.26355\n",
      "Epoch 298 Step 2550/5304 Paf1 608.692139 Paf2 556.518127 Paf3 565.551697 Heatmap 179.826111 Total loss 1910.58801\n",
      "Epoch 298 Step 2560/5304 Paf1 737.622314 Paf2 685.525635 Paf3 681.482483 Heatmap 205.3638 Total loss 2309.99414\n",
      "Epoch 298 Step 2570/5304 Paf1 1280.9043 Paf2 1218.45312 Paf3 1216.1366 Heatmap 455.869263 Total loss 4171.36328\n",
      "Epoch 298 Step 2580/5304 Paf1 569.22345 Paf2 523.947815 Paf3 523.515381 Heatmap 159.141083 Total loss 1775.82776\n",
      "Epoch 298 Step 2590/5304 Paf1 803.210449 Paf2 772.653687 Paf3 757.266479 Heatmap 247.330627 Total loss 2580.46118\n",
      "Epoch 298 Step 2600/5304 Paf1 680.528198 Paf2 616.652954 Paf3 614.249939 Heatmap 193.991821 Total loss 2105.42285\n",
      "Epoch 298 Step 2610/5304 Paf1 875.453186 Paf2 811.991089 Paf3 805.270081 Heatmap 252.27179 Total loss 2744.98633\n",
      "Epoch 298 Step 2620/5304 Paf1 999.159119 Paf2 931.699158 Paf3 926.657837 Heatmap 359.745361 Total loss 3217.26147\n",
      "Epoch 298 Step 2630/5304 Paf1 716.367798 Paf2 681.012939 Paf3 680.927368 Heatmap 248.910065 Total loss 2327.21826\n",
      "Epoch 298 Step 2640/5304 Paf1 1062.73218 Paf2 1041.95874 Paf3 1017.26245 Heatmap 376.245178 Total loss 3498.19849\n",
      "Epoch 298 Step 2650/5304 Paf1 557.651611 Paf2 558.11676 Paf3 551.867126 Heatmap 177.372467 Total loss 1845.00793\n",
      "Epoch 298 Step 2660/5304 Paf1 681.087097 Paf2 656.249329 Paf3 635.86731 Heatmap 221.413 Total loss 2194.6167\n",
      "Epoch 298 Step 2670/5304 Paf1 600.215149 Paf2 553.650818 Paf3 558.198914 Heatmap 155.376373 Total loss 1867.44128\n",
      "Epoch 298 Step 2680/5304 Paf1 573.868225 Paf2 530.178406 Paf3 527.232666 Heatmap 206.563202 Total loss 1837.84253\n",
      "Epoch 298 Step 2690/5304 Paf1 722.090149 Paf2 669.765076 Paf3 677.13385 Heatmap 185.98941 Total loss 2254.97852\n",
      "Epoch 298 Step 2700/5304 Paf1 522.445801 Paf2 483.846283 Paf3 475.496552 Heatmap 156.96228 Total loss 1638.75098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 298 Step 2710/5304 Paf1 985.749268 Paf2 917.304077 Paf3 933.894226 Heatmap 316.318787 Total loss 3153.26636\n",
      "Epoch 298 Step 2720/5304 Paf1 498.695923 Paf2 469.403564 Paf3 465.744324 Heatmap 151.954681 Total loss 1585.79846\n",
      "Epoch 298 Step 2730/5304 Paf1 805.995178 Paf2 772.208496 Paf3 776.984314 Heatmap 311.12793 Total loss 2666.31592\n",
      "Epoch 298 Step 2740/5304 Paf1 690.637756 Paf2 641.258728 Paf3 666.037659 Heatmap 204.252304 Total loss 2202.18652\n",
      "Epoch 298 Step 2750/5304 Paf1 637.388916 Paf2 594.736084 Paf3 610.211365 Heatmap 186.710098 Total loss 2029.04639\n",
      "Epoch 298 Step 2760/5304 Paf1 639.833252 Paf2 585.039124 Paf3 573.942627 Heatmap 205.06958 Total loss 2003.88452\n",
      "Epoch 298 Step 2770/5304 Paf1 390.680573 Paf2 368.992706 Paf3 365.278046 Heatmap 112.711037 Total loss 1237.66235\n",
      "Epoch 298 Step 2780/5304 Paf1 576.438965 Paf2 511.445465 Paf3 516.444458 Heatmap 179.568771 Total loss 1783.89771\n",
      "Epoch 298 Step 2790/5304 Paf1 799.066101 Paf2 761.98877 Paf3 765.279541 Heatmap 232.406738 Total loss 2558.74121\n",
      "Epoch 298 Step 2800/5304 Paf1 514.980835 Paf2 459.551361 Paf3 481.320923 Heatmap 158.845856 Total loss 1614.69897\n",
      "Epoch 298 Step 2810/5304 Paf1 663.498596 Paf2 646.139648 Paf3 644.433472 Heatmap 214.041061 Total loss 2168.11279\n",
      "Epoch 298 Step 2820/5304 Paf1 526.068665 Paf2 498.606 Paf3 491.536377 Heatmap 208.321365 Total loss 1724.53247\n",
      "Epoch 298 Step 2830/5304 Paf1 860.238953 Paf2 836.348083 Paf3 847.518738 Heatmap 299.085693 Total loss 2843.19141\n",
      "Epoch 298 Step 2840/5304 Paf1 807.872192 Paf2 755.122314 Paf3 730.883423 Heatmap 230.939209 Total loss 2524.81714\n",
      "Epoch 298 Step 2850/5304 Paf1 502.381622 Paf2 473.237732 Paf3 481.79126 Heatmap 164.292221 Total loss 1621.70288\n",
      "Epoch 298 Step 2860/5304 Paf1 742.671753 Paf2 701.028748 Paf3 693.319397 Heatmap 223.840973 Total loss 2360.86084\n",
      "Epoch 298 Step 2870/5304 Paf1 670.522522 Paf2 626 Paf3 642.3573 Heatmap 220.838959 Total loss 2159.71875\n",
      "Epoch 298 Step 2880/5304 Paf1 758.453125 Paf2 732.495605 Paf3 724.382385 Heatmap 266.112396 Total loss 2481.44336\n",
      "Epoch 298 Step 2890/5304 Paf1 619.590454 Paf2 556.810547 Paf3 562.756775 Heatmap 189.878265 Total loss 1929.03601\n",
      "Epoch 298 Step 2900/5304 Paf1 591.372864 Paf2 562.149597 Paf3 549.087 Heatmap 160.777527 Total loss 1863.38696\n",
      "Epoch 298 Step 2910/5304 Paf1 581.894531 Paf2 539.279541 Paf3 551.090698 Heatmap 206.423386 Total loss 1878.68823\n",
      "Epoch 298 Step 2920/5304 Paf1 832.30957 Paf2 800.132 Paf3 799.113647 Heatmap 261.869 Total loss 2693.42432\n",
      "Epoch 298 Step 2930/5304 Paf1 531.785095 Paf2 501.854218 Paf3 499.209 Heatmap 168.180618 Total loss 1701.02893\n",
      "Epoch 298 Step 2940/5304 Paf1 853.112793 Paf2 812.773499 Paf3 789.252441 Heatmap 256.352356 Total loss 2711.49097\n",
      "Epoch 298 Step 2950/5304 Paf1 731.149353 Paf2 698.985962 Paf3 709.216431 Heatmap 227.804047 Total loss 2367.15576\n",
      "Epoch 298 Step 2960/5304 Paf1 619.116272 Paf2 574.940552 Paf3 567.135376 Heatmap 170.366669 Total loss 1931.55896\n",
      "Epoch 298 Step 2970/5304 Paf1 523.776794 Paf2 492.148499 Paf3 494.746887 Heatmap 155.472748 Total loss 1666.1449\n",
      "Epoch 298 Step 2980/5304 Paf1 1029.50134 Paf2 978.989136 Paf3 973.506836 Heatmap 343.734192 Total loss 3325.73145\n",
      "Epoch 298 Step 2990/5304 Paf1 810.562927 Paf2 774.543457 Paf3 751.654724 Heatmap 237.706711 Total loss 2574.46777\n",
      "Epoch 298 Step 3000/5304 Paf1 1028.47498 Paf2 992.54364 Paf3 973.843201 Heatmap 381.730286 Total loss 3376.59204\n",
      "Saved checkpoint for step 3000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1785\n",
      "Epoch 298 Step 3010/5304 Paf1 758.161316 Paf2 730.74646 Paf3 738.869141 Heatmap 237.095352 Total loss 2464.87207\n",
      "Epoch 298 Step 3020/5304 Paf1 841.948 Paf2 805.300354 Paf3 783.134155 Heatmap 259.59967 Total loss 2689.98218\n",
      "Epoch 298 Step 3030/5304 Paf1 660.787537 Paf2 633.756287 Paf3 638.079773 Heatmap 227.364563 Total loss 2159.98828\n",
      "Epoch 298 Step 3040/5304 Paf1 772.112732 Paf2 739.513916 Paf3 742.148804 Heatmap 260.027679 Total loss 2513.80322\n",
      "Epoch 298 Step 3050/5304 Paf1 713.613 Paf2 671.278564 Paf3 644.269897 Heatmap 215.176666 Total loss 2244.33813\n",
      "Epoch 298 Step 3060/5304 Paf1 759.947754 Paf2 725.290771 Paf3 703.095825 Heatmap 232.783 Total loss 2421.11743\n",
      "Epoch 298 Step 3070/5304 Paf1 507.511749 Paf2 460.372803 Paf3 442.605957 Heatmap 158.012634 Total loss 1568.50317\n",
      "Epoch 298 Step 3080/5304 Paf1 638.744 Paf2 599.588562 Paf3 603.525574 Heatmap 192.583633 Total loss 2034.44165\n",
      "Epoch 298 Step 3090/5304 Paf1 705.680481 Paf2 655.503662 Paf3 657.188 Heatmap 229.592834 Total loss 2247.96484\n",
      "Epoch 298 Step 3100/5304 Paf1 735.030823 Paf2 692.647827 Paf3 709.065857 Heatmap 233.386078 Total loss 2370.13062\n",
      "Epoch 298 Step 3110/5304 Paf1 510.597443 Paf2 473.136566 Paf3 468.298126 Heatmap 143.963715 Total loss 1595.99585\n",
      "Epoch 298 Step 3120/5304 Paf1 706.90918 Paf2 680.887268 Paf3 686.65918 Heatmap 246.311249 Total loss 2320.76685\n",
      "Epoch 298 Step 3130/5304 Paf1 631.981628 Paf2 585.818787 Paf3 591.651123 Heatmap 207.617477 Total loss 2017.06909\n",
      "Epoch 298 Step 3140/5304 Paf1 569.344788 Paf2 537.020386 Paf3 557.119568 Heatmap 153.660049 Total loss 1817.14478\n",
      "Epoch 298 Step 3150/5304 Paf1 707.215698 Paf2 635.974121 Paf3 640.013184 Heatmap 193.140747 Total loss 2176.34375\n",
      "Epoch 298 Step 3160/5304 Paf1 589.184204 Paf2 573.527588 Paf3 563.741211 Heatmap 150.687775 Total loss 1877.14075\n",
      "Epoch 298 Step 3170/5304 Paf1 788.873535 Paf2 726.384277 Paf3 736.537231 Heatmap 255.149872 Total loss 2506.94482\n",
      "Epoch 298 Step 3180/5304 Paf1 577.425659 Paf2 511.182129 Paf3 513.244873 Heatmap 151.119461 Total loss 1752.97217\n",
      "Epoch 298 Step 3190/5304 Paf1 626.770874 Paf2 569.010803 Paf3 558.576538 Heatmap 199.894287 Total loss 1954.25256\n",
      "Epoch 298 Step 3200/5304 Paf1 552.730835 Paf2 525.734131 Paf3 513.97522 Heatmap 241.391953 Total loss 1833.83215\n",
      "Epoch 298 Step 3210/5304 Paf1 650.565247 Paf2 596.096558 Paf3 624.688782 Heatmap 189.157181 Total loss 2060.50781\n",
      "Epoch 298 Step 3220/5304 Paf1 665.347839 Paf2 607.762634 Paf3 605.929443 Heatmap 204.722885 Total loss 2083.7627\n",
      "Epoch 298 Step 3230/5304 Paf1 740.294434 Paf2 693.455933 Paf3 715.364 Heatmap 250.025864 Total loss 2399.14014\n",
      "Epoch 298 Step 3240/5304 Paf1 1201.01685 Paf2 1155.54272 Paf3 1167.88025 Heatmap 412.502319 Total loss 3936.94214\n",
      "Epoch 298 Step 3250/5304 Paf1 339.840302 Paf2 320.399 Paf3 318.70752 Heatmap 123.579391 Total loss 1102.52612\n",
      "Epoch 298 Step 3260/5304 Paf1 652.451599 Paf2 599.986 Paf3 593.88208 Heatmap 215.30423 Total loss 2061.62402\n",
      "Epoch 298 Step 3270/5304 Paf1 881.5896 Paf2 875.557495 Paf3 865.705 Heatmap 267.805267 Total loss 2890.65723\n",
      "Epoch 298 Step 3280/5304 Paf1 679.375671 Paf2 644.885803 Paf3 665.796448 Heatmap 201.518066 Total loss 2191.57593\n",
      "Epoch 298 Step 3290/5304 Paf1 583.604248 Paf2 554.598633 Paf3 547.967163 Heatmap 149.234283 Total loss 1835.4043\n",
      "Epoch 298 Step 3300/5304 Paf1 848.429138 Paf2 789.99115 Paf3 784.822388 Heatmap 265.280396 Total loss 2688.52295\n",
      "Epoch 298 Step 3310/5304 Paf1 873.220154 Paf2 821.593323 Paf3 825.373108 Heatmap 266.759949 Total loss 2786.94653\n",
      "Epoch 298 Step 3320/5304 Paf1 415.200287 Paf2 383.08725 Paf3 389.80722 Heatmap 140.849991 Total loss 1328.94482\n",
      "Epoch 298 Step 3330/5304 Paf1 540.827271 Paf2 507.61908 Paf3 517.872742 Heatmap 200.765961 Total loss 1767.08496\n",
      "Epoch 298 Step 3340/5304 Paf1 584.294 Paf2 534.329468 Paf3 528.942749 Heatmap 158.387115 Total loss 1805.95337\n",
      "Epoch 298 Step 3350/5304 Paf1 687.895508 Paf2 673.658447 Paf3 661.739136 Heatmap 237.168884 Total loss 2260.46191\n",
      "Epoch 298 Step 3360/5304 Paf1 569.047791 Paf2 535.787415 Paf3 525.078735 Heatmap 163.282639 Total loss 1793.19653\n",
      "Epoch 298 Step 3370/5304 Paf1 589.175293 Paf2 551.159241 Paf3 546.982666 Heatmap 176.180435 Total loss 1863.49756\n",
      "Epoch 298 Step 3380/5304 Paf1 649.427551 Paf2 613.29657 Paf3 612.659546 Heatmap 194.224213 Total loss 2069.60791\n",
      "Epoch 298 Step 3390/5304 Paf1 508.75708 Paf2 472.839508 Paf3 481.694489 Heatmap 173.77594 Total loss 1637.06702\n",
      "Epoch 298 Step 3400/5304 Paf1 557.066406 Paf2 527.784058 Paf3 527.950745 Heatmap 148.765442 Total loss 1761.56665\n",
      "Epoch 298 Step 3410/5304 Paf1 500.712067 Paf2 462.77179 Paf3 474.34024 Heatmap 147.317047 Total loss 1585.14111\n",
      "Epoch 298 Step 3420/5304 Paf1 672.374634 Paf2 629.702332 Paf3 634.835449 Heatmap 197.0811 Total loss 2133.99341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 298 Step 3430/5304 Paf1 653.896118 Paf2 610.59375 Paf3 628.027222 Heatmap 204.484299 Total loss 2097.00146\n",
      "Epoch 298 Step 3440/5304 Paf1 949.094482 Paf2 907.154907 Paf3 896.667236 Heatmap 305.510406 Total loss 3058.427\n",
      "Epoch 298 Step 3450/5304 Paf1 515.234558 Paf2 472.955139 Paf3 463.990784 Heatmap 173.281769 Total loss 1625.46228\n",
      "Epoch 298 Step 3460/5304 Paf1 879.915771 Paf2 837.472595 Paf3 834.938232 Heatmap 332.469543 Total loss 2884.79614\n",
      "Epoch 298 Step 3470/5304 Paf1 497.289185 Paf2 463.468933 Paf3 463.996216 Heatmap 179.589828 Total loss 1604.34424\n",
      "Epoch 298 Step 3480/5304 Paf1 557.616516 Paf2 524.489868 Paf3 527.532349 Heatmap 166.415451 Total loss 1776.0542\n",
      "Epoch 298 Step 3490/5304 Paf1 587.871948 Paf2 546.200256 Paf3 525.202576 Heatmap 168.950958 Total loss 1828.22583\n",
      "Epoch 298 Step 3500/5304 Paf1 567.246033 Paf2 517.01239 Paf3 514.61084 Heatmap 134.678665 Total loss 1733.54785\n",
      "Epoch 298 Step 3510/5304 Paf1 503.134766 Paf2 472.511169 Paf3 470.395905 Heatmap 156.645706 Total loss 1602.6875\n",
      "Epoch 298 Step 3520/5304 Paf1 654.955566 Paf2 620.713318 Paf3 611.032837 Heatmap 194.456543 Total loss 2081.1582\n",
      "Epoch 298 Step 3530/5304 Paf1 638.916443 Paf2 593.480713 Paf3 599.708313 Heatmap 187.652954 Total loss 2019.75854\n",
      "Epoch 298 Step 3540/5304 Paf1 647.866394 Paf2 589.593445 Paf3 608.603088 Heatmap 208.1642 Total loss 2054.22705\n",
      "Epoch 298 Step 3550/5304 Paf1 969.045288 Paf2 906.223633 Paf3 915.86554 Heatmap 275.223022 Total loss 3066.35742\n",
      "Epoch 298 Step 3560/5304 Paf1 900.684082 Paf2 860.560669 Paf3 873.190186 Heatmap 273.429108 Total loss 2907.86401\n",
      "Epoch 298 Step 3570/5304 Paf1 708.820068 Paf2 676.563599 Paf3 670.948853 Heatmap 240.213593 Total loss 2296.54614\n",
      "Epoch 298 Step 3580/5304 Paf1 1228.03271 Paf2 1141.64124 Paf3 1148.47241 Heatmap 387.866821 Total loss 3906.01318\n",
      "Epoch 298 Step 3590/5304 Paf1 580.931213 Paf2 548.101562 Paf3 538.024536 Heatmap 186.942596 Total loss 1853.99988\n",
      "Epoch 298 Step 3600/5304 Paf1 631.411316 Paf2 612.657349 Paf3 602.72937 Heatmap 201.76947 Total loss 2048.56738\n",
      "Epoch 298 Step 3610/5304 Paf1 561.618042 Paf2 550.696655 Paf3 521.950195 Heatmap 208.680939 Total loss 1842.9458\n",
      "Epoch 298 Step 3620/5304 Paf1 663.42 Paf2 636.511536 Paf3 643.70752 Heatmap 197.3573 Total loss 2140.99634\n",
      "Epoch 298 Step 3630/5304 Paf1 475.570923 Paf2 454.706543 Paf3 447.271271 Heatmap 166.952774 Total loss 1544.50146\n",
      "Epoch 298 Step 3640/5304 Paf1 653.60144 Paf2 624.715576 Paf3 626.835815 Heatmap 211.491043 Total loss 2116.6438\n",
      "Epoch 298 Step 3650/5304 Paf1 578.802368 Paf2 545.251526 Paf3 546.634949 Heatmap 166.478241 Total loss 1837.16711\n",
      "Epoch 298 Step 3660/5304 Paf1 672.795654 Paf2 622.702 Paf3 592.10968 Heatmap 208.461853 Total loss 2096.06934\n",
      "Epoch 298 Step 3670/5304 Paf1 1027.96472 Paf2 987.221375 Paf3 990.670898 Heatmap 283.853394 Total loss 3289.71045\n",
      "Epoch 298 Step 3680/5304 Paf1 675.23938 Paf2 639.593933 Paf3 651.819641 Heatmap 224.82312 Total loss 2191.47607\n",
      "Epoch 298 Step 3690/5304 Paf1 620.930542 Paf2 586.502869 Paf3 579.482239 Heatmap 193.796631 Total loss 1980.71216\n",
      "Epoch 298 Step 3700/5304 Paf1 577.483276 Paf2 545.245728 Paf3 544.345886 Heatmap 137.276154 Total loss 1804.35107\n",
      "Epoch 298 Step 3710/5304 Paf1 632.372437 Paf2 584.03949 Paf3 586.846191 Heatmap 204.569122 Total loss 2007.82715\n",
      "Epoch 298 Step 3720/5304 Paf1 543.182373 Paf2 515.405151 Paf3 518.195923 Heatmap 140.820038 Total loss 1717.60352\n",
      "Epoch 298 Step 3730/5304 Paf1 721.929504 Paf2 669.209839 Paf3 673.777 Heatmap 235.738663 Total loss 2300.65503\n",
      "Epoch 298 Step 3740/5304 Paf1 628.452454 Paf2 598.819641 Paf3 589.986206 Heatmap 176.443771 Total loss 1993.70215\n",
      "Epoch 298 Step 3750/5304 Paf1 695.618713 Paf2 670.790771 Paf3 671.851685 Heatmap 220.049332 Total loss 2258.31055\n",
      "Epoch 298 Step 3760/5304 Paf1 1080.30835 Paf2 1032.38904 Paf3 1017.79102 Heatmap 300.044434 Total loss 3430.53271\n",
      "Epoch 298 Step 3770/5304 Paf1 1141.56079 Paf2 1095.54272 Paf3 1101.23572 Heatmap 434.378448 Total loss 3772.71777\n",
      "Epoch 298 Step 3780/5304 Paf1 761.513672 Paf2 711.192627 Paf3 724.182129 Heatmap 228.482758 Total loss 2425.37109\n",
      "Epoch 298 Step 3790/5304 Paf1 615.675476 Paf2 570.224609 Paf3 581.112549 Heatmap 227.597488 Total loss 1994.61011\n",
      "Epoch 298 Step 3800/5304 Paf1 795.957092 Paf2 767.851624 Paf3 751.517578 Heatmap 238.823456 Total loss 2554.15\n",
      "Epoch 298 Step 3810/5304 Paf1 699.169189 Paf2 646.463623 Paf3 634.712769 Heatmap 207.005051 Total loss 2187.35059\n",
      "Epoch 298 Step 3820/5304 Paf1 599.456055 Paf2 568.355164 Paf3 578.52 Heatmap 151.550781 Total loss 1897.88208\n",
      "Epoch 298 Step 3830/5304 Paf1 567.727173 Paf2 536.824 Paf3 520.461487 Heatmap 143.255722 Total loss 1768.26831\n",
      "Epoch 298 Step 3840/5304 Paf1 784.625427 Paf2 733.715 Paf3 744.480164 Heatmap 274.490173 Total loss 2537.31079\n",
      "Epoch 298 Step 3850/5304 Paf1 696.193176 Paf2 634.651489 Paf3 634.783569 Heatmap 202.305344 Total loss 2167.93359\n",
      "Epoch 298 Step 3860/5304 Paf1 670.147339 Paf2 630.770081 Paf3 630.46167 Heatmap 196.018387 Total loss 2127.39746\n",
      "Epoch 298 Step 3870/5304 Paf1 620.079224 Paf2 586.984375 Paf3 604.545471 Heatmap 194.206406 Total loss 2005.81543\n",
      "Epoch 298 Step 3880/5304 Paf1 498.226379 Paf2 462.858032 Paf3 467.44458 Heatmap 132.667816 Total loss 1561.19678\n",
      "Epoch 298 Step 3890/5304 Paf1 680.138733 Paf2 631.531799 Paf3 630.236572 Heatmap 238.210907 Total loss 2180.11816\n",
      "Epoch 298 Step 3900/5304 Paf1 514.495728 Paf2 507.312592 Paf3 492.037689 Heatmap 146.852417 Total loss 1660.69849\n",
      "Epoch 298 Step 3910/5304 Paf1 736.84021 Paf2 701.800537 Paf3 679.735657 Heatmap 235.144287 Total loss 2353.52075\n",
      "Epoch 298 Step 3920/5304 Paf1 745.201782 Paf2 665.31488 Paf3 673.010376 Heatmap 225.714325 Total loss 2309.24121\n",
      "Epoch 298 Step 3930/5304 Paf1 612.608215 Paf2 591.653625 Paf3 586.440125 Heatmap 174.086975 Total loss 1964.78894\n",
      "Epoch 298 Step 3940/5304 Paf1 475.429321 Paf2 444.49054 Paf3 429.802429 Heatmap 125.650574 Total loss 1475.3728\n",
      "Epoch 298 Step 3950/5304 Paf1 453.837372 Paf2 433.477173 Paf3 442.263062 Heatmap 136.481583 Total loss 1466.0592\n",
      "Epoch 298 Step 3960/5304 Paf1 737.67572 Paf2 684.793335 Paf3 670.233704 Heatmap 210.500504 Total loss 2303.20312\n",
      "Epoch 298 Step 3970/5304 Paf1 875.623 Paf2 835.68335 Paf3 835.054871 Heatmap 288.783936 Total loss 2835.14526\n",
      "Epoch 298 Step 3980/5304 Paf1 678.515564 Paf2 631.828552 Paf3 615.760864 Heatmap 223.76683 Total loss 2149.87183\n",
      "Epoch 298 Step 3990/5304 Paf1 573.36969 Paf2 517.218445 Paf3 505.5896 Heatmap 169.562836 Total loss 1765.7406\n",
      "Epoch 298 Step 4000/5304 Paf1 653.406 Paf2 606.270935 Paf3 608.011108 Heatmap 227.785934 Total loss 2095.47412\n",
      "Saved checkpoint for step 4000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1786\n",
      "Epoch 298 Step 4010/5304 Paf1 425.063568 Paf2 395.289612 Paf3 372.76297 Heatmap 121.883553 Total loss 1314.99963\n",
      "Epoch 298 Step 4020/5304 Paf1 558.175659 Paf2 517.721863 Paf3 514.339905 Heatmap 172.618362 Total loss 1762.85571\n",
      "Epoch 298 Step 4030/5304 Paf1 700.10437 Paf2 659.175354 Paf3 666.30365 Heatmap 244.770844 Total loss 2270.35425\n",
      "Epoch 298 Step 4040/5304 Paf1 749.538208 Paf2 678.477478 Paf3 677.970093 Heatmap 225.031906 Total loss 2331.01758\n",
      "Epoch 298 Step 4050/5304 Paf1 587.303223 Paf2 545.228333 Paf3 546.066895 Heatmap 196.563904 Total loss 1875.16235\n",
      "Epoch 298 Step 4060/5304 Paf1 921.289795 Paf2 868.34845 Paf3 857.919189 Heatmap 280.602661 Total loss 2928.16016\n",
      "Epoch 298 Step 4070/5304 Paf1 670.465576 Paf2 637.746521 Paf3 638.959473 Heatmap 169.141647 Total loss 2116.31323\n",
      "Epoch 298 Step 4080/5304 Paf1 547.544495 Paf2 504.715668 Paf3 510.702209 Heatmap 160.914291 Total loss 1723.87671\n",
      "Epoch 298 Step 4090/5304 Paf1 335.764618 Paf2 297.644043 Paf3 291.344696 Heatmap 78.4356232 Total loss 1003.18903\n",
      "Epoch 298 Step 4100/5304 Paf1 516.943 Paf2 504.525085 Paf3 491.733093 Heatmap 185.472794 Total loss 1698.67395\n",
      "Epoch 298 Step 4110/5304 Paf1 515.7052 Paf2 533.367126 Paf3 526.684875 Heatmap 209.309631 Total loss 1785.06677\n",
      "Epoch 298 Step 4120/5304 Paf1 505.14856 Paf2 463.615479 Paf3 459.032166 Heatmap 139.451584 Total loss 1567.2478\n",
      "Epoch 298 Step 4130/5304 Paf1 562.275635 Paf2 500.677948 Paf3 485.265 Heatmap 149.266678 Total loss 1697.48535\n",
      "Epoch 298 Step 4140/5304 Paf1 677.3573 Paf2 635.127869 Paf3 641.890137 Heatmap 208.478271 Total loss 2162.85352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 298 Step 4150/5304 Paf1 656.690125 Paf2 633.39978 Paf3 624.449585 Heatmap 230.628128 Total loss 2145.16748\n",
      "Epoch 298 Step 4160/5304 Paf1 694.863098 Paf2 622.981445 Paf3 627.284729 Heatmap 174.325699 Total loss 2119.45483\n",
      "Epoch 298 Step 4170/5304 Paf1 870.005554 Paf2 796.52832 Paf3 782.56073 Heatmap 245.086502 Total loss 2694.18115\n",
      "Epoch 298 Step 4180/5304 Paf1 941.538208 Paf2 895.30957 Paf3 908.577881 Heatmap 288.108826 Total loss 3033.53467\n",
      "Epoch 298 Step 4190/5304 Paf1 782.154541 Paf2 727.315552 Paf3 729.685181 Heatmap 219.30986 Total loss 2458.46509\n",
      "Epoch 298 Step 4200/5304 Paf1 598.400696 Paf2 553.096497 Paf3 564.359131 Heatmap 178.878922 Total loss 1894.73523\n",
      "Epoch 298 Step 4210/5304 Paf1 535.899658 Paf2 502.330933 Paf3 512.468628 Heatmap 151.991119 Total loss 1702.69031\n",
      "Epoch 298 Step 4220/5304 Paf1 676.751343 Paf2 643.795044 Paf3 642.519165 Heatmap 242.916595 Total loss 2205.98218\n",
      "Epoch 298 Step 4230/5304 Paf1 585.393311 Paf2 564.31781 Paf3 553.8927 Heatmap 168.559479 Total loss 1872.16333\n",
      "Epoch 298 Step 4240/5304 Paf1 764.933899 Paf2 726.929321 Paf3 723.872437 Heatmap 240.220932 Total loss 2455.95654\n",
      "Epoch 298 Step 4250/5304 Paf1 550.298462 Paf2 503.82016 Paf3 508.347382 Heatmap 171.923721 Total loss 1734.38977\n",
      "Epoch 298 Step 4260/5304 Paf1 502.773071 Paf2 462.853882 Paf3 453.319672 Heatmap 161.106201 Total loss 1580.05286\n",
      "Epoch 298 Step 4270/5304 Paf1 743.288696 Paf2 699.671509 Paf3 700.395813 Heatmap 223.724365 Total loss 2367.08032\n",
      "Epoch 298 Step 4280/5304 Paf1 591.289185 Paf2 544.043823 Paf3 545.567383 Heatmap 176.991196 Total loss 1857.8916\n",
      "Epoch 298 Step 4290/5304 Paf1 556.055542 Paf2 501.991699 Paf3 499.289734 Heatmap 167.947464 Total loss 1725.28442\n",
      "Epoch 298 Step 4300/5304 Paf1 555.127747 Paf2 487.481934 Paf3 499.969604 Heatmap 141.958313 Total loss 1684.5376\n",
      "Epoch 298 Step 4310/5304 Paf1 394.966949 Paf2 363.260559 Paf3 354.919556 Heatmap 124.14653 Total loss 1237.2937\n",
      "Epoch 298 Step 4320/5304 Paf1 503.112549 Paf2 461.680084 Paf3 452.600342 Heatmap 157.859833 Total loss 1575.25281\n",
      "Epoch 298 Step 4330/5304 Paf1 719.401245 Paf2 691.612915 Paf3 676.833435 Heatmap 210.139465 Total loss 2297.98706\n",
      "Epoch 298 Step 4340/5304 Paf1 613.954712 Paf2 623.617676 Paf3 601.608887 Heatmap 198.351562 Total loss 2037.53284\n",
      "Epoch 298 Step 4350/5304 Paf1 903.272583 Paf2 866.909119 Paf3 846.353271 Heatmap 284.629944 Total loss 2901.16479\n",
      "Epoch 298 Step 4360/5304 Paf1 495.87381 Paf2 449.071594 Paf3 446.480377 Heatmap 129.590546 Total loss 1521.01636\n",
      "Epoch 298 Step 4370/5304 Paf1 469.784668 Paf2 427.351959 Paf3 420.904 Heatmap 116.867332 Total loss 1434.90796\n",
      "Epoch 298 Step 4380/5304 Paf1 853.970459 Paf2 807.389404 Paf3 805.391541 Heatmap 257.121338 Total loss 2723.8728\n",
      "Epoch 298 Step 4390/5304 Paf1 843.462891 Paf2 765.399597 Paf3 778.855225 Heatmap 237.860397 Total loss 2625.57812\n",
      "Epoch 298 Step 4400/5304 Paf1 516.259521 Paf2 467.118835 Paf3 485.323395 Heatmap 147.963684 Total loss 1616.66553\n",
      "Epoch 298 Step 4410/5304 Paf1 855.759827 Paf2 806.783875 Paf3 797.107056 Heatmap 292.970825 Total loss 2752.62158\n",
      "Epoch 298 Step 4420/5304 Paf1 606.279114 Paf2 577.051514 Paf3 566.589905 Heatmap 154.107681 Total loss 1904.02808\n",
      "Epoch 298 Step 4430/5304 Paf1 662.774719 Paf2 611.737183 Paf3 630.158447 Heatmap 185.006882 Total loss 2089.67725\n",
      "Epoch 298 Step 4440/5304 Paf1 884.909241 Paf2 869.112305 Paf3 850.705322 Heatmap 333.439636 Total loss 2938.1665\n",
      "Epoch 298 Step 4450/5304 Paf1 751.563538 Paf2 701.037842 Paf3 705.305542 Heatmap 264.700531 Total loss 2422.60742\n",
      "Epoch 298 Step 4460/5304 Paf1 495.365295 Paf2 458.520203 Paf3 455.467072 Heatmap 149.936249 Total loss 1559.28882\n",
      "Epoch 298 Step 4470/5304 Paf1 557.384399 Paf2 518.268188 Paf3 514.674927 Heatmap 159.642639 Total loss 1749.97021\n",
      "Epoch 298 Step 4480/5304 Paf1 800.530823 Paf2 723.910522 Paf3 747.361816 Heatmap 209.893494 Total loss 2481.69678\n",
      "Epoch 298 Step 4490/5304 Paf1 831.547729 Paf2 805.107605 Paf3 797.647766 Heatmap 278.783875 Total loss 2713.08691\n",
      "Epoch 298 Step 4500/5304 Paf1 879.114502 Paf2 820.728 Paf3 806.903442 Heatmap 245.01268 Total loss 2751.75879\n",
      "Epoch 298 Step 4510/5304 Paf1 490.270142 Paf2 448.164551 Paf3 445.9953 Heatmap 133.022919 Total loss 1517.45288\n",
      "Epoch 298 Step 4520/5304 Paf1 550.061035 Paf2 517.969727 Paf3 504.976349 Heatmap 159.733398 Total loss 1732.74048\n",
      "Epoch 298 Step 4530/5304 Paf1 873.041 Paf2 814.734619 Paf3 805.855835 Heatmap 250.006805 Total loss 2743.63818\n",
      "Epoch 298 Step 4540/5304 Paf1 337.487701 Paf2 300.176636 Paf3 292.102386 Heatmap 87.0076599 Total loss 1016.77435\n",
      "Epoch 298 Step 4550/5304 Paf1 782.801086 Paf2 774.492493 Paf3 764.198059 Heatmap 278.92688 Total loss 2600.41846\n",
      "Epoch 298 Step 4560/5304 Paf1 831.291443 Paf2 778.966064 Paf3 778.573547 Heatmap 218.22847 Total loss 2607.05957\n",
      "Epoch 298 Step 4570/5304 Paf1 541.913391 Paf2 510.843231 Paf3 523.498291 Heatmap 191.314926 Total loss 1767.56982\n",
      "Epoch 298 Step 4580/5304 Paf1 790.858154 Paf2 724.996 Paf3 706.5047 Heatmap 221.718231 Total loss 2444.07715\n",
      "Epoch 298 Step 4590/5304 Paf1 616.517273 Paf2 611.998901 Paf3 601.251953 Heatmap 194.858521 Total loss 2024.62659\n",
      "Epoch 298 Step 4600/5304 Paf1 688.549072 Paf2 643.084656 Paf3 638.573486 Heatmap 231.93161 Total loss 2202.13892\n",
      "Epoch 298 Step 4610/5304 Paf1 389.379822 Paf2 357.349 Paf3 353.911346 Heatmap 102.38736 Total loss 1203.02759\n",
      "Epoch 298 Step 4620/5304 Paf1 691.184082 Paf2 637.389038 Paf3 640.907593 Heatmap 227.199142 Total loss 2196.68\n",
      "Epoch 298 Step 4630/5304 Paf1 816.459595 Paf2 764.442444 Paf3 771.343201 Heatmap 247.991394 Total loss 2600.23682\n",
      "Epoch 298 Step 4640/5304 Paf1 624.50885 Paf2 588.308594 Paf3 582.137512 Heatmap 212.802475 Total loss 2007.75732\n",
      "Epoch 298 Step 4650/5304 Paf1 885.37439 Paf2 849.535034 Paf3 836.451782 Heatmap 287.516479 Total loss 2858.87769\n",
      "Epoch 298 Step 4660/5304 Paf1 383.090393 Paf2 365.183136 Paf3 376.95575 Heatmap 111.576553 Total loss 1236.80591\n",
      "Epoch 298 Step 4670/5304 Paf1 562.440063 Paf2 509.097839 Paf3 503.635834 Heatmap 156.213074 Total loss 1731.38672\n",
      "Epoch 298 Step 4680/5304 Paf1 671.848389 Paf2 633.420776 Paf3 633.843689 Heatmap 189.642212 Total loss 2128.75513\n",
      "Epoch 298 Step 4690/5304 Paf1 693.075684 Paf2 628.64325 Paf3 626.256409 Heatmap 191.653076 Total loss 2139.62842\n",
      "Epoch 298 Step 4700/5304 Paf1 846.435059 Paf2 810.170044 Paf3 820.951843 Heatmap 292.965332 Total loss 2770.52246\n",
      "Epoch 298 Step 4710/5304 Paf1 949.927124 Paf2 921.380737 Paf3 921.415588 Heatmap 351.704468 Total loss 3144.42798\n",
      "Epoch 298 Step 4720/5304 Paf1 606.332703 Paf2 593.21759 Paf3 581.456665 Heatmap 200.931824 Total loss 1981.93872\n",
      "Epoch 298 Step 4730/5304 Paf1 640.876282 Paf2 604.864746 Paf3 589.287292 Heatmap 168.470016 Total loss 2003.49829\n",
      "Epoch 298 Step 4740/5304 Paf1 968.660339 Paf2 945.28241 Paf3 948.984 Heatmap 339.006653 Total loss 3201.93359\n",
      "Epoch 298 Step 4750/5304 Paf1 618.063721 Paf2 577.920776 Paf3 577.673035 Heatmap 177.091522 Total loss 1950.74902\n",
      "Epoch 298 Step 4760/5304 Paf1 461.104492 Paf2 452.040131 Paf3 424.138519 Heatmap 147.462708 Total loss 1484.74585\n",
      "Epoch 298 Step 4770/5304 Paf1 687.08313 Paf2 678.144226 Paf3 677.534302 Heatmap 213.935654 Total loss 2256.69727\n",
      "Epoch 298 Step 4780/5304 Paf1 948.998352 Paf2 877.999512 Paf3 893.112183 Heatmap 272.548218 Total loss 2992.6582\n",
      "Epoch 298 Step 4790/5304 Paf1 384.545807 Paf2 375.712219 Paf3 369.014526 Heatmap 109.561508 Total loss 1238.83411\n",
      "Epoch 298 Step 4800/5304 Paf1 769.683228 Paf2 716.7323 Paf3 721.313 Heatmap 213.881027 Total loss 2421.60962\n",
      "Epoch 298 Step 4810/5304 Paf1 471.724121 Paf2 440.465149 Paf3 438.03479 Heatmap 139.576675 Total loss 1489.80078\n",
      "Epoch 298 Step 4820/5304 Paf1 488.80954 Paf2 457.32843 Paf3 458.903564 Heatmap 137.534286 Total loss 1542.57581\n",
      "Epoch 298 Step 4830/5304 Paf1 826.204712 Paf2 768.15387 Paf3 786.164795 Heatmap 276.099152 Total loss 2656.62256\n",
      "Epoch 298 Step 4840/5304 Paf1 567.869324 Paf2 550.832397 Paf3 546.777161 Heatmap 176.60228 Total loss 1842.08105\n",
      "Epoch 298 Step 4850/5304 Paf1 594.089722 Paf2 542.014099 Paf3 554.24884 Heatmap 168.874939 Total loss 1859.22754\n",
      "Epoch 298 Step 4860/5304 Paf1 570.951477 Paf2 516.367737 Paf3 515.856689 Heatmap 210.461029 Total loss 1813.63696\n",
      "Epoch 298 Step 4870/5304 Paf1 503.48941 Paf2 458.957 Paf3 460.05481 Heatmap 107.511559 Total loss 1530.0127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 298 Step 4880/5304 Paf1 599.522522 Paf2 553.104126 Paf3 542.733093 Heatmap 200.011719 Total loss 1895.37158\n",
      "Epoch 298 Step 4890/5304 Paf1 670.108521 Paf2 605.134949 Paf3 612.278137 Heatmap 173.854553 Total loss 2061.37598\n",
      "Epoch 298 Step 4900/5304 Paf1 742.792725 Paf2 704.985352 Paf3 707.571838 Heatmap 241.83844 Total loss 2397.18848\n",
      "Epoch 298 Step 4910/5304 Paf1 778.25116 Paf2 736.909851 Paf3 729.222534 Heatmap 238.418243 Total loss 2482.80176\n",
      "Epoch 298 Step 4920/5304 Paf1 634.115356 Paf2 591.800415 Paf3 579.487366 Heatmap 187.049561 Total loss 1992.45264\n",
      "Epoch 298 Step 4930/5304 Paf1 837.908813 Paf2 782.969666 Paf3 781.916138 Heatmap 255.279037 Total loss 2658.07373\n",
      "Epoch 298 Step 4940/5304 Paf1 623.985107 Paf2 576.508606 Paf3 579.941895 Heatmap 170.36618 Total loss 1950.80176\n",
      "Epoch 298 Step 4950/5304 Paf1 642.786255 Paf2 619.749207 Paf3 599.098 Heatmap 155.368698 Total loss 2017.0022\n",
      "Epoch 298 Step 4960/5304 Paf1 893.312927 Paf2 866.803223 Paf3 869.080444 Heatmap 303.016113 Total loss 2932.21289\n",
      "Epoch 298 Step 4970/5304 Paf1 771.82489 Paf2 752.003723 Paf3 746.014526 Heatmap 256.887817 Total loss 2526.73096\n",
      "Epoch 298 Step 4980/5304 Paf1 451.923157 Paf2 395.606171 Paf3 389.211731 Heatmap 127.837288 Total loss 1364.57837\n",
      "Epoch 298 Step 4990/5304 Paf1 670.639343 Paf2 612.552795 Paf3 616.109497 Heatmap 186.240341 Total loss 2085.54199\n",
      "Epoch 298 Step 5000/5304 Paf1 445.494751 Paf2 412.87561 Paf3 400.145264 Heatmap 101.878265 Total loss 1360.39392\n",
      "Saved checkpoint for step 5000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1787\n",
      "Epoch 298 Step 5010/5304 Paf1 593.949402 Paf2 583.786682 Paf3 577.409058 Heatmap 197.203522 Total loss 1952.34863\n",
      "Epoch 298 Step 5020/5304 Paf1 702.584 Paf2 676.033813 Paf3 682.88916 Heatmap 230.873032 Total loss 2292.38\n",
      "Epoch 298 Step 5030/5304 Paf1 797.965515 Paf2 729.138062 Paf3 719.369263 Heatmap 228.031586 Total loss 2474.50439\n",
      "Epoch 298 Step 5040/5304 Paf1 705.748047 Paf2 658.974915 Paf3 645.172852 Heatmap 178.864014 Total loss 2188.75977\n",
      "Epoch 298 Step 5050/5304 Paf1 838.511353 Paf2 823.044495 Paf3 805.474792 Heatmap 289.537598 Total loss 2756.56836\n",
      "Epoch 298 Step 5060/5304 Paf1 571.618469 Paf2 522.017578 Paf3 528.59021 Heatmap 179.970627 Total loss 1802.19678\n",
      "Epoch 298 Step 5070/5304 Paf1 875.883911 Paf2 836.307 Paf3 829.947327 Heatmap 278.760406 Total loss 2820.89868\n",
      "Epoch 298 Step 5080/5304 Paf1 969.780457 Paf2 931.620911 Paf3 924.918762 Heatmap 320.195892 Total loss 3146.51611\n",
      "Epoch 298 Step 5090/5304 Paf1 897.961853 Paf2 854.199829 Paf3 837.039124 Heatmap 261.784485 Total loss 2850.98535\n",
      "Epoch 298 Step 5100/5304 Paf1 881.269592 Paf2 830.815857 Paf3 831.733582 Heatmap 260.383423 Total loss 2804.20239\n",
      "Epoch 298 Step 5110/5304 Paf1 674.082092 Paf2 635.782288 Paf3 634.101135 Heatmap 201.867798 Total loss 2145.83325\n",
      "Epoch 298 Step 5120/5304 Paf1 703.92627 Paf2 646.989 Paf3 624.77124 Heatmap 218.61084 Total loss 2194.29736\n",
      "Epoch 298 Step 5130/5304 Paf1 864.666809 Paf2 834.332703 Paf3 843.855896 Heatmap 281.803955 Total loss 2824.65942\n",
      "Epoch 298 Step 5140/5304 Paf1 535.978821 Paf2 485.956665 Paf3 488.364349 Heatmap 152.496658 Total loss 1662.79651\n",
      "Epoch 298 Step 5150/5304 Paf1 482.91864 Paf2 446.35556 Paf3 429.957947 Heatmap 125.995674 Total loss 1485.22778\n",
      "Epoch 298 Step 5160/5304 Paf1 730.727417 Paf2 682.597534 Paf3 676.313232 Heatmap 227.139603 Total loss 2316.77783\n",
      "Epoch 298 Step 5170/5304 Paf1 459.398102 Paf2 421.04303 Paf3 414.784271 Heatmap 150.913544 Total loss 1446.13892\n",
      "Epoch 298 Step 5180/5304 Paf1 711.759521 Paf2 660.812256 Paf3 691.473145 Heatmap 252.998077 Total loss 2317.04297\n",
      "Epoch 298 Step 5190/5304 Paf1 667.603271 Paf2 620.035034 Paf3 612.211609 Heatmap 195.168488 Total loss 2095.01855\n",
      "Epoch 298 Step 5200/5304 Paf1 1139.30151 Paf2 1084.37378 Paf3 1072.47437 Heatmap 392.623962 Total loss 3688.77368\n",
      "Epoch 298 Step 5210/5304 Paf1 841.811035 Paf2 798.44696 Paf3 809.671631 Heatmap 259.160431 Total loss 2709.09\n",
      "Epoch 298 Step 5220/5304 Paf1 752.303223 Paf2 689.358032 Paf3 693.346802 Heatmap 247.846268 Total loss 2382.85425\n",
      "Epoch 298 Step 5230/5304 Paf1 607.970398 Paf2 583.859558 Paf3 585.416 Heatmap 223.056274 Total loss 2000.30225\n",
      "Epoch 298 Step 5240/5304 Paf1 854.828491 Paf2 807.984619 Paf3 790.018555 Heatmap 313.01828 Total loss 2765.85\n",
      "Epoch 298 Step 5250/5304 Paf1 778.263733 Paf2 743.972778 Paf3 744.177734 Heatmap 225.780396 Total loss 2492.19482\n",
      "Epoch 298 Step 5260/5304 Paf1 808.997498 Paf2 764.940735 Paf3 763.430298 Heatmap 265.416809 Total loss 2602.7854\n",
      "Epoch 298 Step 5270/5304 Paf1 595.885803 Paf2 544.441833 Paf3 542.563232 Heatmap 187.434097 Total loss 1870.32495\n",
      "Epoch 298 Step 5280/5304 Paf1 461.276306 Paf2 420.345642 Paf3 422.164459 Heatmap 142.972092 Total loss 1446.75854\n",
      "Epoch 298 Step 5290/5304 Paf1 1026.53992 Paf2 962.477112 Paf3 976.679443 Heatmap 319.099365 Total loss 3284.7959\n",
      "Epoch 298 Step 5300/5304 Paf1 570.825256 Paf2 545.755493 Paf3 557.118164 Heatmap 172.786545 Total loss 1846.4856\n",
      "Completed epoch 298. Saving weights...\n",
      "Epoch training time: 0:15:51.160696\n",
      "Calculating validation losses...\n",
      "Validation step 0 ...\n",
      "Validation losses for epoch: 298 : Loss paf 680.2965087890625, Loss heatmap 226.671142578125, Total loss 2308.968017578125\n",
      "Start processing epoch 299\n",
      "Epoch 299 Step 10/5304 Paf1 598.287781 Paf2 542.864136 Paf3 542.672668 Heatmap 206.661606 Total loss 1890.48608\n",
      "Epoch 299 Step 20/5304 Paf1 568.561707 Paf2 529.310303 Paf3 526.679321 Heatmap 200.142136 Total loss 1824.6936\n",
      "Epoch 299 Step 30/5304 Paf1 816.142273 Paf2 774.410889 Paf3 762.931763 Heatmap 266.869934 Total loss 2620.35498\n",
      "Epoch 299 Step 40/5304 Paf1 708.793396 Paf2 675.577 Paf3 677.812805 Heatmap 238.976929 Total loss 2301.16016\n",
      "Epoch 299 Step 50/5304 Paf1 673.011719 Paf2 621.722168 Paf3 606.800903 Heatmap 164.554962 Total loss 2066.08984\n",
      "Epoch 299 Step 60/5304 Paf1 407.283234 Paf2 371.914612 Paf3 369.451385 Heatmap 109.573944 Total loss 1258.22314\n",
      "Epoch 299 Step 70/5304 Paf1 616.222168 Paf2 574.439 Paf3 585.112305 Heatmap 177.396408 Total loss 1953.16992\n",
      "Epoch 299 Step 80/5304 Paf1 809.207458 Paf2 755.680603 Paf3 766.132 Heatmap 256.680023 Total loss 2587.7002\n",
      "Epoch 299 Step 90/5304 Paf1 851.433411 Paf2 799.81189 Paf3 815.644226 Heatmap 280.943634 Total loss 2747.83325\n",
      "Epoch 299 Step 100/5304 Paf1 586.197 Paf2 546.373169 Paf3 555.983643 Heatmap 198.426804 Total loss 1886.98071\n",
      "Epoch 299 Step 110/5304 Paf1 693.101746 Paf2 649.965515 Paf3 635.313599 Heatmap 216.434158 Total loss 2194.81494\n",
      "Epoch 299 Step 120/5304 Paf1 485.645477 Paf2 460.134796 Paf3 466.763 Heatmap 146.290085 Total loss 1558.83337\n",
      "Epoch 299 Step 130/5304 Paf1 532.882385 Paf2 519.839478 Paf3 528.802795 Heatmap 188.483459 Total loss 1770.00818\n",
      "Epoch 299 Step 140/5304 Paf1 682.252686 Paf2 634.186462 Paf3 635.733 Heatmap 196.155151 Total loss 2148.32739\n",
      "Epoch 299 Step 150/5304 Paf1 755.624695 Paf2 721.632263 Paf3 711.204407 Heatmap 232.411346 Total loss 2420.87256\n",
      "Epoch 299 Step 160/5304 Paf1 782.725159 Paf2 734.209595 Paf3 746.699036 Heatmap 217.868469 Total loss 2481.50244\n",
      "Epoch 299 Step 170/5304 Paf1 875.181396 Paf2 834.380798 Paf3 823.64447 Heatmap 253.999664 Total loss 2787.20654\n",
      "Epoch 299 Step 180/5304 Paf1 652.819 Paf2 623.641113 Paf3 655.808655 Heatmap 201.24736 Total loss 2133.51611\n",
      "Epoch 299 Step 190/5304 Paf1 541.661804 Paf2 512.926636 Paf3 522.959229 Heatmap 148.430801 Total loss 1725.97839\n",
      "Epoch 299 Step 200/5304 Paf1 649.253174 Paf2 599.469666 Paf3 591.251953 Heatmap 166.780457 Total loss 2006.75537\n",
      "Epoch 299 Step 210/5304 Paf1 668.962769 Paf2 595.551331 Paf3 617.453613 Heatmap 210.24411 Total loss 2092.21191\n",
      "Epoch 299 Step 220/5304 Paf1 1001.89758 Paf2 951.230347 Paf3 961.222778 Heatmap 329.805 Total loss 3244.15576\n",
      "Epoch 299 Step 230/5304 Paf1 527.638489 Paf2 488.260529 Paf3 477.211731 Heatmap 149.53653 Total loss 1642.64734\n",
      "Epoch 299 Step 240/5304 Paf1 710.30835 Paf2 678.637512 Paf3 666.958252 Heatmap 218.859634 Total loss 2274.76367\n",
      "Epoch 299 Step 250/5304 Paf1 574.944275 Paf2 548.694824 Paf3 566.356567 Heatmap 184.487 Total loss 1874.48267\n",
      "Epoch 299 Step 260/5304 Paf1 1050.92664 Paf2 995.148 Paf3 986.274963 Heatmap 373.140076 Total loss 3405.48975\n",
      "Epoch 299 Step 270/5304 Paf1 801.376587 Paf2 739.123413 Paf3 741.172 Heatmap 238.884201 Total loss 2520.55615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 Step 280/5304 Paf1 658.403076 Paf2 642.158325 Paf3 628.910645 Heatmap 218.329132 Total loss 2147.80127\n",
      "Epoch 299 Step 290/5304 Paf1 903.866394 Paf2 885.427612 Paf3 866.566895 Heatmap 282.673035 Total loss 2938.53394\n",
      "Epoch 299 Step 300/5304 Paf1 700.578796 Paf2 687.820312 Paf3 698.971558 Heatmap 213.167694 Total loss 2300.53857\n",
      "Epoch 299 Step 310/5304 Paf1 619.93573 Paf2 576.348816 Paf3 574.646545 Heatmap 175.39859 Total loss 1946.32971\n",
      "Epoch 299 Step 320/5304 Paf1 639.771667 Paf2 594.207886 Paf3 587.078735 Heatmap 210.312866 Total loss 2031.37109\n",
      "Epoch 299 Step 330/5304 Paf1 645.196594 Paf2 601.281616 Paf3 617.603638 Heatmap 201.904968 Total loss 2065.98682\n",
      "Epoch 299 Step 340/5304 Paf1 665.035828 Paf2 612.779358 Paf3 620.429565 Heatmap 201.74379 Total loss 2099.98853\n",
      "Epoch 299 Step 350/5304 Paf1 535.153 Paf2 515.674683 Paf3 493.801575 Heatmap 175.977844 Total loss 1720.60706\n",
      "Epoch 299 Step 360/5304 Paf1 691.3349 Paf2 661.865 Paf3 648.753662 Heatmap 210.786957 Total loss 2212.74072\n",
      "Epoch 299 Step 370/5304 Paf1 705.050598 Paf2 699.798157 Paf3 673.755371 Heatmap 259.460327 Total loss 2338.06445\n",
      "Epoch 299 Step 380/5304 Paf1 818.826 Paf2 756.025696 Paf3 773.374695 Heatmap 239.718704 Total loss 2587.94507\n",
      "Epoch 299 Step 390/5304 Paf1 846.965271 Paf2 790.172485 Paf3 798.132385 Heatmap 271.145142 Total loss 2706.41528\n",
      "Epoch 299 Step 400/5304 Paf1 398.062469 Paf2 375.561493 Paf3 366.754456 Heatmap 125.207657 Total loss 1265.58606\n",
      "Epoch 299 Step 410/5304 Paf1 630.409912 Paf2 575.652832 Paf3 559.471802 Heatmap 217.340561 Total loss 1982.87512\n",
      "Epoch 299 Step 420/5304 Paf1 742.57666 Paf2 668.38916 Paf3 690.829407 Heatmap 242.626862 Total loss 2344.42212\n",
      "Epoch 299 Step 430/5304 Paf1 927.327515 Paf2 894.688416 Paf3 887.778931 Heatmap 288.978851 Total loss 2998.77368\n",
      "Epoch 299 Step 440/5304 Paf1 488.082031 Paf2 488.161163 Paf3 471.778381 Heatmap 156.08783 Total loss 1604.10938\n",
      "Epoch 299 Step 450/5304 Paf1 779.630493 Paf2 723.061462 Paf3 722.293518 Heatmap 224.87352 Total loss 2449.85889\n",
      "Epoch 299 Step 460/5304 Paf1 711.04071 Paf2 674.906677 Paf3 674.170898 Heatmap 245.156769 Total loss 2305.2749\n",
      "Epoch 299 Step 470/5304 Paf1 924.863098 Paf2 866.297058 Paf3 860.122314 Heatmap 272.73645 Total loss 2924.01904\n",
      "Epoch 299 Step 480/5304 Paf1 796.764282 Paf2 736.809692 Paf3 735.486145 Heatmap 251.440598 Total loss 2520.50073\n",
      "Epoch 299 Step 490/5304 Paf1 650.548706 Paf2 590.002747 Paf3 606.793335 Heatmap 202.430527 Total loss 2049.77539\n",
      "Epoch 299 Step 500/5304 Paf1 745.616089 Paf2 717.733521 Paf3 710.200745 Heatmap 253.99292 Total loss 2427.54321\n",
      "Epoch 299 Step 510/5304 Paf1 913.365906 Paf2 839.042053 Paf3 849.297913 Heatmap 304.475586 Total loss 2906.1814\n",
      "Epoch 299 Step 520/5304 Paf1 924.078308 Paf2 889.360962 Paf3 885.042 Heatmap 336.946289 Total loss 3035.42749\n",
      "Epoch 299 Step 530/5304 Paf1 704.783875 Paf2 663.801 Paf3 665.166748 Heatmap 194.14502 Total loss 2227.89673\n",
      "Epoch 299 Step 540/5304 Paf1 524.315063 Paf2 475.619476 Paf3 475.050354 Heatmap 155.722 Total loss 1630.70691\n",
      "Epoch 299 Step 550/5304 Paf1 557.075745 Paf2 512.234863 Paf3 517.957886 Heatmap 167.489655 Total loss 1754.75806\n",
      "Epoch 299 Step 560/5304 Paf1 581.935059 Paf2 536.274231 Paf3 528.070862 Heatmap 150.125488 Total loss 1796.40552\n",
      "Epoch 299 Step 570/5304 Paf1 656.318604 Paf2 605.184753 Paf3 607.924561 Heatmap 183.616135 Total loss 2053.04419\n",
      "Epoch 299 Step 580/5304 Paf1 548.26532 Paf2 541.537659 Paf3 536.769348 Heatmap 187.341675 Total loss 1813.91406\n",
      "Epoch 299 Step 590/5304 Paf1 588.386597 Paf2 557.780273 Paf3 545.135376 Heatmap 173.854095 Total loss 1865.15637\n",
      "Epoch 299 Step 600/5304 Paf1 496.559265 Paf2 462.601562 Paf3 451.276825 Heatmap 158.808929 Total loss 1569.24658\n",
      "Epoch 299 Step 610/5304 Paf1 593.989319 Paf2 555.374207 Paf3 558.053 Heatmap 169.814117 Total loss 1877.23059\n",
      "Epoch 299 Step 620/5304 Paf1 441.147644 Paf2 399.57663 Paf3 399.100464 Heatmap 104.503601 Total loss 1344.32837\n",
      "Epoch 299 Step 630/5304 Paf1 549.372925 Paf2 503.927582 Paf3 528.010681 Heatmap 187.868561 Total loss 1769.17981\n",
      "Epoch 299 Step 640/5304 Paf1 418.008026 Paf2 385.055634 Paf3 387.279388 Heatmap 107.257988 Total loss 1297.60107\n",
      "Epoch 299 Step 650/5304 Paf1 630.785828 Paf2 587.737854 Paf3 594.950867 Heatmap 195.139 Total loss 2008.61353\n",
      "Epoch 299 Step 660/5304 Paf1 726.861694 Paf2 674.096436 Paf3 697.759949 Heatmap 234.329559 Total loss 2333.04761\n",
      "Epoch 299 Step 670/5304 Paf1 909.026672 Paf2 858.581787 Paf3 861.32843 Heatmap 278.725128 Total loss 2907.66211\n",
      "Epoch 299 Step 680/5304 Paf1 686.391174 Paf2 654.478271 Paf3 636.417664 Heatmap 233.315216 Total loss 2210.60229\n",
      "Epoch 299 Step 690/5304 Paf1 546.976074 Paf2 500.932312 Paf3 514.168762 Heatmap 196.048828 Total loss 1758.12598\n",
      "Epoch 299 Step 700/5304 Paf1 668.868164 Paf2 610.012207 Paf3 618.601929 Heatmap 184.334 Total loss 2081.81641\n",
      "Epoch 299 Step 710/5304 Paf1 473.223053 Paf2 444.763092 Paf3 451.674896 Heatmap 114.264557 Total loss 1483.92554\n",
      "Epoch 299 Step 720/5304 Paf1 809.652649 Paf2 743.779236 Paf3 750.042969 Heatmap 260.247925 Total loss 2563.72266\n",
      "Epoch 299 Step 730/5304 Paf1 1128.82288 Paf2 1052.31689 Paf3 1065.36426 Heatmap 332.798309 Total loss 3579.30225\n",
      "Epoch 299 Step 740/5304 Paf1 765.306335 Paf2 720.156372 Paf3 710.753052 Heatmap 181.176041 Total loss 2377.3916\n",
      "Epoch 299 Step 750/5304 Paf1 740.304932 Paf2 708.057312 Paf3 686.708557 Heatmap 262.316284 Total loss 2397.38721\n",
      "Epoch 299 Step 760/5304 Paf1 864.174377 Paf2 812.542175 Paf3 810.035339 Heatmap 296.632751 Total loss 2783.38477\n",
      "Epoch 299 Step 770/5304 Paf1 697.558594 Paf2 645.806519 Paf3 636.662292 Heatmap 183.511108 Total loss 2163.53857\n",
      "Epoch 299 Step 780/5304 Paf1 972.858032 Paf2 906.981689 Paf3 915.779 Heatmap 303.368439 Total loss 3098.9873\n",
      "Epoch 299 Step 790/5304 Paf1 768.360474 Paf2 704.392456 Paf3 688.637 Heatmap 230.69754 Total loss 2392.0874\n",
      "Epoch 299 Step 800/5304 Paf1 578.590698 Paf2 511.565918 Paf3 516.283264 Heatmap 165.547455 Total loss 1771.9873\n",
      "Epoch 299 Step 810/5304 Paf1 670.810913 Paf2 629.258667 Paf3 625.125854 Heatmap 191.70459 Total loss 2116.9\n",
      "Epoch 299 Step 820/5304 Paf1 635.732 Paf2 596.771118 Paf3 592.936523 Heatmap 212.90213 Total loss 2038.3418\n",
      "Epoch 299 Step 830/5304 Paf1 647.159546 Paf2 610.420227 Paf3 615.436646 Heatmap 180.825165 Total loss 2053.8418\n",
      "Epoch 299 Step 840/5304 Paf1 793.865 Paf2 742.950439 Paf3 732.088562 Heatmap 240.12468 Total loss 2509.02881\n",
      "Epoch 299 Step 850/5304 Paf1 893.053284 Paf2 828.765137 Paf3 864.059937 Heatmap 293.095642 Total loss 2878.97388\n",
      "Epoch 299 Step 860/5304 Paf1 684.868591 Paf2 629.483521 Paf3 643.767334 Heatmap 235.612991 Total loss 2193.73242\n",
      "Epoch 299 Step 870/5304 Paf1 702.721802 Paf2 659.360352 Paf3 652.469 Heatmap 226.116394 Total loss 2240.66748\n",
      "Epoch 299 Step 880/5304 Paf1 633.382751 Paf2 579.374329 Paf3 577.863342 Heatmap 194.423584 Total loss 1985.04395\n",
      "Epoch 299 Step 890/5304 Paf1 746.570435 Paf2 717.750549 Paf3 703.093079 Heatmap 268.348083 Total loss 2435.76221\n",
      "Epoch 299 Step 900/5304 Paf1 742.740906 Paf2 710.787 Paf3 713.912415 Heatmap 239.889877 Total loss 2407.33\n",
      "Epoch 299 Step 910/5304 Paf1 837.3396 Paf2 780.857422 Paf3 781.174377 Heatmap 303.954651 Total loss 2703.32617\n",
      "Epoch 299 Step 920/5304 Paf1 710.16626 Paf2 668.137268 Paf3 669.0224 Heatmap 204.72644 Total loss 2252.05225\n",
      "Epoch 299 Step 930/5304 Paf1 784.535767 Paf2 753.194702 Paf3 759.006897 Heatmap 253.142395 Total loss 2549.88\n",
      "Epoch 299 Step 940/5304 Paf1 665.69635 Paf2 635.639648 Paf3 615.377197 Heatmap 182.864944 Total loss 2099.57812\n",
      "Epoch 299 Step 950/5304 Paf1 454.63327 Paf2 415.820099 Paf3 421.686127 Heatmap 131.35437 Total loss 1423.4939\n",
      "Epoch 299 Step 960/5304 Paf1 629.86615 Paf2 598.250488 Paf3 611.485962 Heatmap 179.195251 Total loss 2018.79785\n",
      "Epoch 299 Step 970/5304 Paf1 667.102661 Paf2 625.832581 Paf3 610.970825 Heatmap 223.83725 Total loss 2127.74341\n",
      "Epoch 299 Step 980/5304 Paf1 708.437317 Paf2 647.860046 Paf3 645.615295 Heatmap 211.955841 Total loss 2213.86865\n",
      "Epoch 299 Step 990/5304 Paf1 766.892578 Paf2 721.828125 Paf3 720.357666 Heatmap 198.190033 Total loss 2407.26855\n",
      "Epoch 299 Step 1000/5304 Paf1 853.019165 Paf2 789.729797 Paf3 778.059875 Heatmap 280.473053 Total loss 2701.28198\n",
      "Saved checkpoint for step 1000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 Step 1010/5304 Paf1 646.646667 Paf2 613.635498 Paf3 605.095581 Heatmap 220.666504 Total loss 2086.04443\n",
      "Epoch 299 Step 1020/5304 Paf1 921.326 Paf2 858.361877 Paf3 857.325684 Heatmap 278.814362 Total loss 2915.82788\n",
      "Epoch 299 Step 1030/5304 Paf1 701.150513 Paf2 697.574219 Paf3 676.062683 Heatmap 208.744904 Total loss 2283.53223\n",
      "Epoch 299 Step 1040/5304 Paf1 679.081116 Paf2 655.044067 Paf3 646.454895 Heatmap 221.854065 Total loss 2202.43408\n",
      "Epoch 299 Step 1050/5304 Paf1 717.973816 Paf2 684.361389 Paf3 684.418457 Heatmap 226.744019 Total loss 2313.49756\n",
      "Epoch 299 Step 1060/5304 Paf1 581.815552 Paf2 510.512482 Paf3 535.175781 Heatmap 148.69371 Total loss 1776.19751\n",
      "Epoch 299 Step 1070/5304 Paf1 844.819458 Paf2 783.727844 Paf3 788.582764 Heatmap 238.949524 Total loss 2656.07959\n",
      "Epoch 299 Step 1080/5304 Paf1 759.421448 Paf2 705.657593 Paf3 705.364624 Heatmap 232.544891 Total loss 2402.98877\n",
      "Epoch 299 Step 1090/5304 Paf1 501.704163 Paf2 475.448151 Paf3 466.031555 Heatmap 184.397125 Total loss 1627.58105\n",
      "Epoch 299 Step 1100/5304 Paf1 831.412964 Paf2 809.402832 Paf3 798.172607 Heatmap 256.066956 Total loss 2695.05518\n",
      "Epoch 299 Step 1110/5304 Paf1 473.506073 Paf2 450.096313 Paf3 463.107605 Heatmap 134.694214 Total loss 1521.4043\n",
      "Epoch 299 Step 1120/5304 Paf1 686.791626 Paf2 650.007 Paf3 640.181763 Heatmap 208.397491 Total loss 2185.37793\n",
      "Epoch 299 Step 1130/5304 Paf1 479.305145 Paf2 452.828735 Paf3 460.344635 Heatmap 138.817322 Total loss 1531.2959\n",
      "Epoch 299 Step 1140/5304 Paf1 777.098 Paf2 746.406677 Paf3 761.354248 Heatmap 232.29007 Total loss 2517.14893\n",
      "Epoch 299 Step 1150/5304 Paf1 706.311768 Paf2 668.457 Paf3 693.863098 Heatmap 227.942688 Total loss 2296.57471\n",
      "Epoch 299 Step 1160/5304 Paf1 575.714539 Paf2 527.166748 Paf3 538.210632 Heatmap 158.206375 Total loss 1799.29834\n",
      "Epoch 299 Step 1170/5304 Paf1 1067.58008 Paf2 1018.45471 Paf3 1032.33948 Heatmap 355.521515 Total loss 3473.89551\n",
      "Epoch 299 Step 1180/5304 Paf1 903.618896 Paf2 858.949341 Paf3 854.474731 Heatmap 310.384857 Total loss 2927.42773\n",
      "Epoch 299 Step 1190/5304 Paf1 784.449707 Paf2 762.177246 Paf3 757.395142 Heatmap 236.077759 Total loss 2540.09985\n",
      "Epoch 299 Step 1200/5304 Paf1 597.046387 Paf2 527.603577 Paf3 547.827637 Heatmap 183.297653 Total loss 1855.77515\n",
      "Epoch 299 Step 1210/5304 Paf1 686.684631 Paf2 617.4375 Paf3 628.164246 Heatmap 216.440491 Total loss 2148.72681\n",
      "Epoch 299 Step 1220/5304 Paf1 845.005127 Paf2 826.593506 Paf3 837.749939 Heatmap 295.308777 Total loss 2804.65723\n",
      "Epoch 299 Step 1230/5304 Paf1 876.615417 Paf2 811.431946 Paf3 845.443 Heatmap 241.416458 Total loss 2774.90674\n",
      "Epoch 299 Step 1240/5304 Paf1 970.196899 Paf2 931.193665 Paf3 924.803589 Heatmap 278.572327 Total loss 3104.7666\n",
      "Epoch 299 Step 1250/5304 Paf1 578.639526 Paf2 511.495697 Paf3 516.811646 Heatmap 135.567566 Total loss 1742.5144\n",
      "Epoch 299 Step 1260/5304 Paf1 533.485046 Paf2 510.715759 Paf3 505.661133 Heatmap 152.227051 Total loss 1702.08899\n",
      "Epoch 299 Step 1270/5304 Paf1 868.536133 Paf2 792.37 Paf3 793.325439 Heatmap 241.817291 Total loss 2696.04883\n",
      "Epoch 299 Step 1280/5304 Paf1 688.638306 Paf2 633.018799 Paf3 636.735535 Heatmap 203.785919 Total loss 2162.17871\n",
      "Epoch 299 Step 1290/5304 Paf1 677.570618 Paf2 634.02063 Paf3 630.160278 Heatmap 230.495117 Total loss 2172.24658\n",
      "Epoch 299 Step 1300/5304 Paf1 1032.07556 Paf2 988.307556 Paf3 1002.74811 Heatmap 353.297 Total loss 3376.42822\n",
      "Epoch 299 Step 1310/5304 Paf1 976.440186 Paf2 902.766418 Paf3 934.110352 Heatmap 323.052551 Total loss 3136.36938\n",
      "Epoch 299 Step 1320/5304 Paf1 362.032623 Paf2 327.94043 Paf3 328.135925 Heatmap 108.146637 Total loss 1126.25562\n",
      "Epoch 299 Step 1330/5304 Paf1 673.009827 Paf2 620.741394 Paf3 629.20752 Heatmap 198.51474 Total loss 2121.47363\n",
      "Epoch 299 Step 1340/5304 Paf1 796.029175 Paf2 725.593201 Paf3 730.205566 Heatmap 225.348206 Total loss 2477.17603\n",
      "Epoch 299 Step 1350/5304 Paf1 393.318542 Paf2 360.629791 Paf3 349.952911 Heatmap 110.141769 Total loss 1214.04297\n",
      "Epoch 299 Step 1360/5304 Paf1 685.083 Paf2 652.754761 Paf3 639.838196 Heatmap 213.297028 Total loss 2190.97314\n",
      "Epoch 299 Step 1370/5304 Paf1 450.945404 Paf2 410.263123 Paf3 423.860565 Heatmap 126.164665 Total loss 1411.23364\n",
      "Epoch 299 Step 1380/5304 Paf1 793.821167 Paf2 757.104065 Paf3 744.640198 Heatmap 288.501831 Total loss 2584.06738\n",
      "Epoch 299 Step 1390/5304 Paf1 621.762207 Paf2 559.712341 Paf3 570.792847 Heatmap 176.077591 Total loss 1928.34497\n",
      "Epoch 299 Step 1400/5304 Paf1 812.222351 Paf2 767.960938 Paf3 763.035645 Heatmap 272.089417 Total loss 2615.30835\n",
      "Epoch 299 Step 1410/5304 Paf1 1076.27759 Paf2 1010.94885 Paf3 1023.00458 Heatmap 342.877655 Total loss 3453.10889\n",
      "Epoch 299 Step 1420/5304 Paf1 934.158752 Paf2 893.226929 Paf3 883.928162 Heatmap 288.297241 Total loss 2999.61108\n",
      "Epoch 299 Step 1430/5304 Paf1 786.214966 Paf2 746.092651 Paf3 752.66217 Heatmap 278.359344 Total loss 2563.3291\n",
      "Epoch 299 Step 1440/5304 Paf1 559.647827 Paf2 521.035828 Paf3 508.388428 Heatmap 145.369446 Total loss 1734.44141\n",
      "Epoch 299 Step 1450/5304 Paf1 936.582581 Paf2 849.516785 Paf3 834.331421 Heatmap 260.838898 Total loss 2881.26953\n",
      "Epoch 299 Step 1460/5304 Paf1 554.94989 Paf2 530.464 Paf3 514.098511 Heatmap 174.191895 Total loss 1773.70422\n",
      "Epoch 299 Step 1470/5304 Paf1 943.178955 Paf2 894.899902 Paf3 875.142395 Heatmap 252.585846 Total loss 2965.80713\n",
      "Epoch 299 Step 1480/5304 Paf1 1077.53418 Paf2 1043.72363 Paf3 1029.3584 Heatmap 342.797 Total loss 3493.41309\n",
      "Epoch 299 Step 1490/5304 Paf1 542.23468 Paf2 506.71756 Paf3 516.551636 Heatmap 196.94165 Total loss 1762.44556\n",
      "Epoch 299 Step 1500/5304 Paf1 553.190918 Paf2 515.973083 Paf3 530.497192 Heatmap 189.287064 Total loss 1788.94824\n",
      "Epoch 299 Step 1510/5304 Paf1 612.834167 Paf2 549.033936 Paf3 543.515 Heatmap 183.784866 Total loss 1889.16797\n",
      "Epoch 299 Step 1520/5304 Paf1 429.791626 Paf2 403.982635 Paf3 406.308289 Heatmap 110.797203 Total loss 1350.87976\n",
      "Epoch 299 Step 1530/5304 Paf1 818.488647 Paf2 727.923889 Paf3 729.613159 Heatmap 257.489166 Total loss 2533.51489\n",
      "Epoch 299 Step 1540/5304 Paf1 531.727051 Paf2 499.875458 Paf3 496.316193 Heatmap 162.292831 Total loss 1690.21155\n",
      "Epoch 299 Step 1550/5304 Paf1 591.420349 Paf2 557.883057 Paf3 556.008545 Heatmap 197.368317 Total loss 1902.6803\n",
      "Epoch 299 Step 1560/5304 Paf1 501.263489 Paf2 459.416016 Paf3 470.607544 Heatmap 135.68576 Total loss 1566.9729\n",
      "Epoch 299 Step 1570/5304 Paf1 686.704346 Paf2 651.436035 Paf3 659.626221 Heatmap 218.542236 Total loss 2216.30884\n",
      "Epoch 299 Step 1580/5304 Paf1 771.428101 Paf2 726.510437 Paf3 746.438538 Heatmap 279.743317 Total loss 2524.12036\n",
      "Epoch 299 Step 1590/5304 Paf1 654.846802 Paf2 580.975159 Paf3 578.503906 Heatmap 189.638763 Total loss 2003.96472\n",
      "Epoch 299 Step 1600/5304 Paf1 682.206787 Paf2 634.968323 Paf3 624.626831 Heatmap 243.979584 Total loss 2185.78149\n",
      "Epoch 299 Step 1610/5304 Paf1 582.370728 Paf2 573.002075 Paf3 561.893311 Heatmap 195.217133 Total loss 1912.48328\n",
      "Epoch 299 Step 1620/5304 Paf1 495.43808 Paf2 478.270264 Paf3 465.104706 Heatmap 161.470886 Total loss 1600.28394\n",
      "Epoch 299 Step 1630/5304 Paf1 691.385681 Paf2 665.928711 Paf3 642.969177 Heatmap 221.926727 Total loss 2222.21045\n",
      "Epoch 299 Step 1640/5304 Paf1 677.061035 Paf2 626.001587 Paf3 639.076416 Heatmap 211.41214 Total loss 2153.55127\n",
      "Epoch 299 Step 1650/5304 Paf1 622.062866 Paf2 562.697266 Paf3 582.792847 Heatmap 188.363052 Total loss 1955.91602\n",
      "Epoch 299 Step 1660/5304 Paf1 731.530884 Paf2 720.611145 Paf3 701.319 Heatmap 222.199768 Total loss 2375.66089\n",
      "Epoch 299 Step 1670/5304 Paf1 411.785889 Paf2 402.745056 Paf3 388.953461 Heatmap 116.973198 Total loss 1320.45764\n",
      "Epoch 299 Step 1680/5304 Paf1 596.257385 Paf2 549.668579 Paf3 554.433472 Heatmap 202.817413 Total loss 1903.17688\n",
      "Epoch 299 Step 1690/5304 Paf1 621.141357 Paf2 556.737488 Paf3 556.414856 Heatmap 161.444901 Total loss 1895.73865\n",
      "Epoch 299 Step 1700/5304 Paf1 723.384155 Paf2 697.674194 Paf3 692.366638 Heatmap 222.49292 Total loss 2335.91797\n",
      "Epoch 299 Step 1710/5304 Paf1 756.772278 Paf2 714.920593 Paf3 712.594 Heatmap 210.009674 Total loss 2394.29639\n",
      "Epoch 299 Step 1720/5304 Paf1 1018.99121 Paf2 957.659668 Paf3 947.281921 Heatmap 316.217285 Total loss 3240.15015\n",
      "Epoch 299 Step 1730/5304 Paf1 818.787476 Paf2 788.419189 Paf3 776.223389 Heatmap 236.549957 Total loss 2619.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 Step 1740/5304 Paf1 547.278931 Paf2 513.647766 Paf3 516.78186 Heatmap 165.684296 Total loss 1743.39294\n",
      "Epoch 299 Step 1750/5304 Paf1 653.60083 Paf2 629.96 Paf3 621.934326 Heatmap 188.299316 Total loss 2093.79443\n",
      "Epoch 299 Step 1760/5304 Paf1 533.822876 Paf2 508.824341 Paf3 494.6091 Heatmap 151.820068 Total loss 1689.07642\n",
      "Epoch 299 Step 1770/5304 Paf1 638.83 Paf2 578.660217 Paf3 576.067566 Heatmap 173.63855 Total loss 1967.19629\n",
      "Epoch 299 Step 1780/5304 Paf1 658.640381 Paf2 616.924072 Paf3 617.632 Heatmap 218.465851 Total loss 2111.66235\n",
      "Epoch 299 Step 1790/5304 Paf1 576.841919 Paf2 540.825195 Paf3 550.193848 Heatmap 165.943008 Total loss 1833.80396\n",
      "Epoch 299 Step 1800/5304 Paf1 613.805786 Paf2 556.863586 Paf3 556.632629 Heatmap 216.735382 Total loss 1944.03748\n",
      "Epoch 299 Step 1810/5304 Paf1 789.160645 Paf2 766.000305 Paf3 764.032 Heatmap 241.776276 Total loss 2560.96924\n",
      "Epoch 299 Step 1820/5304 Paf1 1009.31177 Paf2 955.191101 Paf3 950.443 Heatmap 353.124329 Total loss 3268.07031\n",
      "Epoch 299 Step 1830/5304 Paf1 944.210205 Paf2 910.148438 Paf3 876.116638 Heatmap 273.693237 Total loss 3004.16846\n",
      "Epoch 299 Step 1840/5304 Paf1 490.663422 Paf2 480.717072 Paf3 485.553 Heatmap 139.630554 Total loss 1596.56409\n",
      "Epoch 299 Step 1850/5304 Paf1 659.794739 Paf2 628.421 Paf3 627.121826 Heatmap 225.824554 Total loss 2141.16211\n",
      "Epoch 299 Step 1860/5304 Paf1 630.875244 Paf2 580.77356 Paf3 591.222534 Heatmap 193.467636 Total loss 1996.33899\n",
      "Epoch 299 Step 1870/5304 Paf1 562.240845 Paf2 523.649719 Paf3 533.028625 Heatmap 141.97818 Total loss 1760.89746\n",
      "Epoch 299 Step 1880/5304 Paf1 780.814514 Paf2 742.137146 Paf3 749.789 Heatmap 198.731018 Total loss 2471.47168\n",
      "Epoch 299 Step 1890/5304 Paf1 481.685425 Paf2 461.419403 Paf3 451.617767 Heatmap 152.530106 Total loss 1547.25269\n",
      "Epoch 299 Step 1900/5304 Paf1 376.258636 Paf2 362.425537 Paf3 349.389465 Heatmap 112.81076 Total loss 1200.8844\n",
      "Epoch 299 Step 1910/5304 Paf1 644.962769 Paf2 610.962769 Paf3 593.908142 Heatmap 187.300415 Total loss 2037.13403\n",
      "Epoch 299 Step 1920/5304 Paf1 761.303406 Paf2 701.986511 Paf3 706.298218 Heatmap 239.533905 Total loss 2409.12207\n",
      "Epoch 299 Step 1930/5304 Paf1 820.007874 Paf2 752.907288 Paf3 773.470886 Heatmap 265.093048 Total loss 2611.479\n",
      "Epoch 299 Step 1940/5304 Paf1 633.09137 Paf2 625.893494 Paf3 625.791504 Heatmap 202.902771 Total loss 2087.6792\n",
      "Epoch 299 Step 1950/5304 Paf1 484.527466 Paf2 457.630676 Paf3 468.578674 Heatmap 123.352028 Total loss 1534.08887\n",
      "Epoch 299 Step 1960/5304 Paf1 976.18457 Paf2 921.770691 Paf3 929.503784 Heatmap 365.082458 Total loss 3192.5415\n",
      "Epoch 299 Step 1970/5304 Paf1 670.094177 Paf2 630.34613 Paf3 616.931641 Heatmap 222.536087 Total loss 2139.90796\n",
      "Epoch 299 Step 1980/5304 Paf1 726.892456 Paf2 670.133301 Paf3 668.958618 Heatmap 227.062927 Total loss 2293.04736\n",
      "Epoch 299 Step 1990/5304 Paf1 685.496643 Paf2 624.890869 Paf3 611.860352 Heatmap 182.349823 Total loss 2104.59766\n",
      "Epoch 299 Step 2000/5304 Paf1 700.371704 Paf2 660.908569 Paf3 652.198 Heatmap 227.958496 Total loss 2241.43677\n",
      "Saved checkpoint for step 2000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1790\n",
      "Epoch 299 Step 2010/5304 Paf1 548.572 Paf2 517.381104 Paf3 522.900574 Heatmap 164.126511 Total loss 1752.98022\n",
      "Epoch 299 Step 2020/5304 Paf1 659.47168 Paf2 609.488 Paf3 609.520325 Heatmap 239.408676 Total loss 2117.88867\n",
      "Epoch 299 Step 2030/5304 Paf1 636.205139 Paf2 589.485291 Paf3 584.837585 Heatmap 214.969772 Total loss 2025.4978\n",
      "Epoch 299 Step 2040/5304 Paf1 782.165039 Paf2 732.1875 Paf3 728.881653 Heatmap 239.774445 Total loss 2483.00879\n",
      "Epoch 299 Step 2050/5304 Paf1 868.141907 Paf2 796.308838 Paf3 814.560425 Heatmap 241.109268 Total loss 2720.12036\n",
      "Epoch 299 Step 2060/5304 Paf1 553.627625 Paf2 496.27417 Paf3 499.11438 Heatmap 155.635147 Total loss 1704.65137\n",
      "Epoch 299 Step 2070/5304 Paf1 674.725586 Paf2 616.492676 Paf3 612.416199 Heatmap 215.226776 Total loss 2118.86133\n",
      "Epoch 299 Step 2080/5304 Paf1 636.670471 Paf2 625.639343 Paf3 629.986816 Heatmap 211.3479 Total loss 2103.64453\n",
      "Epoch 299 Step 2090/5304 Paf1 556.398193 Paf2 535.283875 Paf3 533.470764 Heatmap 171.098633 Total loss 1796.25146\n",
      "Epoch 299 Step 2100/5304 Paf1 530.543152 Paf2 498.829254 Paf3 492.143311 Heatmap 169.164337 Total loss 1690.68\n",
      "Epoch 299 Step 2110/5304 Paf1 844.676636 Paf2 792.661499 Paf3 777.667114 Heatmap 241.16748 Total loss 2656.17285\n",
      "Epoch 299 Step 2120/5304 Paf1 654.229797 Paf2 623.84137 Paf3 636.905 Heatmap 222.834732 Total loss 2137.81104\n",
      "Epoch 299 Step 2130/5304 Paf1 529.446594 Paf2 489.897736 Paf3 491.746033 Heatmap 156.126099 Total loss 1667.21655\n",
      "Epoch 299 Step 2140/5304 Paf1 476.249817 Paf2 440.869812 Paf3 454.704468 Heatmap 169.631744 Total loss 1541.45581\n",
      "Epoch 299 Step 2150/5304 Paf1 978.412842 Paf2 935.416748 Paf3 913.541565 Heatmap 307.322571 Total loss 3134.69385\n",
      "Epoch 299 Step 2160/5304 Paf1 892.998657 Paf2 832.799866 Paf3 831.897644 Heatmap 255.231598 Total loss 2812.92773\n",
      "Epoch 299 Step 2170/5304 Paf1 770.927795 Paf2 715.418518 Paf3 715.080627 Heatmap 236.13 Total loss 2437.55688\n",
      "Epoch 299 Step 2180/5304 Paf1 490.13327 Paf2 479.314209 Paf3 488.245483 Heatmap 142.169159 Total loss 1599.86218\n",
      "Epoch 299 Step 2190/5304 Paf1 707.9729 Paf2 647.750427 Paf3 644.696899 Heatmap 186.288116 Total loss 2186.7085\n",
      "Epoch 299 Step 2200/5304 Paf1 658.063843 Paf2 641.49353 Paf3 650.162231 Heatmap 197.431488 Total loss 2147.15112\n",
      "Epoch 299 Step 2210/5304 Paf1 538.63916 Paf2 481.215027 Paf3 487.860138 Heatmap 132.2146 Total loss 1639.92896\n",
      "Epoch 299 Step 2220/5304 Paf1 833.986572 Paf2 762.42218 Paf3 750.990051 Heatmap 245.201355 Total loss 2592.6\n",
      "Epoch 299 Step 2230/5304 Paf1 868.156677 Paf2 840.915649 Paf3 815.095581 Heatmap 251.500031 Total loss 2775.66797\n",
      "Epoch 299 Step 2240/5304 Paf1 603.999207 Paf2 577.582031 Paf3 577.467468 Heatmap 172.599243 Total loss 1931.64795\n",
      "Epoch 299 Step 2250/5304 Paf1 490.207764 Paf2 459.191101 Paf3 468.948242 Heatmap 156.174 Total loss 1574.52112\n",
      "Epoch 299 Step 2260/5304 Paf1 543.644531 Paf2 488.429932 Paf3 477.603455 Heatmap 172.810318 Total loss 1682.48828\n",
      "Epoch 299 Step 2270/5304 Paf1 634.619873 Paf2 593.763306 Paf3 583.764038 Heatmap 177.965088 Total loss 1990.1123\n",
      "Epoch 299 Step 2280/5304 Paf1 688.753357 Paf2 655.294861 Paf3 631.714111 Heatmap 194.162933 Total loss 2169.92529\n",
      "Epoch 299 Step 2290/5304 Paf1 566.323 Paf2 526.513794 Paf3 528.056702 Heatmap 166.496216 Total loss 1787.38965\n",
      "Epoch 299 Step 2300/5304 Paf1 707.63208 Paf2 644.839905 Paf3 637.977478 Heatmap 192.380829 Total loss 2182.83\n",
      "Epoch 299 Step 2310/5304 Paf1 667.092773 Paf2 611.917 Paf3 606.545837 Heatmap 199.937958 Total loss 2085.49365\n",
      "Epoch 299 Step 2320/5304 Paf1 429.752594 Paf2 384.742859 Paf3 370.845032 Heatmap 143.416626 Total loss 1328.75708\n",
      "Epoch 299 Step 2330/5304 Paf1 538.866882 Paf2 519.3302 Paf3 511.795197 Heatmap 180.521912 Total loss 1750.51416\n",
      "Epoch 299 Step 2340/5304 Paf1 781.708 Paf2 724.642 Paf3 732.574829 Heatmap 256.00943 Total loss 2494.93433\n",
      "Epoch 299 Step 2350/5304 Paf1 690.95929 Paf2 650.521912 Paf3 663.662231 Heatmap 227.665497 Total loss 2232.80908\n",
      "Epoch 299 Step 2360/5304 Paf1 656.006348 Paf2 613.744202 Paf3 606.095337 Heatmap 204.905869 Total loss 2080.75171\n",
      "Epoch 299 Step 2370/5304 Paf1 661.683 Paf2 622.861938 Paf3 640.811829 Heatmap 223.135742 Total loss 2148.49243\n",
      "Epoch 299 Step 2380/5304 Paf1 1300.18103 Paf2 1228.17773 Paf3 1213.09924 Heatmap 400.462463 Total loss 4141.9209\n",
      "Epoch 299 Step 2390/5304 Paf1 344.92218 Paf2 308.057983 Paf3 312.820526 Heatmap 107.934212 Total loss 1073.73486\n",
      "Epoch 299 Step 2400/5304 Paf1 1053.26721 Paf2 980.680237 Paf3 966.449341 Heatmap 274.563171 Total loss 3274.96\n",
      "Epoch 299 Step 2410/5304 Paf1 694.545532 Paf2 654.145813 Paf3 651.200867 Heatmap 228.770782 Total loss 2228.66309\n",
      "Epoch 299 Step 2420/5304 Paf1 705.049133 Paf2 674.982666 Paf3 676.188721 Heatmap 233.155853 Total loss 2289.37646\n",
      "Epoch 299 Step 2430/5304 Paf1 673.894714 Paf2 646.462097 Paf3 658.165588 Heatmap 186.349167 Total loss 2164.87158\n",
      "Epoch 299 Step 2440/5304 Paf1 650.56134 Paf2 575.614258 Paf3 567.579773 Heatmap 193.685165 Total loss 1987.44043\n",
      "Epoch 299 Step 2450/5304 Paf1 472.293365 Paf2 434.300781 Paf3 434.672821 Heatmap 151.827133 Total loss 1493.09399\n",
      "Epoch 299 Step 2460/5304 Paf1 679.603943 Paf2 639.580444 Paf3 625.592957 Heatmap 216.872223 Total loss 2161.64941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 Step 2470/5304 Paf1 812.551147 Paf2 744.936646 Paf3 729.814087 Heatmap 246.504028 Total loss 2533.80591\n",
      "Epoch 299 Step 2480/5304 Paf1 466.556671 Paf2 413.957 Paf3 426.885803 Heatmap 137.809296 Total loss 1445.20874\n",
      "Epoch 299 Step 2490/5304 Paf1 720.449158 Paf2 693.525635 Paf3 692.403198 Heatmap 192.984 Total loss 2299.36206\n",
      "Epoch 299 Step 2500/5304 Paf1 734.633789 Paf2 676.536133 Paf3 680.037903 Heatmap 208.993042 Total loss 2300.20093\n",
      "Epoch 299 Step 2510/5304 Paf1 981.250061 Paf2 914.944397 Paf3 928.561646 Heatmap 293.085419 Total loss 3117.84155\n",
      "Epoch 299 Step 2520/5304 Paf1 757.701782 Paf2 685.863953 Paf3 689.120178 Heatmap 211.800903 Total loss 2344.48682\n",
      "Epoch 299 Step 2530/5304 Paf1 629.084717 Paf2 580.075623 Paf3 591.447205 Heatmap 208.094727 Total loss 2008.70239\n",
      "Epoch 299 Step 2540/5304 Paf1 894.085938 Paf2 836.862793 Paf3 857.947937 Heatmap 297.023438 Total loss 2885.92017\n",
      "Epoch 299 Step 2550/5304 Paf1 681.179504 Paf2 658.566162 Paf3 644.082336 Heatmap 204.733093 Total loss 2188.56104\n",
      "Epoch 299 Step 2560/5304 Paf1 916.442871 Paf2 866.155701 Paf3 870.209351 Heatmap 283.445 Total loss 2936.25293\n",
      "Epoch 299 Step 2570/5304 Paf1 710.947632 Paf2 676.658691 Paf3 678.50238 Heatmap 215.981262 Total loss 2282.08984\n",
      "Epoch 299 Step 2580/5304 Paf1 657.939 Paf2 585.950134 Paf3 605.909 Heatmap 181.066406 Total loss 2030.8645\n",
      "Epoch 299 Step 2590/5304 Paf1 892.934143 Paf2 837.818848 Paf3 842.814575 Heatmap 326.17511 Total loss 2899.74268\n",
      "Epoch 299 Step 2600/5304 Paf1 749.192139 Paf2 681.922119 Paf3 680.781189 Heatmap 199.688736 Total loss 2311.58423\n",
      "Epoch 299 Step 2610/5304 Paf1 1186.66284 Paf2 1146.59351 Paf3 1139.67761 Heatmap 433.826 Total loss 3906.76\n",
      "Epoch 299 Step 2620/5304 Paf1 912.812378 Paf2 870.068726 Paf3 870.346741 Heatmap 343.869965 Total loss 2997.09766\n",
      "Epoch 299 Step 2630/5304 Paf1 757.216736 Paf2 734.593872 Paf3 729.987366 Heatmap 252.726059 Total loss 2474.52393\n",
      "Epoch 299 Step 2640/5304 Paf1 498.123016 Paf2 464.062622 Paf3 456.151245 Heatmap 144.037186 Total loss 1562.37402\n",
      "Epoch 299 Step 2650/5304 Paf1 559.580627 Paf2 512.793152 Paf3 504.659454 Heatmap 138.776093 Total loss 1715.80933\n",
      "Epoch 299 Step 2660/5304 Paf1 454.898621 Paf2 419.721619 Paf3 418.528 Heatmap 146.452118 Total loss 1439.60034\n",
      "Epoch 299 Step 2670/5304 Paf1 1076.66162 Paf2 1044.54858 Paf3 1042.08948 Heatmap 403.704834 Total loss 3567.00439\n",
      "Epoch 299 Step 2680/5304 Paf1 657.130188 Paf2 616.385193 Paf3 609.361816 Heatmap 170.547134 Total loss 2053.42432\n",
      "Epoch 299 Step 2690/5304 Paf1 918.887573 Paf2 840.989 Paf3 856.748352 Heatmap 321.55545 Total loss 2938.18042\n",
      "Epoch 299 Step 2700/5304 Paf1 1021.62677 Paf2 967.232849 Paf3 965.937622 Heatmap 358.115143 Total loss 3312.91235\n",
      "Epoch 299 Step 2710/5304 Paf1 1025.54199 Paf2 1014.80603 Paf3 999.323853 Heatmap 341.692444 Total loss 3381.36426\n",
      "Epoch 299 Step 2720/5304 Paf1 717.358459 Paf2 689.876709 Paf3 674.383423 Heatmap 232.693924 Total loss 2314.3125\n",
      "Epoch 299 Step 2730/5304 Paf1 734.63916 Paf2 683.548523 Paf3 684.053 Heatmap 242.753601 Total loss 2344.99438\n",
      "Epoch 299 Step 2740/5304 Paf1 635.694397 Paf2 616.263367 Paf3 617.015442 Heatmap 208.835907 Total loss 2077.80908\n",
      "Epoch 299 Step 2750/5304 Paf1 736.17218 Paf2 686.243591 Paf3 692.189392 Heatmap 232.695221 Total loss 2347.30029\n",
      "Epoch 299 Step 2760/5304 Paf1 744.03894 Paf2 687.728943 Paf3 698.451477 Heatmap 235.817719 Total loss 2366.03711\n",
      "Epoch 299 Step 2770/5304 Paf1 685.608643 Paf2 619.817871 Paf3 625.662 Heatmap 203.671051 Total loss 2134.75952\n",
      "Epoch 299 Step 2780/5304 Paf1 612.204651 Paf2 550.508484 Paf3 556.251892 Heatmap 175.849838 Total loss 1894.81494\n",
      "Epoch 299 Step 2790/5304 Paf1 816.274536 Paf2 766.849243 Paf3 775.221497 Heatmap 290.29248 Total loss 2648.6377\n",
      "Epoch 299 Step 2800/5304 Paf1 711.343262 Paf2 667.404968 Paf3 667.345825 Heatmap 218.510605 Total loss 2264.60474\n",
      "Epoch 299 Step 2810/5304 Paf1 814.39447 Paf2 787.8302 Paf3 769.388123 Heatmap 252.378357 Total loss 2623.99121\n",
      "Epoch 299 Step 2820/5304 Paf1 646.082153 Paf2 607.673828 Paf3 603.384888 Heatmap 180.899414 Total loss 2038.04028\n",
      "Epoch 299 Step 2830/5304 Paf1 686.004 Paf2 645.899048 Paf3 631.432556 Heatmap 206.557709 Total loss 2169.89331\n",
      "Epoch 299 Step 2840/5304 Paf1 586.510559 Paf2 550.082642 Paf3 548.124939 Heatmap 183.884277 Total loss 1868.60254\n",
      "Epoch 299 Step 2850/5304 Paf1 895.104736 Paf2 821.665222 Paf3 827.127258 Heatmap 253.932373 Total loss 2797.82959\n",
      "Epoch 299 Step 2860/5304 Paf1 779.859619 Paf2 725.705872 Paf3 709.656372 Heatmap 265.437347 Total loss 2480.65918\n",
      "Epoch 299 Step 2870/5304 Paf1 684.01123 Paf2 657.125427 Paf3 640.032776 Heatmap 229.882477 Total loss 2211.052\n",
      "Epoch 299 Step 2880/5304 Paf1 835.426941 Paf2 786.268555 Paf3 798.567871 Heatmap 241.097443 Total loss 2661.36084\n",
      "Epoch 299 Step 2890/5304 Paf1 733.264282 Paf2 697.798645 Paf3 698.027161 Heatmap 226.898895 Total loss 2355.98901\n",
      "Epoch 299 Step 2900/5304 Paf1 543.441711 Paf2 517.842407 Paf3 522.443176 Heatmap 160.016876 Total loss 1743.74426\n",
      "Epoch 299 Step 2910/5304 Paf1 890.975769 Paf2 814.192688 Paf3 823.032104 Heatmap 234.573975 Total loss 2762.77441\n",
      "Epoch 299 Step 2920/5304 Paf1 717.862183 Paf2 669.099792 Paf3 671.963623 Heatmap 244.960175 Total loss 2303.88574\n",
      "Epoch 299 Step 2930/5304 Paf1 559.389221 Paf2 541.591492 Paf3 529.301147 Heatmap 181.719696 Total loss 1812.00159\n",
      "Epoch 299 Step 2940/5304 Paf1 578.384949 Paf2 552.932068 Paf3 543.772 Heatmap 148.267456 Total loss 1823.35645\n",
      "Epoch 299 Step 2950/5304 Paf1 583.13385 Paf2 525.813721 Paf3 516.615906 Heatmap 156.56105 Total loss 1782.12451\n",
      "Epoch 299 Step 2960/5304 Paf1 722.813599 Paf2 667.334045 Paf3 657.763306 Heatmap 239.00473 Total loss 2286.91577\n",
      "Epoch 299 Step 2970/5304 Paf1 536.628113 Paf2 492.00824 Paf3 496.659973 Heatmap 158.106384 Total loss 1683.40271\n",
      "Epoch 299 Step 2980/5304 Paf1 426.003906 Paf2 379.620361 Paf3 380.481567 Heatmap 110.209671 Total loss 1296.31543\n",
      "Epoch 299 Step 2990/5304 Paf1 624.299194 Paf2 600.126587 Paf3 589.060303 Heatmap 207.503876 Total loss 2020.99\n",
      "Epoch 299 Step 3000/5304 Paf1 805.127686 Paf2 743.541687 Paf3 755.698 Heatmap 322.566376 Total loss 2626.93384\n",
      "Saved checkpoint for step 3000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1791\n",
      "Epoch 299 Step 3010/5304 Paf1 910.708923 Paf2 860.076721 Paf3 857.908203 Heatmap 337.042664 Total loss 2965.73657\n",
      "Epoch 299 Step 3020/5304 Paf1 688.66864 Paf2 639.973206 Paf3 629.505493 Heatmap 212.68573 Total loss 2170.83301\n",
      "Epoch 299 Step 3030/5304 Paf1 876.945801 Paf2 813.823486 Paf3 817.873718 Heatmap 276.982269 Total loss 2785.62524\n",
      "Epoch 299 Step 3040/5304 Paf1 587.153 Paf2 573.27594 Paf3 555.333862 Heatmap 165.000854 Total loss 1880.76367\n",
      "Epoch 299 Step 3050/5304 Paf1 584.996826 Paf2 569.614502 Paf3 558.867493 Heatmap 211.874512 Total loss 1925.35327\n",
      "Epoch 299 Step 3060/5304 Paf1 629.835083 Paf2 606.616577 Paf3 584.680725 Heatmap 178.004929 Total loss 1999.13733\n",
      "Epoch 299 Step 3070/5304 Paf1 1044.84631 Paf2 974.512329 Paf3 991.636963 Heatmap 280.330811 Total loss 3291.32642\n",
      "Epoch 299 Step 3080/5304 Paf1 543.805054 Paf2 522.776428 Paf3 526.953 Heatmap 144.232773 Total loss 1737.76733\n",
      "Epoch 299 Step 3090/5304 Paf1 790.422363 Paf2 743.494263 Paf3 738.580933 Heatmap 257.104462 Total loss 2529.60205\n",
      "Epoch 299 Step 3100/5304 Paf1 624.57428 Paf2 587.621338 Paf3 594.423218 Heatmap 180.569061 Total loss 1987.18787\n",
      "Epoch 299 Step 3110/5304 Paf1 670.098389 Paf2 648.954285 Paf3 639.392212 Heatmap 210.86618 Total loss 2169.31104\n",
      "Epoch 299 Step 3120/5304 Paf1 755.148254 Paf2 723.88092 Paf3 698.464 Heatmap 211.605911 Total loss 2389.09912\n",
      "Epoch 299 Step 3130/5304 Paf1 607.109131 Paf2 553.414368 Paf3 560.42981 Heatmap 174.930389 Total loss 1895.88367\n",
      "Epoch 299 Step 3140/5304 Paf1 736.428162 Paf2 697.380249 Paf3 709.721191 Heatmap 232.110107 Total loss 2375.63965\n",
      "Epoch 299 Step 3150/5304 Paf1 531.163452 Paf2 496.611938 Paf3 499.363922 Heatmap 147.491699 Total loss 1674.63098\n",
      "Epoch 299 Step 3160/5304 Paf1 712.374939 Paf2 710.171692 Paf3 702.336548 Heatmap 219.209747 Total loss 2344.09277\n",
      "Epoch 299 Step 3170/5304 Paf1 798.715637 Paf2 752.76825 Paf3 769.918 Heatmap 265.20816 Total loss 2586.61\n",
      "Epoch 299 Step 3180/5304 Paf1 428.7677 Paf2 391.129761 Paf3 395.86908 Heatmap 121.272842 Total loss 1337.03931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 Step 3190/5304 Paf1 533.482117 Paf2 501.658539 Paf3 508.46521 Heatmap 174.875946 Total loss 1718.48181\n",
      "Epoch 299 Step 3200/5304 Paf1 525.617 Paf2 492.838104 Paf3 492.666595 Heatmap 158.68457 Total loss 1669.80627\n",
      "Epoch 299 Step 3210/5304 Paf1 731.926514 Paf2 682.706177 Paf3 694.805542 Heatmap 204.923691 Total loss 2314.36182\n",
      "Epoch 299 Step 3220/5304 Paf1 502.699646 Paf2 464.775635 Paf3 461.39328 Heatmap 145.9776 Total loss 1574.84619\n",
      "Epoch 299 Step 3230/5304 Paf1 629.339 Paf2 613.355408 Paf3 603.493835 Heatmap 182.860474 Total loss 2029.04858\n",
      "Epoch 299 Step 3240/5304 Paf1 724.168823 Paf2 691.122742 Paf3 698.146362 Heatmap 245.167145 Total loss 2358.60498\n",
      "Epoch 299 Step 3250/5304 Paf1 712.831543 Paf2 670.319153 Paf3 676.181274 Heatmap 216.111298 Total loss 2275.44336\n",
      "Epoch 299 Step 3260/5304 Paf1 736.573364 Paf2 734.291809 Paf3 724.179688 Heatmap 279.692749 Total loss 2474.73779\n",
      "Epoch 299 Step 3270/5304 Paf1 683.658691 Paf2 648.394653 Paf3 661.491272 Heatmap 208.185959 Total loss 2201.73047\n",
      "Epoch 299 Step 3280/5304 Paf1 709.196106 Paf2 654.714844 Paf3 636.624207 Heatmap 205.565033 Total loss 2206.1\n",
      "Epoch 299 Step 3290/5304 Paf1 641.942627 Paf2 592.581909 Paf3 590.40686 Heatmap 199.627136 Total loss 2024.55859\n",
      "Epoch 299 Step 3300/5304 Paf1 702.235107 Paf2 676.645264 Paf3 668.921875 Heatmap 233.087616 Total loss 2280.89\n",
      "Epoch 299 Step 3310/5304 Paf1 651.032959 Paf2 611.365234 Paf3 594.070129 Heatmap 184.24823 Total loss 2040.71655\n",
      "Epoch 299 Step 3320/5304 Paf1 613.537354 Paf2 574.131836 Paf3 585.812 Heatmap 148.686859 Total loss 1922.16809\n",
      "Epoch 299 Step 3330/5304 Paf1 545.803589 Paf2 519.381592 Paf3 527.372 Heatmap 154.786865 Total loss 1747.34399\n",
      "Epoch 299 Step 3340/5304 Paf1 678.909912 Paf2 636.175354 Paf3 627.342896 Heatmap 173.259109 Total loss 2115.68726\n",
      "Epoch 299 Step 3350/5304 Paf1 578.457397 Paf2 550.582947 Paf3 548.3479 Heatmap 210.036224 Total loss 1887.42444\n",
      "Epoch 299 Step 3360/5304 Paf1 632.085327 Paf2 594.202576 Paf3 607.901 Heatmap 168.931259 Total loss 2003.12012\n",
      "Epoch 299 Step 3370/5304 Paf1 640.459961 Paf2 595.188599 Paf3 609.086243 Heatmap 190.84903 Total loss 2035.58386\n",
      "Epoch 299 Step 3380/5304 Paf1 616.295044 Paf2 604.296265 Paf3 596.598511 Heatmap 201.459198 Total loss 2018.64905\n",
      "Epoch 299 Step 3390/5304 Paf1 1080.72766 Paf2 1065.74805 Paf3 1054.22632 Heatmap 354.462555 Total loss 3555.16455\n",
      "Epoch 299 Step 3400/5304 Paf1 1010.99817 Paf2 942.212769 Paf3 965.005615 Heatmap 298.74585 Total loss 3216.9624\n",
      "Epoch 299 Step 3410/5304 Paf1 783.161865 Paf2 719.78125 Paf3 722.391479 Heatmap 230.93486 Total loss 2456.26953\n",
      "Epoch 299 Step 3420/5304 Paf1 807.16687 Paf2 758.339722 Paf3 757.34021 Heatmap 256.711975 Total loss 2579.55884\n",
      "Epoch 299 Step 3430/5304 Paf1 493.564392 Paf2 465.06778 Paf3 466.113892 Heatmap 160.561569 Total loss 1585.30762\n",
      "Epoch 299 Step 3440/5304 Paf1 709.171814 Paf2 655.881226 Paf3 661.13855 Heatmap 251.409454 Total loss 2277.60107\n",
      "Epoch 299 Step 3450/5304 Paf1 501.43396 Paf2 478.1633 Paf3 460.161652 Heatmap 146.865204 Total loss 1586.62415\n",
      "Epoch 299 Step 3460/5304 Paf1 472.452942 Paf2 417.30835 Paf3 414.966888 Heatmap 125.193092 Total loss 1429.92126\n",
      "Epoch 299 Step 3470/5304 Paf1 557.351318 Paf2 516.19342 Paf3 515.800232 Heatmap 180.625015 Total loss 1769.97\n",
      "Epoch 299 Step 3480/5304 Paf1 924.652466 Paf2 847.592 Paf3 850.046448 Heatmap 229.97525 Total loss 2852.26611\n",
      "Epoch 299 Step 3490/5304 Paf1 581.06604 Paf2 552.949097 Paf3 543.342529 Heatmap 164.464554 Total loss 1841.82227\n",
      "Epoch 299 Step 3500/5304 Paf1 459.973267 Paf2 417.553711 Paf3 417.945648 Heatmap 144.182907 Total loss 1439.65552\n",
      "Epoch 299 Step 3510/5304 Paf1 571.944641 Paf2 536.90863 Paf3 534.259 Heatmap 150.704071 Total loss 1793.81628\n",
      "Epoch 299 Step 3520/5304 Paf1 681.288513 Paf2 650.973389 Paf3 645.492798 Heatmap 226.959076 Total loss 2204.71387\n",
      "Epoch 299 Step 3530/5304 Paf1 749.718811 Paf2 701.259827 Paf3 704.107 Heatmap 243.725 Total loss 2398.81055\n",
      "Epoch 299 Step 3540/5304 Paf1 662.650085 Paf2 629.624451 Paf3 624.371094 Heatmap 200.171234 Total loss 2116.81689\n",
      "Epoch 299 Step 3550/5304 Paf1 387.519714 Paf2 358.055359 Paf3 350.740143 Heatmap 120.055145 Total loss 1216.37036\n",
      "Epoch 299 Step 3560/5304 Paf1 831.259277 Paf2 799.679443 Paf3 797.821289 Heatmap 266.483429 Total loss 2695.24341\n",
      "Epoch 299 Step 3570/5304 Paf1 1008.5293 Paf2 946.599854 Paf3 949.443848 Heatmap 307.640686 Total loss 3212.21362\n",
      "Epoch 299 Step 3580/5304 Paf1 725.084229 Paf2 699.463806 Paf3 701.665833 Heatmap 252.811859 Total loss 2379.02588\n",
      "Epoch 299 Step 3590/5304 Paf1 410.164093 Paf2 352.517731 Paf3 343.188141 Heatmap 118.311119 Total loss 1224.18115\n",
      "Epoch 299 Step 3600/5304 Paf1 842.250916 Paf2 783.0578 Paf3 770.457031 Heatmap 247.752518 Total loss 2643.51831\n",
      "Epoch 299 Step 3610/5304 Paf1 597.838318 Paf2 565.551 Paf3 567.531433 Heatmap 146.204346 Total loss 1877.12524\n",
      "Epoch 299 Step 3620/5304 Paf1 454.469696 Paf2 409.123871 Paf3 404.713837 Heatmap 122.031044 Total loss 1390.33838\n",
      "Epoch 299 Step 3630/5304 Paf1 720.558472 Paf2 674.975281 Paf3 688.308167 Heatmap 274.553589 Total loss 2358.39551\n",
      "Epoch 299 Step 3640/5304 Paf1 848.686401 Paf2 802.025757 Paf3 781.889343 Heatmap 294.495117 Total loss 2727.09668\n",
      "Epoch 299 Step 3650/5304 Paf1 652.823364 Paf2 615.25293 Paf3 613.866699 Heatmap 210.16066 Total loss 2092.10352\n",
      "Epoch 299 Step 3660/5304 Paf1 730.917542 Paf2 708.941284 Paf3 695.351196 Heatmap 229.999176 Total loss 2365.20923\n",
      "Epoch 299 Step 3670/5304 Paf1 568.253357 Paf2 535.367249 Paf3 538.672546 Heatmap 182.051743 Total loss 1824.34497\n",
      "Epoch 299 Step 3680/5304 Paf1 534.221313 Paf2 485.89328 Paf3 492.764954 Heatmap 190.457718 Total loss 1703.33728\n",
      "Epoch 299 Step 3690/5304 Paf1 521.59491 Paf2 462.085236 Paf3 471.013702 Heatmap 130.622681 Total loss 1585.31653\n",
      "Epoch 299 Step 3700/5304 Paf1 509.64 Paf2 467.090302 Paf3 464.553467 Heatmap 134.429977 Total loss 1575.71387\n",
      "Epoch 299 Step 3710/5304 Paf1 670.123596 Paf2 610 Paf3 609.989624 Heatmap 198.972687 Total loss 2089.08594\n",
      "Epoch 299 Step 3720/5304 Paf1 790.274475 Paf2 757.15 Paf3 758.423 Heatmap 224.624329 Total loss 2530.47192\n",
      "Epoch 299 Step 3730/5304 Paf1 446.192871 Paf2 425.402496 Paf3 415.064301 Heatmap 125.884842 Total loss 1412.54443\n",
      "Epoch 299 Step 3740/5304 Paf1 772.423096 Paf2 697.197388 Paf3 708.608276 Heatmap 242.584686 Total loss 2420.81348\n",
      "Epoch 299 Step 3750/5304 Paf1 535.185059 Paf2 474.567841 Paf3 470.047119 Heatmap 137.648788 Total loss 1617.44885\n",
      "Epoch 299 Step 3760/5304 Paf1 474.199 Paf2 449.384521 Paf3 462.51947 Heatmap 125.794289 Total loss 1511.89722\n",
      "Epoch 299 Step 3770/5304 Paf1 854.783569 Paf2 820.979797 Paf3 820.900208 Heatmap 291.173889 Total loss 2787.8374\n",
      "Epoch 299 Step 3780/5304 Paf1 657.595886 Paf2 600.439575 Paf3 602.262634 Heatmap 225.008698 Total loss 2085.30664\n",
      "Epoch 299 Step 3790/5304 Paf1 476.668427 Paf2 444.05246 Paf3 447.959412 Heatmap 122.062462 Total loss 1490.74268\n",
      "Epoch 299 Step 3800/5304 Paf1 896.070862 Paf2 839.403076 Paf3 856.699646 Heatmap 316.143555 Total loss 2908.31714\n",
      "Epoch 299 Step 3810/5304 Paf1 788.878052 Paf2 735.211853 Paf3 729.332214 Heatmap 252.540436 Total loss 2505.9624\n",
      "Epoch 299 Step 3820/5304 Paf1 368.216858 Paf2 337.881073 Paf3 338.789734 Heatmap 111.572159 Total loss 1156.45972\n",
      "Epoch 299 Step 3830/5304 Paf1 697.644531 Paf2 628.176758 Paf3 614.134155 Heatmap 191.165375 Total loss 2131.12085\n",
      "Epoch 299 Step 3840/5304 Paf1 444.087769 Paf2 390.732605 Paf3 406.472687 Heatmap 147.338409 Total loss 1388.63147\n",
      "Epoch 299 Step 3850/5304 Paf1 506.642303 Paf2 457.308594 Paf3 446.044952 Heatmap 144.379272 Total loss 1554.37512\n",
      "Epoch 299 Step 3860/5304 Paf1 764.707764 Paf2 714.351929 Paf3 699.868103 Heatmap 219.467392 Total loss 2398.39526\n",
      "Epoch 299 Step 3870/5304 Paf1 747.149353 Paf2 704.571045 Paf3 713.888123 Heatmap 234.208588 Total loss 2399.81714\n",
      "Epoch 299 Step 3880/5304 Paf1 873.942627 Paf2 838.887817 Paf3 814.784912 Heatmap 292.686035 Total loss 2820.30127\n",
      "Epoch 299 Step 3890/5304 Paf1 601.846619 Paf2 579.703 Paf3 593.153625 Heatmap 183.900543 Total loss 1958.60376\n",
      "Epoch 299 Step 3900/5304 Paf1 721.560669 Paf2 660.720642 Paf3 664.091797 Heatmap 201.084015 Total loss 2247.45703\n",
      "Epoch 299 Step 3910/5304 Paf1 820.442 Paf2 766.325256 Paf3 771.749878 Heatmap 241.679153 Total loss 2600.19629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 Step 3920/5304 Paf1 868.297913 Paf2 844.808 Paf3 849.861877 Heatmap 267.065247 Total loss 2830.0332\n",
      "Epoch 299 Step 3930/5304 Paf1 727.891968 Paf2 680.815186 Paf3 690.64032 Heatmap 199.056976 Total loss 2298.4043\n",
      "Epoch 299 Step 3940/5304 Paf1 615.871094 Paf2 577.625916 Paf3 577.442444 Heatmap 156.33548 Total loss 1927.27502\n",
      "Epoch 299 Step 3950/5304 Paf1 662.845215 Paf2 648.500916 Paf3 635.622437 Heatmap 212.510345 Total loss 2159.479\n",
      "Epoch 299 Step 3960/5304 Paf1 561.001404 Paf2 498.064331 Paf3 496.493713 Heatmap 150.302612 Total loss 1705.86206\n",
      "Epoch 299 Step 3970/5304 Paf1 570.09668 Paf2 559.884094 Paf3 551.83551 Heatmap 160.185455 Total loss 1842.00171\n",
      "Epoch 299 Step 3980/5304 Paf1 744.097656 Paf2 705.005249 Paf3 680.220581 Heatmap 200.651276 Total loss 2329.97461\n",
      "Epoch 299 Step 3990/5304 Paf1 841.489319 Paf2 802.109131 Paf3 786.669373 Heatmap 255.518341 Total loss 2685.78613\n",
      "Epoch 299 Step 4000/5304 Paf1 556.639648 Paf2 513.780945 Paf3 519.170105 Heatmap 168.833054 Total loss 1758.42383\n",
      "Saved checkpoint for step 4000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1792\n",
      "Epoch 299 Step 4010/5304 Paf1 530.930115 Paf2 498.717499 Paf3 499.031403 Heatmap 170.944275 Total loss 1699.62329\n",
      "Epoch 299 Step 4020/5304 Paf1 621.350891 Paf2 565.222046 Paf3 574.859131 Heatmap 165.931366 Total loss 1927.36353\n",
      "Epoch 299 Step 4030/5304 Paf1 665.700928 Paf2 630.060059 Paf3 614.798279 Heatmap 230.891647 Total loss 2141.45093\n",
      "Epoch 299 Step 4040/5304 Paf1 603.676 Paf2 570.358887 Paf3 551.400452 Heatmap 195.588516 Total loss 1921.02393\n",
      "Epoch 299 Step 4050/5304 Paf1 1018.29578 Paf2 961.653259 Paf3 947.350037 Heatmap 354.479797 Total loss 3281.77881\n",
      "Epoch 299 Step 4060/5304 Paf1 791.445618 Paf2 760.079 Paf3 755.059 Heatmap 286.27887 Total loss 2592.86255\n",
      "Epoch 299 Step 4070/5304 Paf1 1017.19141 Paf2 955.848877 Paf3 965.971924 Heatmap 347.18396 Total loss 3286.19629\n",
      "Epoch 299 Step 4080/5304 Paf1 697.985046 Paf2 643.997803 Paf3 617.924866 Heatmap 224.432434 Total loss 2184.34033\n",
      "Epoch 299 Step 4090/5304 Paf1 804.280884 Paf2 748.689636 Paf3 756.058 Heatmap 226.168533 Total loss 2535.19702\n",
      "Epoch 299 Step 4100/5304 Paf1 563.236633 Paf2 501.621582 Paf3 503.559113 Heatmap 142.431732 Total loss 1710.849\n",
      "Epoch 299 Step 4110/5304 Paf1 785.040283 Paf2 731.389587 Paf3 745.34613 Heatmap 294.043091 Total loss 2555.81909\n",
      "Epoch 299 Step 4120/5304 Paf1 537.545532 Paf2 472.032227 Paf3 479.101196 Heatmap 137.993378 Total loss 1626.67236\n",
      "Epoch 299 Step 4130/5304 Paf1 702.526245 Paf2 646.830688 Paf3 650.231445 Heatmap 203.085358 Total loss 2202.67383\n",
      "Epoch 299 Step 4140/5304 Paf1 477.907379 Paf2 424.968262 Paf3 418.152863 Heatmap 133.720032 Total loss 1454.74854\n",
      "Epoch 299 Step 4150/5304 Paf1 675.106567 Paf2 623.321533 Paf3 627.959473 Heatmap 206.864044 Total loss 2133.25146\n",
      "Epoch 299 Step 4160/5304 Paf1 735.176392 Paf2 726.362671 Paf3 714.806763 Heatmap 233.528931 Total loss 2409.87476\n",
      "Epoch 299 Step 4170/5304 Paf1 740.325562 Paf2 672.757324 Paf3 680.214783 Heatmap 244.788223 Total loss 2338.08594\n",
      "Epoch 299 Step 4180/5304 Paf1 684.038513 Paf2 624.484741 Paf3 612.436584 Heatmap 206.095306 Total loss 2127.05518\n",
      "Epoch 299 Step 4190/5304 Paf1 757.524658 Paf2 711.830811 Paf3 706.129 Heatmap 246.95929 Total loss 2422.44385\n",
      "Epoch 299 Step 4200/5304 Paf1 880.006958 Paf2 844.802856 Paf3 830.093811 Heatmap 273.829468 Total loss 2828.73315\n",
      "Epoch 299 Step 4210/5304 Paf1 612.374329 Paf2 578.072388 Paf3 578.547913 Heatmap 183.858246 Total loss 1952.85291\n",
      "Epoch 299 Step 4220/5304 Paf1 458.287598 Paf2 420.219818 Paf3 429.241455 Heatmap 156.240753 Total loss 1463.98962\n",
      "Epoch 299 Step 4230/5304 Paf1 880.42 Paf2 820.287354 Paf3 806.748 Heatmap 279.804199 Total loss 2787.25952\n",
      "Epoch 299 Step 4240/5304 Paf1 560.857178 Paf2 520.902832 Paf3 523.22467 Heatmap 140.643341 Total loss 1745.62805\n",
      "Epoch 299 Step 4250/5304 Paf1 706.606567 Paf2 685.076782 Paf3 686.681824 Heatmap 235.762604 Total loss 2314.12793\n",
      "Epoch 299 Step 4260/5304 Paf1 498.384888 Paf2 466.669983 Paf3 449.307312 Heatmap 137.611938 Total loss 1551.97412\n",
      "Epoch 299 Step 4270/5304 Paf1 621.909546 Paf2 561.363159 Paf3 576.873535 Heatmap 174.547394 Total loss 1934.6936\n",
      "Epoch 299 Step 4280/5304 Paf1 705.657654 Paf2 665.153564 Paf3 654.723206 Heatmap 230.083023 Total loss 2255.61743\n",
      "Epoch 299 Step 4290/5304 Paf1 548.896301 Paf2 488.948181 Paf3 492.444824 Heatmap 163.895264 Total loss 1694.18457\n",
      "Epoch 299 Step 4300/5304 Paf1 775.16217 Paf2 714.782593 Paf3 730.02655 Heatmap 260.004211 Total loss 2479.97559\n",
      "Epoch 299 Step 4310/5304 Paf1 479.332703 Paf2 444.269775 Paf3 439.669281 Heatmap 129.959442 Total loss 1493.2312\n",
      "Epoch 299 Step 4320/5304 Paf1 465.753723 Paf2 456.426147 Paf3 461.039124 Heatmap 156.78743 Total loss 1540.00635\n",
      "Epoch 299 Step 4330/5304 Paf1 514.131165 Paf2 475.214935 Paf3 487.370056 Heatmap 178.700897 Total loss 1655.41699\n",
      "Epoch 299 Step 4340/5304 Paf1 516.558777 Paf2 479.902222 Paf3 485.142639 Heatmap 159.133041 Total loss 1640.73669\n",
      "Epoch 299 Step 4350/5304 Paf1 688.565552 Paf2 626.476624 Paf3 633.946045 Heatmap 190.203156 Total loss 2139.19141\n",
      "Epoch 299 Step 4360/5304 Paf1 653.153137 Paf2 631.255493 Paf3 611.776489 Heatmap 263.813721 Total loss 2159.99902\n",
      "Epoch 299 Step 4370/5304 Paf1 784.302856 Paf2 743.097778 Paf3 744.037415 Heatmap 259.850311 Total loss 2531.28833\n",
      "Epoch 299 Step 4380/5304 Paf1 458.871307 Paf2 439.177185 Paf3 431.141357 Heatmap 123.579613 Total loss 1452.76941\n",
      "Epoch 299 Step 4390/5304 Paf1 708.903625 Paf2 683.23877 Paf3 668.052124 Heatmap 215.408691 Total loss 2275.60303\n",
      "Epoch 299 Step 4400/5304 Paf1 675.367432 Paf2 602.685852 Paf3 622.606323 Heatmap 164.938416 Total loss 2065.5979\n",
      "Epoch 299 Step 4410/5304 Paf1 650.610229 Paf2 589.040649 Paf3 578.464233 Heatmap 203.338898 Total loss 2021.45398\n",
      "Epoch 299 Step 4420/5304 Paf1 943.61322 Paf2 876.174805 Paf3 874.152222 Heatmap 286.833618 Total loss 2980.77393\n",
      "Epoch 299 Step 4430/5304 Paf1 403.266327 Paf2 367.516571 Paf3 365.209625 Heatmap 116.77803 Total loss 1252.77051\n",
      "Epoch 299 Step 4440/5304 Paf1 426.310791 Paf2 389.075897 Paf3 385.40686 Heatmap 125.744904 Total loss 1326.53845\n",
      "Epoch 299 Step 4450/5304 Paf1 657.441 Paf2 621.118896 Paf3 617.89917 Heatmap 198.662598 Total loss 2095.12158\n",
      "Epoch 299 Step 4460/5304 Paf1 586.449402 Paf2 561.958496 Paf3 553.338501 Heatmap 190.15657 Total loss 1891.90308\n",
      "Epoch 299 Step 4470/5304 Paf1 822.569031 Paf2 816.839233 Paf3 792.866089 Heatmap 277.521667 Total loss 2709.7959\n",
      "Epoch 299 Step 4480/5304 Paf1 651.223633 Paf2 589.126526 Paf3 603.539062 Heatmap 190.293518 Total loss 2034.18262\n",
      "Epoch 299 Step 4490/5304 Paf1 539.796387 Paf2 538.07312 Paf3 521.17041 Heatmap 151.783524 Total loss 1750.82349\n",
      "Epoch 299 Step 4500/5304 Paf1 694.076721 Paf2 639.920532 Paf3 624.74 Heatmap 211.359711 Total loss 2170.09717\n",
      "Epoch 299 Step 4510/5304 Paf1 607.641 Paf2 570.039734 Paf3 557.053528 Heatmap 181.608795 Total loss 1916.34302\n",
      "Epoch 299 Step 4520/5304 Paf1 773.23645 Paf2 776.292969 Paf3 748.277039 Heatmap 262.169556 Total loss 2559.97607\n",
      "Epoch 299 Step 4530/5304 Paf1 843.189209 Paf2 801.225586 Paf3 790.52124 Heatmap 321.663086 Total loss 2756.59912\n",
      "Epoch 299 Step 4540/5304 Paf1 484.066833 Paf2 461.107025 Paf3 468.824066 Heatmap 138.902405 Total loss 1552.90027\n",
      "Epoch 299 Step 4550/5304 Paf1 997.156189 Paf2 933.9646 Paf3 927.078369 Heatmap 306.7229 Total loss 3164.92212\n",
      "Epoch 299 Step 4560/5304 Paf1 970.575256 Paf2 919.246704 Paf3 915.571472 Heatmap 276.411682 Total loss 3081.80518\n",
      "Epoch 299 Step 4570/5304 Paf1 780.230103 Paf2 733.782471 Paf3 725.644958 Heatmap 228.563171 Total loss 2468.2207\n",
      "Epoch 299 Step 4580/5304 Paf1 1030.38647 Paf2 978.138672 Paf3 977.172 Heatmap 342.481689 Total loss 3328.17871\n",
      "Epoch 299 Step 4590/5304 Paf1 466.083801 Paf2 432.987396 Paf3 435.336 Heatmap 139.449051 Total loss 1473.8562\n",
      "Epoch 299 Step 4600/5304 Paf1 529.021729 Paf2 489.62793 Paf3 472.233215 Heatmap 154.102112 Total loss 1644.98499\n",
      "Epoch 299 Step 4610/5304 Paf1 697.872 Paf2 665.283813 Paf3 657.483032 Heatmap 225.038406 Total loss 2245.67725\n",
      "Epoch 299 Step 4620/5304 Paf1 739.484253 Paf2 715.548767 Paf3 714.701538 Heatmap 216.579926 Total loss 2386.31445\n",
      "Epoch 299 Step 4630/5304 Paf1 521.709351 Paf2 488.385345 Paf3 478.353455 Heatmap 165.389099 Total loss 1653.83728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 Step 4640/5304 Paf1 479.308136 Paf2 454.663513 Paf3 462.883636 Heatmap 121.299965 Total loss 1518.15527\n",
      "Epoch 299 Step 4650/5304 Paf1 500.687347 Paf2 474.214966 Paf3 481.527313 Heatmap 157.486404 Total loss 1613.91602\n",
      "Epoch 299 Step 4660/5304 Paf1 545.979919 Paf2 489.581512 Paf3 505.599548 Heatmap 152.620392 Total loss 1693.78137\n",
      "Epoch 299 Step 4670/5304 Paf1 712.100159 Paf2 666.649414 Paf3 656.389282 Heatmap 216.870728 Total loss 2252.00952\n",
      "Epoch 299 Step 4680/5304 Paf1 784.323547 Paf2 732.321411 Paf3 737.324463 Heatmap 234.070358 Total loss 2488.03979\n",
      "Epoch 299 Step 4690/5304 Paf1 648.062378 Paf2 584.547791 Paf3 579.056 Heatmap 243.176834 Total loss 2054.84302\n",
      "Epoch 299 Step 4700/5304 Paf1 923.238098 Paf2 866.800171 Paf3 872.117615 Heatmap 298.718567 Total loss 2960.87451\n",
      "Epoch 299 Step 4710/5304 Paf1 586.492065 Paf2 539.66156 Paf3 527.230591 Heatmap 155.356415 Total loss 1808.7406\n",
      "Epoch 299 Step 4720/5304 Paf1 751.983704 Paf2 681.602051 Paf3 674.956116 Heatmap 231.29509 Total loss 2339.83691\n",
      "Epoch 299 Step 4730/5304 Paf1 658.327087 Paf2 591.325 Paf3 574.151428 Heatmap 194.156799 Total loss 2017.96033\n",
      "Epoch 299 Step 4740/5304 Paf1 748.501526 Paf2 699.866394 Paf3 691.139648 Heatmap 244.121246 Total loss 2383.62891\n",
      "Epoch 299 Step 4750/5304 Paf1 1067.76392 Paf2 996.895752 Paf3 986.42749 Heatmap 309.598328 Total loss 3360.68555\n",
      "Epoch 299 Step 4760/5304 Paf1 677.383545 Paf2 610.672913 Paf3 608.164795 Heatmap 206.571762 Total loss 2102.79297\n",
      "Epoch 299 Step 4770/5304 Paf1 633.670776 Paf2 595.021484 Paf3 577.378296 Heatmap 179.083984 Total loss 1985.15454\n",
      "Epoch 299 Step 4780/5304 Paf1 1001.73932 Paf2 970.457031 Paf3 947.98822 Heatmap 384.418152 Total loss 3304.60254\n",
      "Epoch 299 Step 4790/5304 Paf1 622.730103 Paf2 598.887512 Paf3 605.131042 Heatmap 212.496506 Total loss 2039.24524\n",
      "Epoch 299 Step 4800/5304 Paf1 642.53418 Paf2 601.547058 Paf3 598.6026 Heatmap 192.718109 Total loss 2035.40198\n",
      "Epoch 299 Step 4810/5304 Paf1 507.499664 Paf2 483.027435 Paf3 483.357117 Heatmap 164.59491 Total loss 1638.47913\n",
      "Epoch 299 Step 4820/5304 Paf1 619.962 Paf2 574.060669 Paf3 558.524536 Heatmap 169.553207 Total loss 1922.10046\n",
      "Epoch 299 Step 4830/5304 Paf1 569.891968 Paf2 539.00708 Paf3 552.995483 Heatmap 182.504395 Total loss 1844.39893\n",
      "Epoch 299 Step 4840/5304 Paf1 578.515625 Paf2 528.163574 Paf3 520.380615 Heatmap 132.476898 Total loss 1759.53674\n",
      "Epoch 299 Step 4850/5304 Paf1 1037.10986 Paf2 999.59259 Paf3 1001.00305 Heatmap 359.21698 Total loss 3396.92236\n",
      "Epoch 299 Step 4860/5304 Paf1 1036.21948 Paf2 976.775818 Paf3 993.108093 Heatmap 368.075958 Total loss 3374.17944\n",
      "Epoch 299 Step 4870/5304 Paf1 500.677368 Paf2 458.218353 Paf3 467.786438 Heatmap 131.086136 Total loss 1557.76831\n",
      "Epoch 299 Step 4880/5304 Paf1 783.52417 Paf2 759.449219 Paf3 755.725098 Heatmap 276.858887 Total loss 2575.55737\n",
      "Epoch 299 Step 4890/5304 Paf1 1086.27991 Paf2 1026.13867 Paf3 1032.94604 Heatmap 393.025696 Total loss 3538.39014\n",
      "Epoch 299 Step 4900/5304 Paf1 669.096069 Paf2 631.323364 Paf3 630.883 Heatmap 204.697601 Total loss 2136\n",
      "Epoch 299 Step 4910/5304 Paf1 718.850098 Paf2 697.476257 Paf3 693.271362 Heatmap 246.187378 Total loss 2355.78516\n",
      "Epoch 299 Step 4920/5304 Paf1 1044.2334 Paf2 975.675049 Paf3 998.92511 Heatmap 283.938202 Total loss 3302.77173\n",
      "Epoch 299 Step 4930/5304 Paf1 611.572754 Paf2 568.076477 Paf3 596.194519 Heatmap 190.699158 Total loss 1966.54285\n",
      "Epoch 299 Step 4940/5304 Paf1 573.714783 Paf2 542.669556 Paf3 549.70105 Heatmap 196.357544 Total loss 1862.44287\n",
      "Epoch 299 Step 4950/5304 Paf1 544.847351 Paf2 496.87 Paf3 505.658569 Heatmap 163.712219 Total loss 1711.08813\n",
      "Epoch 299 Step 4960/5304 Paf1 759.717407 Paf2 722.509705 Paf3 713.881775 Heatmap 260.404755 Total loss 2456.51367\n",
      "Epoch 299 Step 4970/5304 Paf1 740.102661 Paf2 695.156616 Paf3 710.966492 Heatmap 223.848145 Total loss 2370.07397\n",
      "Epoch 299 Step 4980/5304 Paf1 895.051758 Paf2 860.872 Paf3 842.385925 Heatmap 291.831604 Total loss 2890.14136\n",
      "Epoch 299 Step 4990/5304 Paf1 512.178101 Paf2 484.054016 Paf3 475.044586 Heatmap 156.685699 Total loss 1627.9624\n",
      "Epoch 299 Step 5000/5304 Paf1 594.397461 Paf2 562.715149 Paf3 556.187256 Heatmap 175.618668 Total loss 1888.91846\n",
      "Saved checkpoint for step 5000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1793\n",
      "Epoch 299 Step 5010/5304 Paf1 638.62738 Paf2 578.126 Paf3 584.891357 Heatmap 175.037186 Total loss 1976.68188\n",
      "Epoch 299 Step 5020/5304 Paf1 694.385742 Paf2 661.977905 Paf3 656.539124 Heatmap 244.699799 Total loss 2257.60254\n",
      "Epoch 299 Step 5030/5304 Paf1 546.75885 Paf2 523.380249 Paf3 522.418762 Heatmap 175.361832 Total loss 1767.91968\n",
      "Epoch 299 Step 5040/5304 Paf1 805.325317 Paf2 733.179 Paf3 739.601929 Heatmap 230.972076 Total loss 2509.07837\n",
      "Epoch 299 Step 5050/5304 Paf1 529.190857 Paf2 495.674103 Paf3 504.077423 Heatmap 178.322098 Total loss 1707.26453\n",
      "Epoch 299 Step 5060/5304 Paf1 649.070679 Paf2 605.46637 Paf3 620.06134 Heatmap 243.353683 Total loss 2117.95215\n",
      "Epoch 299 Step 5070/5304 Paf1 765.084229 Paf2 692.528076 Paf3 713.019958 Heatmap 245.52179 Total loss 2416.15405\n",
      "Epoch 299 Step 5080/5304 Paf1 462.079163 Paf2 425.345154 Paf3 426.585052 Heatmap 125.42218 Total loss 1439.43152\n",
      "Epoch 299 Step 5090/5304 Paf1 796.634399 Paf2 751.579773 Paf3 747.678223 Heatmap 245.114182 Total loss 2541.00659\n",
      "Epoch 299 Step 5100/5304 Paf1 724.105469 Paf2 690.032166 Paf3 691.035217 Heatmap 195.52832 Total loss 2300.70117\n",
      "Epoch 299 Step 5110/5304 Paf1 639.135925 Paf2 616.779419 Paf3 627.592407 Heatmap 220.912262 Total loss 2104.42\n",
      "Epoch 299 Step 5120/5304 Paf1 844.735901 Paf2 806.202881 Paf3 795.218384 Heatmap 310.708069 Total loss 2756.86523\n",
      "Epoch 299 Step 5130/5304 Paf1 602.864685 Paf2 564.425537 Paf3 570.851807 Heatmap 186.972198 Total loss 1925.11426\n",
      "Epoch 299 Step 5140/5304 Paf1 519.217041 Paf2 464.882141 Paf3 481.998138 Heatmap 158.875275 Total loss 1624.97266\n",
      "Epoch 299 Step 5150/5304 Paf1 478.534424 Paf2 439.7341 Paf3 418.93045 Heatmap 126.144142 Total loss 1463.34314\n",
      "Epoch 299 Step 5160/5304 Paf1 676.981812 Paf2 626.819519 Paf3 633.282 Heatmap 191.25174 Total loss 2128.33496\n",
      "Epoch 299 Step 5170/5304 Paf1 599.498108 Paf2 555.192505 Paf3 555.257 Heatmap 177.867401 Total loss 1887.81506\n",
      "Epoch 299 Step 5180/5304 Paf1 1106.47583 Paf2 1017.53723 Paf3 1004.32959 Heatmap 332.760529 Total loss 3461.10327\n",
      "Epoch 299 Step 5190/5304 Paf1 651.753052 Paf2 619.37915 Paf3 601.422668 Heatmap 242.823975 Total loss 2115.37891\n",
      "Epoch 299 Step 5200/5304 Paf1 544.873413 Paf2 489.976257 Paf3 498.405396 Heatmap 169.693436 Total loss 1702.94849\n",
      "Epoch 299 Step 5210/5304 Paf1 919.150635 Paf2 858.114 Paf3 871.321716 Heatmap 257.646729 Total loss 2906.23315\n",
      "Epoch 299 Step 5220/5304 Paf1 708.572754 Paf2 678.403259 Paf3 678.444641 Heatmap 228.057892 Total loss 2293.47852\n",
      "Epoch 299 Step 5230/5304 Paf1 691.374146 Paf2 657.43927 Paf3 642.74353 Heatmap 181.161789 Total loss 2172.71875\n",
      "Epoch 299 Step 5240/5304 Paf1 696.486145 Paf2 668.95343 Paf3 667.836731 Heatmap 217.595367 Total loss 2250.87158\n",
      "Epoch 299 Step 5250/5304 Paf1 685.00885 Paf2 632.997192 Paf3 648.374634 Heatmap 200.579407 Total loss 2166.96021\n",
      "Epoch 299 Step 5260/5304 Paf1 890.361389 Paf2 860.634399 Paf3 852.548889 Heatmap 281.028168 Total loss 2884.57275\n",
      "Epoch 299 Step 5270/5304 Paf1 560.888794 Paf2 544.920471 Paf3 531.460693 Heatmap 208.415588 Total loss 1845.68555\n",
      "Epoch 299 Step 5280/5304 Paf1 786.893188 Paf2 745.20813 Paf3 746.381226 Heatmap 249.820984 Total loss 2528.30347\n",
      "Epoch 299 Step 5290/5304 Paf1 802.98053 Paf2 741.805908 Paf3 753.30542 Heatmap 285.49762 Total loss 2583.58936\n",
      "Epoch 299 Step 5300/5304 Paf1 540.530579 Paf2 523.425781 Paf3 521.54834 Heatmap 146.846466 Total loss 1732.35107\n",
      "Completed epoch 299. Saving weights...\n",
      "Epoch training time: 0:15:47.510980\n",
      "Calculating validation losses...\n",
      "Validation step 0 ...\n",
      "Validation losses for epoch: 299 : Loss paf 681.9847412109375, Loss heatmap 225.65615844726562, Total loss 2310.356201171875\n",
      "Start processing epoch 300\n",
      "Epoch 300 Step 10/5304 Paf1 653.174377 Paf2 598.629 Paf3 603.571716 Heatmap 211.799 Total loss 2067.17432\n",
      "Epoch 300 Step 20/5304 Paf1 827.550049 Paf2 797.574463 Paf3 822.998779 Heatmap 284.933594 Total loss 2733.05688\n",
      "Epoch 300 Step 30/5304 Paf1 795.722 Paf2 721.556824 Paf3 739.249695 Heatmap 230.915726 Total loss 2487.44434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300 Step 40/5304 Paf1 482.304626 Paf2 450.75238 Paf3 441.359131 Heatmap 131.800781 Total loss 1506.21692\n",
      "Epoch 300 Step 50/5304 Paf1 814.530823 Paf2 806.345032 Paf3 782.247864 Heatmap 312.409332 Total loss 2715.5332\n",
      "Epoch 300 Step 60/5304 Paf1 752.067261 Paf2 696.685 Paf3 679.416382 Heatmap 197.969711 Total loss 2326.13818\n",
      "Epoch 300 Step 70/5304 Paf1 614.078918 Paf2 577.143188 Paf3 574.134155 Heatmap 175.156677 Total loss 1940.51294\n",
      "Epoch 300 Step 80/5304 Paf1 883.047546 Paf2 862.314209 Paf3 826.820435 Heatmap 244.794037 Total loss 2816.97632\n",
      "Epoch 300 Step 90/5304 Paf1 1059.62988 Paf2 1032.32117 Paf3 1019.44592 Heatmap 362.752197 Total loss 3474.14941\n",
      "Epoch 300 Step 100/5304 Paf1 559.572632 Paf2 513.57605 Paf3 508.682343 Heatmap 161.181671 Total loss 1743.0127\n",
      "Epoch 300 Step 110/5304 Paf1 624.580444 Paf2 588.846313 Paf3 613.957092 Heatmap 199.070663 Total loss 2026.45459\n",
      "Epoch 300 Step 120/5304 Paf1 731.542114 Paf2 680.220581 Paf3 695.117188 Heatmap 218.138153 Total loss 2325.01807\n",
      "Epoch 300 Step 130/5304 Paf1 863.272583 Paf2 810.25946 Paf3 800.787964 Heatmap 259.971771 Total loss 2734.29175\n",
      "Epoch 300 Step 140/5304 Paf1 592.889404 Paf2 575.135742 Paf3 539.125 Heatmap 174.254791 Total loss 1881.40491\n",
      "Epoch 300 Step 150/5304 Paf1 547.299683 Paf2 492.256348 Paf3 478.117767 Heatmap 157.051514 Total loss 1674.72534\n",
      "Epoch 300 Step 160/5304 Paf1 926.624 Paf2 862.502 Paf3 863.312073 Heatmap 278.228638 Total loss 2930.66675\n",
      "Epoch 300 Step 170/5304 Paf1 564.65155 Paf2 536.453125 Paf3 527.930603 Heatmap 210.344696 Total loss 1839.38\n",
      "Epoch 300 Step 180/5304 Paf1 608.887634 Paf2 552.949524 Paf3 552.05249 Heatmap 168.18898 Total loss 1882.07861\n",
      "Epoch 300 Step 190/5304 Paf1 719.723145 Paf2 678.441162 Paf3 675.239685 Heatmap 211.944321 Total loss 2285.34839\n",
      "Epoch 300 Step 200/5304 Paf1 568.928589 Paf2 514.712 Paf3 527.272766 Heatmap 130.250397 Total loss 1741.16382\n",
      "Epoch 300 Step 210/5304 Paf1 824.586426 Paf2 780.973633 Paf3 791.377319 Heatmap 279.431641 Total loss 2676.36914\n",
      "Epoch 300 Step 220/5304 Paf1 609.46405 Paf2 578.29895 Paf3 575.346741 Heatmap 189.162598 Total loss 1952.27222\n",
      "Epoch 300 Step 230/5304 Paf1 528.0177 Paf2 482.171265 Paf3 472.929626 Heatmap 160.791656 Total loss 1643.91028\n",
      "Epoch 300 Step 240/5304 Paf1 641.675964 Paf2 615.197388 Paf3 607.55896 Heatmap 190.147217 Total loss 2054.57959\n",
      "Epoch 300 Step 250/5304 Paf1 686.724854 Paf2 643.70752 Paf3 643.849487 Heatmap 245.647568 Total loss 2219.92944\n",
      "Epoch 300 Step 260/5304 Paf1 1336.54102 Paf2 1254.20801 Paf3 1250.75488 Heatmap 543.031372 Total loss 4384.53516\n",
      "Epoch 300 Step 270/5304 Paf1 745.713745 Paf2 680.432129 Paf3 693.068359 Heatmap 201.62915 Total loss 2320.84326\n",
      "Epoch 300 Step 280/5304 Paf1 882.627563 Paf2 818.039 Paf3 819.324829 Heatmap 262.102966 Total loss 2782.09424\n",
      "Epoch 300 Step 290/5304 Paf1 864.163574 Paf2 794.191589 Paf3 780.313171 Heatmap 276.702698 Total loss 2715.37109\n",
      "Epoch 300 Step 300/5304 Paf1 1026.45044 Paf2 972.698608 Paf3 969.62 Heatmap 285.521362 Total loss 3254.29053\n",
      "Epoch 300 Step 310/5304 Paf1 669.957764 Paf2 630.334595 Paf3 625.295471 Heatmap 181.733582 Total loss 2107.32129\n",
      "Epoch 300 Step 320/5304 Paf1 640.49823 Paf2 606.674316 Paf3 585.261414 Heatmap 184.893845 Total loss 2017.32788\n",
      "Epoch 300 Step 330/5304 Paf1 692.0755 Paf2 662.944031 Paf3 649.514 Heatmap 228.646225 Total loss 2233.17969\n",
      "Epoch 300 Step 340/5304 Paf1 620.297363 Paf2 585.211243 Paf3 573.129272 Heatmap 175.312363 Total loss 1953.9502\n",
      "Epoch 300 Step 350/5304 Paf1 823.071777 Paf2 793.306396 Paf3 788.881653 Heatmap 306.661682 Total loss 2711.92139\n",
      "Epoch 300 Step 360/5304 Paf1 832.202454 Paf2 809.600159 Paf3 809.556 Heatmap 279.611816 Total loss 2730.97046\n",
      "Epoch 300 Step 370/5304 Paf1 847.434509 Paf2 823.726196 Paf3 825.306824 Heatmap 318.447052 Total loss 2814.91455\n",
      "Epoch 300 Step 380/5304 Paf1 530.752258 Paf2 500.374481 Paf3 496.1362 Heatmap 191.011688 Total loss 1718.27466\n",
      "Epoch 300 Step 390/5304 Paf1 515.873657 Paf2 480.372864 Paf3 485.114319 Heatmap 155.014175 Total loss 1636.375\n",
      "Epoch 300 Step 400/5304 Paf1 608.737122 Paf2 582.773682 Paf3 589.197876 Heatmap 180.079849 Total loss 1960.78845\n",
      "Epoch 300 Step 410/5304 Paf1 505.013031 Paf2 448.505432 Paf3 456.443573 Heatmap 131.1185 Total loss 1541.08057\n",
      "Epoch 300 Step 420/5304 Paf1 563.17157 Paf2 531.004089 Paf3 516.914124 Heatmap 169.518616 Total loss 1780.6084\n",
      "Epoch 300 Step 430/5304 Paf1 823.138306 Paf2 772.053772 Paf3 765.170227 Heatmap 235.978271 Total loss 2596.34058\n",
      "Epoch 300 Step 440/5304 Paf1 503.411713 Paf2 476.858795 Paf3 472.138824 Heatmap 170.473267 Total loss 1622.88257\n",
      "Epoch 300 Step 450/5304 Paf1 439.667175 Paf2 409.056458 Paf3 400.49881 Heatmap 117.121346 Total loss 1366.34375\n",
      "Epoch 300 Step 460/5304 Paf1 710.367432 Paf2 638.070679 Paf3 639.504944 Heatmap 224.141693 Total loss 2212.08472\n",
      "Epoch 300 Step 470/5304 Paf1 546.814 Paf2 502.179382 Paf3 498.847 Heatmap 159.428894 Total loss 1707.26929\n",
      "Epoch 300 Step 480/5304 Paf1 698.031128 Paf2 647.471924 Paf3 652.490356 Heatmap 235.983 Total loss 2233.97656\n",
      "Epoch 300 Step 490/5304 Paf1 1053.22485 Paf2 1025.94458 Paf3 1025.10327 Heatmap 356.766571 Total loss 3461.03931\n",
      "Epoch 300 Step 500/5304 Paf1 868.898376 Paf2 790.123779 Paf3 807.621338 Heatmap 270.988342 Total loss 2737.63184\n",
      "Epoch 300 Step 510/5304 Paf1 595.669678 Paf2 577.537659 Paf3 582.655396 Heatmap 201.267609 Total loss 1957.13025\n",
      "Epoch 300 Step 520/5304 Paf1 620.570068 Paf2 608.690552 Paf3 606.605713 Heatmap 202.632874 Total loss 2038.49927\n",
      "Epoch 300 Step 530/5304 Paf1 485.684235 Paf2 466.87442 Paf3 468.255768 Heatmap 155.745361 Total loss 1576.55981\n",
      "Epoch 300 Step 540/5304 Paf1 1024.40088 Paf2 979.255859 Paf3 993.701782 Heatmap 342.630707 Total loss 3339.98926\n",
      "Epoch 300 Step 550/5304 Paf1 652.874512 Paf2 609.039307 Paf3 614.738159 Heatmap 187.438782 Total loss 2064.09082\n",
      "Epoch 300 Step 560/5304 Paf1 621.33606 Paf2 585.574463 Paf3 586.783386 Heatmap 193.900116 Total loss 1987.59399\n",
      "Epoch 300 Step 570/5304 Paf1 558.95105 Paf2 505.644836 Paf3 515.883484 Heatmap 186.150269 Total loss 1766.62964\n",
      "Epoch 300 Step 580/5304 Paf1 817.042114 Paf2 751.858154 Paf3 757.101746 Heatmap 237.426056 Total loss 2563.42822\n",
      "Epoch 300 Step 590/5304 Paf1 864.724243 Paf2 813.13031 Paf3 813.671 Heatmap 268.579742 Total loss 2760.10522\n",
      "Epoch 300 Step 600/5304 Paf1 736.296 Paf2 711.753845 Paf3 688.915955 Heatmap 237.854309 Total loss 2374.82\n",
      "Epoch 300 Step 610/5304 Paf1 624.909363 Paf2 588.947449 Paf3 587.481873 Heatmap 225.44017 Total loss 2026.77881\n",
      "Epoch 300 Step 620/5304 Paf1 808.339294 Paf2 772.645447 Paf3 749.937561 Heatmap 225.692154 Total loss 2556.6145\n",
      "Epoch 300 Step 630/5304 Paf1 696.346069 Paf2 645.596558 Paf3 631.078735 Heatmap 213.070984 Total loss 2186.09229\n",
      "Epoch 300 Step 640/5304 Paf1 741.115906 Paf2 687.279419 Paf3 710.778 Heatmap 245.307358 Total loss 2384.48071\n",
      "Epoch 300 Step 650/5304 Paf1 698.819397 Paf2 651.565308 Paf3 638.585938 Heatmap 183.393936 Total loss 2172.36475\n",
      "Epoch 300 Step 660/5304 Paf1 779.486 Paf2 759.321716 Paf3 747.048767 Heatmap 296.469269 Total loss 2582.32568\n",
      "Epoch 300 Step 670/5304 Paf1 703.729797 Paf2 666.063843 Paf3 682.690735 Heatmap 231.058563 Total loss 2283.54297\n",
      "Epoch 300 Step 680/5304 Paf1 1224.45215 Paf2 1146.29504 Paf3 1130.86194 Heatmap 354.559326 Total loss 3856.16846\n",
      "Epoch 300 Step 690/5304 Paf1 715.541 Paf2 675.641235 Paf3 674.620483 Heatmap 197.602814 Total loss 2263.40552\n",
      "Epoch 300 Step 700/5304 Paf1 384.160339 Paf2 326.597595 Paf3 334.873505 Heatmap 109.006996 Total loss 1154.63843\n",
      "Epoch 300 Step 710/5304 Paf1 741.039551 Paf2 710.017456 Paf3 713.627136 Heatmap 265.829834 Total loss 2430.51392\n",
      "Epoch 300 Step 720/5304 Paf1 768.692444 Paf2 755.059448 Paf3 745.19989 Heatmap 253.062958 Total loss 2522.01465\n",
      "Epoch 300 Step 730/5304 Paf1 607.702271 Paf2 545.115662 Paf3 542.165039 Heatmap 188.704987 Total loss 1883.68787\n",
      "Epoch 300 Step 740/5304 Paf1 895.097717 Paf2 817.269897 Paf3 818.890869 Heatmap 249.898834 Total loss 2781.15723\n",
      "Epoch 300 Step 750/5304 Paf1 751.80835 Paf2 679.207825 Paf3 690.307495 Heatmap 190.606033 Total loss 2311.92969\n",
      "Epoch 300 Step 760/5304 Paf1 720.928345 Paf2 661.341 Paf3 666.373718 Heatmap 243.859024 Total loss 2292.50195\n",
      "Epoch 300 Step 770/5304 Paf1 656.928101 Paf2 628.91687 Paf3 614.933716 Heatmap 235.177979 Total loss 2135.95654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300 Step 780/5304 Paf1 511.011871 Paf2 463.810638 Paf3 465.696289 Heatmap 137.009903 Total loss 1577.52869\n",
      "Epoch 300 Step 790/5304 Paf1 599.547852 Paf2 577.886414 Paf3 560.781738 Heatmap 193.432678 Total loss 1931.64868\n",
      "Epoch 300 Step 800/5304 Paf1 423.451752 Paf2 402.789978 Paf3 405.10968 Heatmap 108.137772 Total loss 1339.48914\n",
      "Epoch 300 Step 810/5304 Paf1 551.039429 Paf2 506.443756 Paf3 501.961761 Heatmap 150.621674 Total loss 1710.06665\n",
      "Epoch 300 Step 820/5304 Paf1 637.850586 Paf2 583.006653 Paf3 596.191162 Heatmap 186.271851 Total loss 2003.32019\n",
      "Epoch 300 Step 830/5304 Paf1 424.961395 Paf2 414.356934 Paf3 408.588776 Heatmap 102.536629 Total loss 1350.44373\n",
      "Epoch 300 Step 840/5304 Paf1 606.746643 Paf2 584.380737 Paf3 568.510254 Heatmap 171.574692 Total loss 1931.2124\n",
      "Epoch 300 Step 850/5304 Paf1 1207.63293 Paf2 1106.10559 Paf3 1107.59351 Heatmap 391.847382 Total loss 3813.17944\n",
      "Epoch 300 Step 860/5304 Paf1 475.341248 Paf2 411.619 Paf3 409.907593 Heatmap 125.82695 Total loss 1422.69482\n",
      "Epoch 300 Step 870/5304 Paf1 602.290833 Paf2 586.054565 Paf3 574.456604 Heatmap 189.91803 Total loss 1952.72009\n",
      "Epoch 300 Step 880/5304 Paf1 920.708191 Paf2 900.183655 Paf3 886.664124 Heatmap 307.49585 Total loss 3015.05176\n",
      "Epoch 300 Step 890/5304 Paf1 527.611816 Paf2 506.659485 Paf3 504.832458 Heatmap 161.889175 Total loss 1700.99292\n",
      "Epoch 300 Step 900/5304 Paf1 726.874512 Paf2 678.420044 Paf3 683.090088 Heatmap 261.496033 Total loss 2349.88062\n",
      "Epoch 300 Step 910/5304 Paf1 491.040283 Paf2 451.848267 Paf3 471.579224 Heatmap 128.018829 Total loss 1542.48657\n",
      "Epoch 300 Step 920/5304 Paf1 586.029968 Paf2 533.878052 Paf3 526.053711 Heatmap 148.264542 Total loss 1794.2262\n",
      "Epoch 300 Step 930/5304 Paf1 596.94 Paf2 556.719 Paf3 561.25177 Heatmap 172.978622 Total loss 1887.8894\n",
      "Epoch 300 Step 940/5304 Paf1 489.971344 Paf2 456.039337 Paf3 443.159882 Heatmap 147.384644 Total loss 1536.55518\n",
      "Epoch 300 Step 950/5304 Paf1 763.25354 Paf2 746.760315 Paf3 716.372803 Heatmap 247.247192 Total loss 2473.63379\n",
      "Epoch 300 Step 960/5304 Paf1 719.998169 Paf2 670.319214 Paf3 688.157837 Heatmap 220.479034 Total loss 2298.9541\n",
      "Epoch 300 Step 970/5304 Paf1 669.698059 Paf2 664.633606 Paf3 636.426758 Heatmap 201.88118 Total loss 2172.63965\n",
      "Epoch 300 Step 980/5304 Paf1 863.739502 Paf2 814.537 Paf3 805.377197 Heatmap 293.362976 Total loss 2777.0166\n",
      "Epoch 300 Step 990/5304 Paf1 619.112427 Paf2 585.810669 Paf3 569.200317 Heatmap 172.913956 Total loss 1947.03735\n",
      "Epoch 300 Step 1000/5304 Paf1 1278.23828 Paf2 1234.16858 Paf3 1209.55847 Heatmap 409.847321 Total loss 4131.8125\n",
      "Saved checkpoint for step 1000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1795\n",
      "Epoch 300 Step 1010/5304 Paf1 650.434143 Paf2 623.089539 Paf3 614.677429 Heatmap 226.582947 Total loss 2114.78418\n",
      "Epoch 300 Step 1020/5304 Paf1 429.074402 Paf2 392.880402 Paf3 398.888275 Heatmap 91.7447357 Total loss 1312.58789\n",
      "Epoch 300 Step 1030/5304 Paf1 975.089417 Paf2 925.572632 Paf3 939.066467 Heatmap 300.237 Total loss 3139.96558\n",
      "Epoch 300 Step 1040/5304 Paf1 674.246765 Paf2 647.590637 Paf3 616.710083 Heatmap 199.583618 Total loss 2138.1311\n",
      "Epoch 300 Step 1050/5304 Paf1 632.195068 Paf2 580.634766 Paf3 597.214417 Heatmap 188.896912 Total loss 1998.94116\n",
      "Epoch 300 Step 1060/5304 Paf1 770.716919 Paf2 742.256653 Paf3 740.563782 Heatmap 281.304779 Total loss 2534.84229\n",
      "Epoch 300 Step 1070/5304 Paf1 506.593048 Paf2 490.117798 Paf3 486.19574 Heatmap 177.722046 Total loss 1660.62866\n",
      "Epoch 300 Step 1080/5304 Paf1 787.018372 Paf2 767.979919 Paf3 768.491577 Heatmap 259.601471 Total loss 2583.09131\n",
      "Epoch 300 Step 1090/5304 Paf1 555.44574 Paf2 493.871124 Paf3 506.189728 Heatmap 136.724548 Total loss 1692.2312\n",
      "Epoch 300 Step 1100/5304 Paf1 604.248596 Paf2 556.787354 Paf3 555.684509 Heatmap 159.548126 Total loss 1876.26855\n",
      "Epoch 300 Step 1110/5304 Paf1 866.029907 Paf2 818.650269 Paf3 834.376282 Heatmap 283.56 Total loss 2802.61646\n",
      "Epoch 300 Step 1120/5304 Paf1 649.184204 Paf2 596.360413 Paf3 597.359619 Heatmap 164.449707 Total loss 2007.354\n",
      "Epoch 300 Step 1130/5304 Paf1 606.898804 Paf2 584.412048 Paf3 577.546875 Heatmap 196.530945 Total loss 1965.38867\n",
      "Epoch 300 Step 1140/5304 Paf1 503.071289 Paf2 441.721832 Paf3 442.700714 Heatmap 126.832077 Total loss 1514.32593\n",
      "Epoch 300 Step 1150/5304 Paf1 843.136597 Paf2 785.554138 Paf3 768.466492 Heatmap 277.517334 Total loss 2674.67456\n",
      "Epoch 300 Step 1160/5304 Paf1 685.5672 Paf2 635.93335 Paf3 632.769043 Heatmap 192.625092 Total loss 2146.89453\n",
      "Epoch 300 Step 1170/5304 Paf1 533.963074 Paf2 489.044922 Paf3 484.05719 Heatmap 153.523468 Total loss 1660.58862\n",
      "Epoch 300 Step 1180/5304 Paf1 448.477783 Paf2 392.155426 Paf3 392.26593 Heatmap 99.7992783 Total loss 1332.69836\n",
      "Epoch 300 Step 1190/5304 Paf1 651.206055 Paf2 630.337585 Paf3 620.198303 Heatmap 182.963364 Total loss 2084.70532\n",
      "Epoch 300 Step 1200/5304 Paf1 517.676086 Paf2 486.702057 Paf3 480.488037 Heatmap 151.678909 Total loss 1636.54517\n",
      "Epoch 300 Step 1210/5304 Paf1 641.371704 Paf2 579.479248 Paf3 591.674927 Heatmap 208.926514 Total loss 2021.45239\n",
      "Epoch 300 Step 1220/5304 Paf1 708.728088 Paf2 689.743164 Paf3 687.036865 Heatmap 237.235458 Total loss 2322.74365\n",
      "Epoch 300 Step 1230/5304 Paf1 900.087402 Paf2 865.632385 Paf3 869.50116 Heatmap 313.76828 Total loss 2948.98926\n",
      "Epoch 300 Step 1240/5304 Paf1 1001.01276 Paf2 938.283813 Paf3 935.714722 Heatmap 346.109833 Total loss 3221.12109\n",
      "Epoch 300 Step 1250/5304 Paf1 681.629272 Paf2 651.397461 Paf3 639.663635 Heatmap 194.064575 Total loss 2166.75488\n",
      "Epoch 300 Step 1260/5304 Paf1 778.152405 Paf2 733.842407 Paf3 730.56604 Heatmap 232.473358 Total loss 2475.03418\n",
      "Epoch 300 Step 1270/5304 Paf1 646.910095 Paf2 584.136841 Paf3 581.82373 Heatmap 196.351166 Total loss 2009.2218\n",
      "Epoch 300 Step 1280/5304 Paf1 997.060242 Paf2 934.25061 Paf3 953.652954 Heatmap 309.64 Total loss 3194.60376\n",
      "Epoch 300 Step 1290/5304 Paf1 1036.83044 Paf2 963.378052 Paf3 970.777405 Heatmap 301.575745 Total loss 3272.56152\n",
      "Epoch 300 Step 1300/5304 Paf1 414.298737 Paf2 381.175385 Paf3 378.530579 Heatmap 123.82608 Total loss 1297.83081\n",
      "Epoch 300 Step 1310/5304 Paf1 754.42041 Paf2 707.221436 Paf3 712.517944 Heatmap 258.677612 Total loss 2432.8374\n",
      "Epoch 300 Step 1320/5304 Paf1 549.653809 Paf2 510.486267 Paf3 502.582703 Heatmap 160.033737 Total loss 1722.75659\n",
      "Epoch 300 Step 1330/5304 Paf1 853.204163 Paf2 794.763123 Paf3 817.026306 Heatmap 242.631897 Total loss 2707.62549\n",
      "Epoch 300 Step 1340/5304 Paf1 485.899017 Paf2 461.303741 Paf3 458.588318 Heatmap 163.789154 Total loss 1569.5802\n",
      "Epoch 300 Step 1350/5304 Paf1 532.194458 Paf2 504.889984 Paf3 496.660492 Heatmap 192.792786 Total loss 1726.53772\n",
      "Epoch 300 Step 1360/5304 Paf1 784.833374 Paf2 735.161255 Paf3 734.098755 Heatmap 201.387253 Total loss 2455.48071\n",
      "Epoch 300 Step 1370/5304 Paf1 809.909668 Paf2 743.258362 Paf3 722.253662 Heatmap 221.02179 Total loss 2496.44336\n",
      "Epoch 300 Step 1380/5304 Paf1 785.039307 Paf2 722.419434 Paf3 728.818237 Heatmap 198.988281 Total loss 2435.26514\n",
      "Epoch 300 Step 1390/5304 Paf1 597.172119 Paf2 555.118164 Paf3 548.919556 Heatmap 191.07605 Total loss 1892.28589\n",
      "Epoch 300 Step 1400/5304 Paf1 1106.02271 Paf2 1024.68884 Paf3 1046.08435 Heatmap 350.217682 Total loss 3527.01343\n",
      "Epoch 300 Step 1410/5304 Paf1 837.309204 Paf2 791.307861 Paf3 811.039307 Heatmap 261.910706 Total loss 2701.56689\n",
      "Epoch 300 Step 1420/5304 Paf1 699.871887 Paf2 672.809 Paf3 660.899658 Heatmap 213.397964 Total loss 2246.97852\n",
      "Epoch 300 Step 1430/5304 Paf1 705.066589 Paf2 646.025208 Paf3 623.830688 Heatmap 205.953796 Total loss 2180.87622\n",
      "Epoch 300 Step 1440/5304 Paf1 822.586426 Paf2 774.361267 Paf3 771.813599 Heatmap 261.664368 Total loss 2630.42578\n",
      "Epoch 300 Step 1450/5304 Paf1 686.798218 Paf2 651.387756 Paf3 644.652832 Heatmap 208.726822 Total loss 2191.56567\n",
      "Epoch 300 Step 1460/5304 Paf1 370.133423 Paf2 347.606232 Paf3 340.966919 Heatmap 115.325851 Total loss 1174.03247\n",
      "Epoch 300 Step 1470/5304 Paf1 712.632629 Paf2 662.444824 Paf3 674.179688 Heatmap 263.828979 Total loss 2313.08594\n",
      "Epoch 300 Step 1480/5304 Paf1 769.856873 Paf2 745.88208 Paf3 753.243347 Heatmap 260.397461 Total loss 2529.38\n",
      "Epoch 300 Step 1490/5304 Paf1 767.522095 Paf2 723.342896 Paf3 724.603088 Heatmap 207.943054 Total loss 2423.41113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300 Step 1500/5304 Paf1 486.029358 Paf2 460.357391 Paf3 462.806152 Heatmap 116.500725 Total loss 1525.6936\n",
      "Epoch 300 Step 1510/5304 Paf1 751.022461 Paf2 714.822449 Paf3 726.784058 Heatmap 260.349426 Total loss 2452.97852\n",
      "Epoch 300 Step 1520/5304 Paf1 698.498901 Paf2 613.871399 Paf3 637.348206 Heatmap 176.282532 Total loss 2126.00098\n",
      "Epoch 300 Step 1530/5304 Paf1 585.357483 Paf2 550.083557 Paf3 547.813232 Heatmap 161.773911 Total loss 1845.0282\n",
      "Epoch 300 Step 1540/5304 Paf1 717.916321 Paf2 678.649963 Paf3 691.446289 Heatmap 246.862885 Total loss 2334.87549\n",
      "Epoch 300 Step 1550/5304 Paf1 643.130371 Paf2 619.313171 Paf3 619.69104 Heatmap 227.71756 Total loss 2109.85205\n",
      "Epoch 300 Step 1560/5304 Paf1 844.26416 Paf2 788.985107 Paf3 789.452393 Heatmap 229.73407 Total loss 2652.43579\n",
      "Epoch 300 Step 1570/5304 Paf1 602.204468 Paf2 563.220215 Paf3 543.174805 Heatmap 190.596039 Total loss 1899.19556\n",
      "Epoch 300 Step 1580/5304 Paf1 702.049561 Paf2 644.733398 Paf3 637.116089 Heatmap 250.9487 Total loss 2234.84766\n",
      "Epoch 300 Step 1590/5304 Paf1 1094.75256 Paf2 1064.35095 Paf3 1051.41711 Heatmap 372.697266 Total loss 3583.21777\n",
      "Epoch 300 Step 1600/5304 Paf1 756.424744 Paf2 728.179565 Paf3 713.668152 Heatmap 230.112671 Total loss 2428.38501\n",
      "Epoch 300 Step 1610/5304 Paf1 733.643799 Paf2 709.669922 Paf3 693.620422 Heatmap 210.04454 Total loss 2346.97876\n",
      "Epoch 300 Step 1620/5304 Paf1 842.667847 Paf2 785.945801 Paf3 802.463806 Heatmap 251.019409 Total loss 2682.09668\n",
      "Epoch 300 Step 1630/5304 Paf1 868.187378 Paf2 810.980774 Paf3 799.719116 Heatmap 271.345093 Total loss 2750.23242\n",
      "Epoch 300 Step 1640/5304 Paf1 647.104858 Paf2 595.275635 Paf3 611.31488 Heatmap 196.910736 Total loss 2050.60596\n",
      "Epoch 300 Step 1650/5304 Paf1 500.186279 Paf2 477.806152 Paf3 477.974365 Heatmap 162.799515 Total loss 1618.76636\n",
      "Epoch 300 Step 1660/5304 Paf1 603.142395 Paf2 555.781372 Paf3 552.957458 Heatmap 207.501022 Total loss 1919.38232\n",
      "Epoch 300 Step 1670/5304 Paf1 532.792297 Paf2 502.605 Paf3 507.951416 Heatmap 149.381317 Total loss 1692.7301\n",
      "Epoch 300 Step 1680/5304 Paf1 608.361 Paf2 570.34967 Paf3 554.125671 Heatmap 196.204971 Total loss 1929.04126\n",
      "Epoch 300 Step 1690/5304 Paf1 690.296631 Paf2 637.209229 Paf3 634.270691 Heatmap 246.27359 Total loss 2208.05029\n",
      "Epoch 300 Step 1700/5304 Paf1 671.972168 Paf2 632.911438 Paf3 623.247 Heatmap 210.740173 Total loss 2138.87061\n",
      "Epoch 300 Step 1710/5304 Paf1 608.259644 Paf2 557.203613 Paf3 565.887085 Heatmap 195.832825 Total loss 1927.18311\n",
      "Epoch 300 Step 1720/5304 Paf1 860.495605 Paf2 787.825073 Paf3 779.24054 Heatmap 240.935226 Total loss 2668.49658\n",
      "Epoch 300 Step 1730/5304 Paf1 749.350098 Paf2 713.973 Paf3 696.777954 Heatmap 244.71933 Total loss 2404.82031\n",
      "Epoch 300 Step 1740/5304 Paf1 654.826111 Paf2 600.516907 Paf3 603.187866 Heatmap 203.281097 Total loss 2061.81201\n",
      "Epoch 300 Step 1750/5304 Paf1 682.198242 Paf2 660.589905 Paf3 639.029785 Heatmap 227.688019 Total loss 2209.50586\n",
      "Epoch 300 Step 1760/5304 Paf1 749.639832 Paf2 724.2052 Paf3 706.364807 Heatmap 268.148315 Total loss 2448.35815\n",
      "Epoch 300 Step 1770/5304 Paf1 749.644592 Paf2 723.456055 Paf3 711.867798 Heatmap 252.376724 Total loss 2437.34521\n",
      "Epoch 300 Step 1780/5304 Paf1 840.198486 Paf2 813.539795 Paf3 823.655457 Heatmap 369.663513 Total loss 2847.05713\n",
      "Epoch 300 Step 1790/5304 Paf1 914.678101 Paf2 864.237549 Paf3 862.870483 Heatmap 274.739166 Total loss 2916.52539\n",
      "Epoch 300 Step 1800/5304 Paf1 507.610565 Paf2 477.596405 Paf3 471.320251 Heatmap 149.055862 Total loss 1605.58301\n",
      "Epoch 300 Step 1810/5304 Paf1 892.222961 Paf2 874.599915 Paf3 863.587036 Heatmap 310.907898 Total loss 2941.31787\n",
      "Epoch 300 Step 1820/5304 Paf1 621.122498 Paf2 578.583557 Paf3 583.169556 Heatmap 224.917953 Total loss 2007.79358\n",
      "Epoch 300 Step 1830/5304 Paf1 700.337952 Paf2 667.361694 Paf3 664.857788 Heatmap 240.555237 Total loss 2273.11279\n",
      "Epoch 300 Step 1840/5304 Paf1 622.756714 Paf2 589.592896 Paf3 588.272034 Heatmap 194.675812 Total loss 1995.29749\n",
      "Epoch 300 Step 1850/5304 Paf1 692.535217 Paf2 660.698914 Paf3 648.407532 Heatmap 209.876678 Total loss 2211.51831\n",
      "Epoch 300 Step 1860/5304 Paf1 564.086914 Paf2 521.27356 Paf3 508.027222 Heatmap 162.232651 Total loss 1755.62036\n",
      "Epoch 300 Step 1870/5304 Paf1 1077.37866 Paf2 989.474 Paf3 1041.85925 Heatmap 316.339355 Total loss 3425.05127\n",
      "Epoch 300 Step 1880/5304 Paf1 697.160583 Paf2 650.51825 Paf3 656.956238 Heatmap 245.473663 Total loss 2250.10889\n",
      "Epoch 300 Step 1890/5304 Paf1 345.584137 Paf2 332.592072 Paf3 326.752655 Heatmap 104.534378 Total loss 1109.46326\n",
      "Epoch 300 Step 1900/5304 Paf1 684.667969 Paf2 649.961 Paf3 639.634399 Heatmap 225.485519 Total loss 2199.74878\n",
      "Epoch 300 Step 1910/5304 Paf1 661.11084 Paf2 617.833618 Paf3 608.175537 Heatmap 202.537964 Total loss 2089.65796\n",
      "Epoch 300 Step 1920/5304 Paf1 803.260681 Paf2 761.045776 Paf3 738.832 Heatmap 229.921814 Total loss 2533.06\n",
      "Epoch 300 Step 1930/5304 Paf1 964.754089 Paf2 903.704285 Paf3 919.530212 Heatmap 337.334351 Total loss 3125.32275\n",
      "Epoch 300 Step 1940/5304 Paf1 858.977539 Paf2 803.771729 Paf3 809.643494 Heatmap 299.798279 Total loss 2772.19092\n",
      "Epoch 300 Step 1950/5304 Paf1 833.056152 Paf2 759.240234 Paf3 758.988 Heatmap 244.722595 Total loss 2596.00684\n",
      "Epoch 300 Step 1960/5304 Paf1 548.699646 Paf2 497.689392 Paf3 497.067657 Heatmap 158.878296 Total loss 1702.33496\n",
      "Epoch 300 Step 1970/5304 Paf1 691.474121 Paf2 641.643738 Paf3 645.573303 Heatmap 222.260254 Total loss 2200.95142\n",
      "Epoch 300 Step 1980/5304 Paf1 728.77356 Paf2 667.978516 Paf3 675.644775 Heatmap 198.023743 Total loss 2270.42065\n",
      "Epoch 300 Step 1990/5304 Paf1 967.903442 Paf2 930.430847 Paf3 950.189697 Heatmap 290.892456 Total loss 3139.4165\n",
      "Epoch 300 Step 2000/5304 Paf1 612.149414 Paf2 574.907654 Paf3 569.927673 Heatmap 174.838058 Total loss 1931.82288\n",
      "Saved checkpoint for step 2000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1796\n",
      "Epoch 300 Step 2010/5304 Paf1 753.754272 Paf2 688.566956 Paf3 700.603455 Heatmap 229.87384 Total loss 2372.79858\n",
      "Epoch 300 Step 2020/5304 Paf1 445.717834 Paf2 416.09964 Paf3 441.052307 Heatmap 135.453903 Total loss 1438.32373\n",
      "Epoch 300 Step 2030/5304 Paf1 631.580444 Paf2 563.905884 Paf3 578.393 Heatmap 157.342804 Total loss 1931.22217\n",
      "Epoch 300 Step 2040/5304 Paf1 579.425293 Paf2 551.851562 Paf3 550.269775 Heatmap 174.868408 Total loss 1856.41504\n",
      "Epoch 300 Step 2050/5304 Paf1 811.877319 Paf2 756.469604 Paf3 757.333862 Heatmap 277.538 Total loss 2603.21875\n",
      "Epoch 300 Step 2060/5304 Paf1 639.702087 Paf2 591.457458 Paf3 587.995728 Heatmap 158.541122 Total loss 1977.69641\n",
      "Epoch 300 Step 2070/5304 Paf1 699.496216 Paf2 666.035828 Paf3 654.004333 Heatmap 278.457611 Total loss 2297.9939\n",
      "Epoch 300 Step 2080/5304 Paf1 656.848694 Paf2 610.419128 Paf3 602.36853 Heatmap 188.959717 Total loss 2058.59619\n",
      "Epoch 300 Step 2090/5304 Paf1 717.705933 Paf2 684.665161 Paf3 669.117493 Heatmap 256.008942 Total loss 2327.49756\n",
      "Epoch 300 Step 2100/5304 Paf1 618.612915 Paf2 568.237549 Paf3 569.301086 Heatmap 190.115036 Total loss 1946.2666\n",
      "Epoch 300 Step 2110/5304 Paf1 706.121765 Paf2 681.885376 Paf3 685.929749 Heatmap 207.27594 Total loss 2281.21289\n",
      "Epoch 300 Step 2120/5304 Paf1 615.199829 Paf2 576.811035 Paf3 572.511 Heatmap 158.835587 Total loss 1923.35742\n",
      "Epoch 300 Step 2130/5304 Paf1 985.414 Paf2 956.263916 Paf3 940.588501 Heatmap 344.511017 Total loss 3226.77734\n",
      "Epoch 300 Step 2140/5304 Paf1 853.187256 Paf2 823.251282 Paf3 808.760925 Heatmap 266.592773 Total loss 2751.79224\n",
      "Epoch 300 Step 2150/5304 Paf1 562.119385 Paf2 521.507568 Paf3 501.955841 Heatmap 153.932159 Total loss 1739.51489\n",
      "Epoch 300 Step 2160/5304 Paf1 438.060242 Paf2 408.124695 Paf3 412.335144 Heatmap 128.975723 Total loss 1387.49585\n",
      "Epoch 300 Step 2170/5304 Paf1 742.832581 Paf2 706.795288 Paf3 698.930847 Heatmap 236.288025 Total loss 2384.84668\n",
      "Epoch 300 Step 2180/5304 Paf1 624.818237 Paf2 599.57019 Paf3 599.915588 Heatmap 205.615021 Total loss 2029.91907\n",
      "Epoch 300 Step 2190/5304 Paf1 511.733398 Paf2 479.958313 Paf3 467.992432 Heatmap 135.547882 Total loss 1595.23193\n",
      "Epoch 300 Step 2200/5304 Paf1 958.916931 Paf2 897.865845 Paf3 887.788147 Heatmap 301.520844 Total loss 3046.0918\n",
      "Epoch 300 Step 2210/5304 Paf1 631.832092 Paf2 596.173828 Paf3 602.299683 Heatmap 222.37735 Total loss 2052.68286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300 Step 2220/5304 Paf1 854.885925 Paf2 843.037231 Paf3 845.555664 Heatmap 250.104614 Total loss 2793.5835\n",
      "Epoch 300 Step 2230/5304 Paf1 632.508301 Paf2 607.200745 Paf3 608.652 Heatmap 161.166977 Total loss 2009.52795\n",
      "Epoch 300 Step 2240/5304 Paf1 639.206299 Paf2 611.589111 Paf3 602.768311 Heatmap 165.110077 Total loss 2018.67383\n",
      "Epoch 300 Step 2250/5304 Paf1 818.838684 Paf2 771.856079 Paf3 783.602417 Heatmap 264.492432 Total loss 2638.78955\n",
      "Epoch 300 Step 2260/5304 Paf1 646.628 Paf2 603.310242 Paf3 626.097778 Heatmap 193.013641 Total loss 2069.0498\n",
      "Epoch 300 Step 2270/5304 Paf1 774.180969 Paf2 714.332764 Paf3 726.553833 Heatmap 264.938965 Total loss 2480.00635\n",
      "Epoch 300 Step 2280/5304 Paf1 890.679077 Paf2 836.674622 Paf3 838.78363 Heatmap 292.04068 Total loss 2858.17822\n",
      "Epoch 300 Step 2290/5304 Paf1 952.024231 Paf2 892.921936 Paf3 897.112793 Heatmap 273.319122 Total loss 3015.37793\n",
      "Epoch 300 Step 2300/5304 Paf1 513.031616 Paf2 496.665405 Paf3 495.210419 Heatmap 156.274963 Total loss 1661.18237\n",
      "Epoch 300 Step 2310/5304 Paf1 548.001709 Paf2 526.4151 Paf3 527.540771 Heatmap 141.278381 Total loss 1743.23584\n",
      "Epoch 300 Step 2320/5304 Paf1 593.932556 Paf2 549.357849 Paf3 531.384766 Heatmap 149.00705 Total loss 1823.68225\n",
      "Epoch 300 Step 2330/5304 Paf1 575.270081 Paf2 544.363342 Paf3 561.932373 Heatmap 168.951172 Total loss 1850.51697\n",
      "Epoch 300 Step 2340/5304 Paf1 444.482941 Paf2 420.4953 Paf3 418.302399 Heatmap 110.5578 Total loss 1393.8385\n",
      "Epoch 300 Step 2350/5304 Paf1 478.261108 Paf2 444.721375 Paf3 434.109711 Heatmap 134.185745 Total loss 1491.27795\n",
      "Epoch 300 Step 2360/5304 Paf1 443.769073 Paf2 407.611389 Paf3 402.688019 Heatmap 133.102249 Total loss 1387.17078\n",
      "Epoch 300 Step 2370/5304 Paf1 771.810669 Paf2 738.956177 Paf3 727.851562 Heatmap 236.670471 Total loss 2475.28882\n",
      "Epoch 300 Step 2380/5304 Paf1 921.591614 Paf2 888.959595 Paf3 883.583069 Heatmap 288.326294 Total loss 2982.46069\n",
      "Epoch 300 Step 2390/5304 Paf1 577.879456 Paf2 530.813538 Paf3 543.080505 Heatmap 166.18222 Total loss 1817.95569\n",
      "Epoch 300 Step 2400/5304 Paf1 556.379272 Paf2 510.964264 Paf3 506.539612 Heatmap 149.301178 Total loss 1723.18433\n",
      "Epoch 300 Step 2410/5304 Paf1 850.150879 Paf2 802.907104 Paf3 772.859314 Heatmap 271.529938 Total loss 2697.44727\n",
      "Epoch 300 Step 2420/5304 Paf1 499.383453 Paf2 449.345795 Paf3 440.756104 Heatmap 113.486496 Total loss 1502.97192\n",
      "Epoch 300 Step 2430/5304 Paf1 1121.08826 Paf2 1074.22974 Paf3 1066.13953 Heatmap 399.037598 Total loss 3660.49512\n",
      "Epoch 300 Step 2440/5304 Paf1 871.179138 Paf2 820.791809 Paf3 828.529907 Heatmap 250.693237 Total loss 2771.19409\n",
      "Epoch 300 Step 2450/5304 Paf1 573.642395 Paf2 540.198303 Paf3 545.457703 Heatmap 180.889496 Total loss 1840.18787\n",
      "Epoch 300 Step 2460/5304 Paf1 526.112305 Paf2 493.499908 Paf3 499.63385 Heatmap 155.097992 Total loss 1674.34399\n",
      "Epoch 300 Step 2470/5304 Paf1 807.393677 Paf2 773.182495 Paf3 767.503113 Heatmap 265.064148 Total loss 2613.14355\n",
      "Epoch 300 Step 2480/5304 Paf1 853.006409 Paf2 819.557068 Paf3 818.298645 Heatmap 280.985199 Total loss 2771.84717\n",
      "Epoch 300 Step 2490/5304 Paf1 642.22821 Paf2 612.264282 Paf3 610.942444 Heatmap 224.77742 Total loss 2090.2124\n",
      "Epoch 300 Step 2500/5304 Paf1 428.657806 Paf2 383.350983 Paf3 382.343231 Heatmap 117.875214 Total loss 1312.22729\n",
      "Epoch 300 Step 2510/5304 Paf1 362.773254 Paf2 335.703247 Paf3 341.217407 Heatmap 96.30439 Total loss 1135.99829\n",
      "Epoch 300 Step 2520/5304 Paf1 608.5896 Paf2 581.186035 Paf3 576.30957 Heatmap 201.409134 Total loss 1967.49438\n",
      "Epoch 300 Step 2530/5304 Paf1 938.869385 Paf2 911.748352 Paf3 908.82312 Heatmap 295.267578 Total loss 3054.7085\n",
      "Epoch 300 Step 2540/5304 Paf1 711.834229 Paf2 661.987122 Paf3 655.043335 Heatmap 226.519012 Total loss 2255.38379\n",
      "Epoch 300 Step 2550/5304 Paf1 635.792786 Paf2 571.391785 Paf3 582.728577 Heatmap 195.030106 Total loss 1984.94324\n",
      "Epoch 300 Step 2560/5304 Paf1 793.047119 Paf2 759.07373 Paf3 727.065613 Heatmap 292.938599 Total loss 2572.125\n",
      "Epoch 300 Step 2570/5304 Paf1 723.983826 Paf2 691.59137 Paf3 687.290955 Heatmap 184.116028 Total loss 2286.98218\n",
      "Epoch 300 Step 2580/5304 Paf1 545.984253 Paf2 495.84964 Paf3 485.185059 Heatmap 156.706604 Total loss 1683.72559\n",
      "Epoch 300 Step 2590/5304 Paf1 793.505432 Paf2 757.458 Paf3 776.073608 Heatmap 279.652222 Total loss 2606.68921\n",
      "Epoch 300 Step 2600/5304 Paf1 776.465 Paf2 711.641052 Paf3 726.338135 Heatmap 203.517868 Total loss 2417.96216\n",
      "Epoch 300 Step 2610/5304 Paf1 518.19873 Paf2 481.854065 Paf3 473.542084 Heatmap 164.483978 Total loss 1638.07886\n",
      "Epoch 300 Step 2620/5304 Paf1 543.985596 Paf2 468.394897 Paf3 484.849518 Heatmap 145.294891 Total loss 1642.5249\n",
      "Epoch 300 Step 2630/5304 Paf1 643.131348 Paf2 607.940369 Paf3 623.468689 Heatmap 173.282898 Total loss 2047.82336\n",
      "Epoch 300 Step 2640/5304 Paf1 749.956299 Paf2 723.3396 Paf3 727.458435 Heatmap 223.863968 Total loss 2424.61816\n",
      "Epoch 300 Step 2650/5304 Paf1 709.191162 Paf2 648.408142 Paf3 649.779236 Heatmap 204.187897 Total loss 2211.56641\n",
      "Epoch 300 Step 2660/5304 Paf1 481.814209 Paf2 454.461 Paf3 465.158173 Heatmap 154.569199 Total loss 1556.00256\n",
      "Epoch 300 Step 2670/5304 Paf1 707.958679 Paf2 662.814514 Paf3 659.870911 Heatmap 269.228088 Total loss 2299.87207\n",
      "Epoch 300 Step 2680/5304 Paf1 870.27179 Paf2 788.671204 Paf3 803.430786 Heatmap 272.571198 Total loss 2734.94482\n",
      "Epoch 300 Step 2690/5304 Paf1 955.874451 Paf2 881.421692 Paf3 899.487732 Heatmap 263.375153 Total loss 3000.15918\n",
      "Epoch 300 Step 2700/5304 Paf1 478.995056 Paf2 453.312683 Paf3 433.364044 Heatmap 127.766693 Total loss 1493.43848\n",
      "Epoch 300 Step 2710/5304 Paf1 669.721313 Paf2 629.996948 Paf3 629.536 Heatmap 196.844223 Total loss 2126.09863\n",
      "Epoch 300 Step 2720/5304 Paf1 1065.47656 Paf2 1036.68164 Paf3 1060.6709 Heatmap 356.34845 Total loss 3519.17749\n",
      "Epoch 300 Step 2730/5304 Paf1 783.440918 Paf2 715.669189 Paf3 719.521 Heatmap 224.442596 Total loss 2443.07373\n",
      "Epoch 300 Step 2740/5304 Paf1 498.48703 Paf2 470.740509 Paf3 454.046417 Heatmap 133.34668 Total loss 1556.62061\n",
      "Epoch 300 Step 2750/5304 Paf1 521.592407 Paf2 488.550293 Paf3 500.179871 Heatmap 152.173462 Total loss 1662.49609\n",
      "Epoch 300 Step 2760/5304 Paf1 516.940735 Paf2 497.10968 Paf3 490.750824 Heatmap 171.491089 Total loss 1676.29236\n",
      "Epoch 300 Step 2770/5304 Paf1 435.305 Paf2 402.916626 Paf3 407.662445 Heatmap 127.736992 Total loss 1373.62109\n",
      "Epoch 300 Step 2780/5304 Paf1 738.321655 Paf2 702.074402 Paf3 698.881104 Heatmap 212.184204 Total loss 2351.46143\n",
      "Epoch 300 Step 2790/5304 Paf1 611.055 Paf2 540.028503 Paf3 559.402527 Heatmap 183.298218 Total loss 1893.78418\n",
      "Epoch 300 Step 2800/5304 Paf1 433.902527 Paf2 414.425751 Paf3 392.006287 Heatmap 122.962791 Total loss 1363.29736\n",
      "Epoch 300 Step 2810/5304 Paf1 557.718 Paf2 513.59729 Paf3 509.226532 Heatmap 148.00354 Total loss 1728.54541\n",
      "Epoch 300 Step 2820/5304 Paf1 961.997375 Paf2 920.104797 Paf3 930.023254 Heatmap 279.599915 Total loss 3091.72534\n",
      "Epoch 300 Step 2830/5304 Paf1 797.400391 Paf2 746.831909 Paf3 769.083374 Heatmap 239.061859 Total loss 2552.37744\n",
      "Epoch 300 Step 2840/5304 Paf1 796.665466 Paf2 728.767334 Paf3 746.308228 Heatmap 232.500946 Total loss 2504.24219\n",
      "Epoch 300 Step 2850/5304 Paf1 548.74707 Paf2 503.559 Paf3 503.128418 Heatmap 176.361252 Total loss 1731.79565\n",
      "Epoch 300 Step 2860/5304 Paf1 867.687439 Paf2 819.304871 Paf3 811.457275 Heatmap 241.463165 Total loss 2739.9126\n",
      "Epoch 300 Step 2870/5304 Paf1 706.49707 Paf2 675.460449 Paf3 667.183899 Heatmap 198.766327 Total loss 2247.90771\n",
      "Epoch 300 Step 2880/5304 Paf1 640.978271 Paf2 597.073425 Paf3 601.530334 Heatmap 153.56778 Total loss 1993.1499\n",
      "Epoch 300 Step 2890/5304 Paf1 608.3573 Paf2 580.844055 Paf3 562.019775 Heatmap 187.857483 Total loss 1939.07861\n",
      "Epoch 300 Step 2900/5304 Paf1 538.09137 Paf2 486.739685 Paf3 498.933105 Heatmap 170.620087 Total loss 1694.38428\n",
      "Epoch 300 Step 2910/5304 Paf1 645.21405 Paf2 608.363342 Paf3 622.249756 Heatmap 237.835022 Total loss 2113.66211\n",
      "Epoch 300 Step 2920/5304 Paf1 829.505127 Paf2 763.737732 Paf3 762.378052 Heatmap 258.090332 Total loss 2613.71143\n",
      "Epoch 300 Step 2930/5304 Paf1 616.984192 Paf2 590.646545 Paf3 590.503052 Heatmap 214.364487 Total loss 2012.49829\n",
      "Epoch 300 Step 2940/5304 Paf1 764.582275 Paf2 718.118042 Paf3 722.11554 Heatmap 241.925629 Total loss 2446.74146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300 Step 2950/5304 Paf1 672.858765 Paf2 651.500366 Paf3 650.704163 Heatmap 244.141266 Total loss 2219.20459\n",
      "Epoch 300 Step 2960/5304 Paf1 788.179688 Paf2 728.44989 Paf3 726.145813 Heatmap 251.521713 Total loss 2494.29712\n",
      "Epoch 300 Step 2970/5304 Paf1 550.433 Paf2 510.263855 Paf3 493.07608 Heatmap 151.33783 Total loss 1705.11072\n",
      "Epoch 300 Step 2980/5304 Paf1 711.778 Paf2 684.02655 Paf3 657.220032 Heatmap 181.383316 Total loss 2234.40796\n",
      "Epoch 300 Step 2990/5304 Paf1 695.574 Paf2 685.439758 Paf3 682.006226 Heatmap 224.562378 Total loss 2287.58228\n",
      "Epoch 300 Step 3000/5304 Paf1 538.978394 Paf2 488.60437 Paf3 493.18042 Heatmap 155.013763 Total loss 1675.77698\n",
      "Saved checkpoint for step 3000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1797\n",
      "Epoch 300 Step 3010/5304 Paf1 465.6185 Paf2 425.193115 Paf3 437.584595 Heatmap 129.937195 Total loss 1458.3335\n",
      "Epoch 300 Step 3020/5304 Paf1 671.998535 Paf2 603.172424 Paf3 622.85675 Heatmap 211.95697 Total loss 2109.98462\n",
      "Epoch 300 Step 3030/5304 Paf1 672.248 Paf2 632.522034 Paf3 622.902344 Heatmap 239.555649 Total loss 2167.22803\n",
      "Epoch 300 Step 3040/5304 Paf1 573.99408 Paf2 538.012756 Paf3 546.529724 Heatmap 175.507416 Total loss 1834.04395\n",
      "Epoch 300 Step 3050/5304 Paf1 518.512085 Paf2 498.553741 Paf3 504.375824 Heatmap 174.191528 Total loss 1695.63318\n",
      "Epoch 300 Step 3060/5304 Paf1 813.305725 Paf2 722.893494 Paf3 728.907166 Heatmap 208.990265 Total loss 2474.09668\n",
      "Epoch 300 Step 3070/5304 Paf1 642.333801 Paf2 588.425781 Paf3 602.901 Heatmap 184.195663 Total loss 2017.8562\n",
      "Epoch 300 Step 3080/5304 Paf1 652.892 Paf2 622.71521 Paf3 598.204895 Heatmap 186.566498 Total loss 2060.37842\n",
      "Epoch 300 Step 3090/5304 Paf1 508.4646 Paf2 465.849 Paf3 466.753723 Heatmap 146.082565 Total loss 1587.1499\n",
      "Epoch 300 Step 3100/5304 Paf1 701.4823 Paf2 667.608398 Paf3 681.728455 Heatmap 228.817444 Total loss 2279.63672\n",
      "Epoch 300 Step 3110/5304 Paf1 648.760254 Paf2 614.486328 Paf3 609.510559 Heatmap 186.709534 Total loss 2059.4668\n",
      "Epoch 300 Step 3120/5304 Paf1 809.108398 Paf2 766.397339 Paf3 772.242432 Heatmap 302.59549 Total loss 2650.34375\n",
      "Epoch 300 Step 3130/5304 Paf1 486.303772 Paf2 417.874359 Paf3 417.450073 Heatmap 119.659256 Total loss 1441.28735\n",
      "Epoch 300 Step 3140/5304 Paf1 548.294312 Paf2 519.309204 Paf3 535.984253 Heatmap 185.334564 Total loss 1788.92236\n",
      "Epoch 300 Step 3150/5304 Paf1 574.053528 Paf2 550.014099 Paf3 545.928711 Heatmap 162.600708 Total loss 1832.59705\n",
      "Epoch 300 Step 3160/5304 Paf1 602.327271 Paf2 554.773193 Paf3 556.073181 Heatmap 149.442047 Total loss 1862.61572\n",
      "Epoch 300 Step 3170/5304 Paf1 651.415833 Paf2 614.155151 Paf3 622.488281 Heatmap 202.394897 Total loss 2090.4541\n",
      "Epoch 300 Step 3180/5304 Paf1 720.962 Paf2 688.203308 Paf3 687.816895 Heatmap 237.81015 Total loss 2334.79248\n",
      "Epoch 300 Step 3190/5304 Paf1 867.07 Paf2 827.094849 Paf3 813.9375 Heatmap 264.620819 Total loss 2772.72314\n",
      "Epoch 300 Step 3200/5304 Paf1 795.864 Paf2 740.455078 Paf3 746.122498 Heatmap 329.053436 Total loss 2611.49512\n",
      "Epoch 300 Step 3210/5304 Paf1 544.513123 Paf2 502.817932 Paf3 507.536407 Heatmap 143.763351 Total loss 1698.63086\n",
      "Epoch 300 Step 3220/5304 Paf1 680.308289 Paf2 634.201233 Paf3 641.207825 Heatmap 199.227 Total loss 2154.94434\n",
      "Epoch 300 Step 3230/5304 Paf1 493.589447 Paf2 452.377686 Paf3 451.873322 Heatmap 155.264252 Total loss 1553.10474\n",
      "Epoch 300 Step 3240/5304 Paf1 445.635803 Paf2 410.980286 Paf3 411.574768 Heatmap 110.964935 Total loss 1379.15576\n",
      "Epoch 300 Step 3250/5304 Paf1 689.576721 Paf2 648.068726 Paf3 649.139954 Heatmap 208.135651 Total loss 2194.92114\n",
      "Epoch 300 Step 3260/5304 Paf1 665.643188 Paf2 626.113892 Paf3 640.216064 Heatmap 220.590088 Total loss 2152.56323\n",
      "Epoch 300 Step 3270/5304 Paf1 642.061646 Paf2 598.046631 Paf3 602.079041 Heatmap 200.685211 Total loss 2042.87256\n",
      "Epoch 300 Step 3280/5304 Paf1 802.750183 Paf2 758.667725 Paf3 761.502502 Heatmap 187.416153 Total loss 2510.33667\n",
      "Epoch 300 Step 3290/5304 Paf1 727.454407 Paf2 694.974548 Paf3 682.676575 Heatmap 188.722076 Total loss 2293.82764\n",
      "Epoch 300 Step 3300/5304 Paf1 624.778 Paf2 541.939148 Paf3 569.396484 Heatmap 160.903748 Total loss 1897.01733\n",
      "Epoch 300 Step 3310/5304 Paf1 532.197632 Paf2 484.167145 Paf3 497.225861 Heatmap 148.39151 Total loss 1661.98218\n",
      "Epoch 300 Step 3320/5304 Paf1 529.907471 Paf2 498.813721 Paf3 510.604828 Heatmap 164.570435 Total loss 1703.89648\n",
      "Epoch 300 Step 3330/5304 Paf1 522.962 Paf2 488.510498 Paf3 479.565186 Heatmap 175.118225 Total loss 1666.15588\n",
      "Epoch 300 Step 3340/5304 Paf1 665.53064 Paf2 639.271545 Paf3 639.993835 Heatmap 202.5578 Total loss 2147.354\n",
      "Epoch 300 Step 3350/5304 Paf1 658.093689 Paf2 597.378418 Paf3 598.686768 Heatmap 218.287643 Total loss 2072.44653\n",
      "Epoch 300 Step 3360/5304 Paf1 607.458557 Paf2 595.462769 Paf3 583.404419 Heatmap 174.719086 Total loss 1961.04492\n",
      "Epoch 300 Step 3370/5304 Paf1 546.945862 Paf2 484.088043 Paf3 494.835724 Heatmap 151.145233 Total loss 1677.01489\n",
      "Epoch 300 Step 3380/5304 Paf1 775.041809 Paf2 708.811279 Paf3 699.902893 Heatmap 216.860336 Total loss 2400.61621\n",
      "Epoch 300 Step 3390/5304 Paf1 951.13324 Paf2 881.029358 Paf3 896.134888 Heatmap 300.399109 Total loss 3028.69653\n",
      "Epoch 300 Step 3400/5304 Paf1 755.831848 Paf2 710.872498 Paf3 700.488281 Heatmap 231.180969 Total loss 2398.37354\n",
      "Epoch 300 Step 3410/5304 Paf1 583.761719 Paf2 586.64386 Paf3 575.578857 Heatmap 199.847244 Total loss 1945.83154\n",
      "Epoch 300 Step 3420/5304 Paf1 574.627563 Paf2 523.500366 Paf3 524.007812 Heatmap 146.347244 Total loss 1768.48291\n",
      "Epoch 300 Step 3430/5304 Paf1 593.540649 Paf2 548.002625 Paf3 555.545593 Heatmap 178.11972 Total loss 1875.2085\n",
      "Epoch 300 Step 3440/5304 Paf1 787.686523 Paf2 763.180603 Paf3 745.102112 Heatmap 264.1492 Total loss 2560.11865\n",
      "Epoch 300 Step 3450/5304 Paf1 558.056091 Paf2 564.319641 Paf3 559.327271 Heatmap 165.746735 Total loss 1847.44971\n",
      "Epoch 300 Step 3460/5304 Paf1 658.64624 Paf2 601.375793 Paf3 605.464966 Heatmap 200.855347 Total loss 2066.34229\n",
      "Epoch 300 Step 3470/5304 Paf1 548.481201 Paf2 518.003479 Paf3 501.272858 Heatmap 165.477036 Total loss 1733.2345\n",
      "Epoch 300 Step 3480/5304 Paf1 599.356934 Paf2 556.024902 Paf3 558.886963 Heatmap 175.744019 Total loss 1890.01282\n",
      "Epoch 300 Step 3490/5304 Paf1 525.275208 Paf2 496.552734 Paf3 480.586029 Heatmap 146.753769 Total loss 1649.16772\n",
      "Epoch 300 Step 3500/5304 Paf1 638.584473 Paf2 602.329407 Paf3 604.968262 Heatmap 209.671707 Total loss 2055.55371\n",
      "Epoch 300 Step 3510/5304 Paf1 734.986389 Paf2 689.133118 Paf3 708.126831 Heatmap 227.077621 Total loss 2359.32397\n",
      "Epoch 300 Step 3520/5304 Paf1 757.597168 Paf2 711.713074 Paf3 706.793945 Heatmap 233.59845 Total loss 2409.70264\n",
      "Epoch 300 Step 3530/5304 Paf1 773.366882 Paf2 721.095215 Paf3 724.765198 Heatmap 241.984558 Total loss 2461.21191\n",
      "Epoch 300 Step 3540/5304 Paf1 743.569458 Paf2 675.64917 Paf3 667.876587 Heatmap 246.527023 Total loss 2333.62231\n",
      "Epoch 300 Step 3550/5304 Paf1 784.304382 Paf2 728.18042 Paf3 730.803711 Heatmap 236.185364 Total loss 2479.47388\n",
      "Epoch 300 Step 3560/5304 Paf1 523.760315 Paf2 506.798218 Paf3 504.111053 Heatmap 163.062103 Total loss 1697.73169\n",
      "Epoch 300 Step 3570/5304 Paf1 555.112366 Paf2 538.581421 Paf3 534.503662 Heatmap 191.823639 Total loss 1820.02112\n",
      "Epoch 300 Step 3580/5304 Paf1 462.22464 Paf2 438.058136 Paf3 448.274567 Heatmap 119.806427 Total loss 1468.36377\n",
      "Epoch 300 Step 3590/5304 Paf1 518.951416 Paf2 477.659485 Paf3 476.808167 Heatmap 184.119278 Total loss 1657.53833\n",
      "Epoch 300 Step 3600/5304 Paf1 505.06485 Paf2 466.467957 Paf3 476.943176 Heatmap 130.868744 Total loss 1579.34473\n",
      "Epoch 300 Step 3610/5304 Paf1 744.833923 Paf2 709.510437 Paf3 718.657104 Heatmap 204.678497 Total loss 2377.68\n",
      "Epoch 300 Step 3620/5304 Paf1 719.758911 Paf2 687.184692 Paf3 680.546204 Heatmap 225.866852 Total loss 2313.35669\n",
      "Epoch 300 Step 3630/5304 Paf1 706.099121 Paf2 637.677185 Paf3 648.294556 Heatmap 178.070969 Total loss 2170.14185\n",
      "Epoch 300 Step 3640/5304 Paf1 699.83075 Paf2 649.155945 Paf3 653.261902 Heatmap 196.799881 Total loss 2199.04834\n",
      "Epoch 300 Step 3650/5304 Paf1 699.608093 Paf2 655.223267 Paf3 653.129456 Heatmap 219.671539 Total loss 2227.63232\n",
      "Epoch 300 Step 3660/5304 Paf1 630.790894 Paf2 574.68457 Paf3 567.05957 Heatmap 192.177032 Total loss 1964.71204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300 Step 3670/5304 Paf1 732.708069 Paf2 660.232666 Paf3 645.816284 Heatmap 228.208298 Total loss 2266.96533\n",
      "Epoch 300 Step 3680/5304 Paf1 713.24646 Paf2 673.137085 Paf3 682.014 Heatmap 190.455017 Total loss 2258.85254\n",
      "Epoch 300 Step 3690/5304 Paf1 594.954041 Paf2 576.927368 Paf3 562.063477 Heatmap 209.327042 Total loss 1943.27185\n",
      "Epoch 300 Step 3700/5304 Paf1 664.645142 Paf2 622.184082 Paf3 639.88562 Heatmap 265.028 Total loss 2191.74292\n",
      "Epoch 300 Step 3710/5304 Paf1 580.507751 Paf2 544.388184 Paf3 538.072144 Heatmap 166.786316 Total loss 1829.75439\n",
      "Epoch 300 Step 3720/5304 Paf1 707.27594 Paf2 687.502319 Paf3 674.938904 Heatmap 219.974701 Total loss 2289.69189\n",
      "Epoch 300 Step 3730/5304 Paf1 616.749268 Paf2 566.730103 Paf3 564.811279 Heatmap 163.572662 Total loss 1911.86328\n",
      "Epoch 300 Step 3740/5304 Paf1 651.635681 Paf2 613.514465 Paf3 611.333862 Heatmap 163.31488 Total loss 2039.79883\n",
      "Epoch 300 Step 3750/5304 Paf1 711.706299 Paf2 642.765686 Paf3 642.770264 Heatmap 216.172516 Total loss 2213.41455\n",
      "Epoch 300 Step 3760/5304 Paf1 967.021606 Paf2 926.252808 Paf3 928.574097 Heatmap 341.334747 Total loss 3163.18311\n",
      "Epoch 300 Step 3770/5304 Paf1 597.548401 Paf2 543.345581 Paf3 538.867493 Heatmap 169.021439 Total loss 1848.78296\n",
      "Epoch 300 Step 3780/5304 Paf1 499.059357 Paf2 458.569305 Paf3 463.625 Heatmap 154.890228 Total loss 1576.14392\n",
      "Epoch 300 Step 3790/5304 Paf1 727.172607 Paf2 659.625854 Paf3 685.896 Heatmap 208.137115 Total loss 2280.83154\n",
      "Epoch 300 Step 3800/5304 Paf1 515.988525 Paf2 485.616333 Paf3 476.296173 Heatmap 137.739136 Total loss 1615.64014\n",
      "Epoch 300 Step 3810/5304 Paf1 486.609802 Paf2 471.731201 Paf3 469.271942 Heatmap 117.073479 Total loss 1544.6864\n",
      "Epoch 300 Step 3820/5304 Paf1 742.948853 Paf2 716.123 Paf3 696.538574 Heatmap 271.239136 Total loss 2426.84961\n",
      "Epoch 300 Step 3830/5304 Paf1 927.973816 Paf2 853.975708 Paf3 848.274292 Heatmap 283.022949 Total loss 2913.24658\n",
      "Epoch 300 Step 3840/5304 Paf1 791.817505 Paf2 740.789368 Paf3 736.661499 Heatmap 278.203918 Total loss 2547.47241\n",
      "Epoch 300 Step 3850/5304 Paf1 783.414856 Paf2 739.241089 Paf3 745.573 Heatmap 199.811462 Total loss 2468.04053\n",
      "Epoch 300 Step 3860/5304 Paf1 428.659515 Paf2 400.334473 Paf3 414.153839 Heatmap 138.910034 Total loss 1382.05786\n",
      "Epoch 300 Step 3870/5304 Paf1 638.02771 Paf2 602.113525 Paf3 590.08429 Heatmap 180.794739 Total loss 2011.02026\n",
      "Epoch 300 Step 3880/5304 Paf1 575.987366 Paf2 534.369446 Paf3 531.527527 Heatmap 139.9841 Total loss 1781.86841\n",
      "Epoch 300 Step 3890/5304 Paf1 514.891846 Paf2 485.0495 Paf3 494.594086 Heatmap 154.892578 Total loss 1649.42798\n",
      "Epoch 300 Step 3900/5304 Paf1 554.407043 Paf2 512.243164 Paf3 497.711609 Heatmap 162.774414 Total loss 1727.13623\n",
      "Epoch 300 Step 3910/5304 Paf1 668.595215 Paf2 631.417236 Paf3 627.044434 Heatmap 211.294922 Total loss 2138.35181\n",
      "Epoch 300 Step 3920/5304 Paf1 471.478058 Paf2 439.135071 Paf3 444.367188 Heatmap 138.628464 Total loss 1493.60889\n",
      "Epoch 300 Step 3930/5304 Paf1 413.407104 Paf2 370.508026 Paf3 358.169952 Heatmap 107.845337 Total loss 1249.93042\n",
      "Epoch 300 Step 3940/5304 Paf1 609.202087 Paf2 580.110291 Paf3 567.926575 Heatmap 182.871048 Total loss 1940.11\n",
      "Epoch 300 Step 3950/5304 Paf1 507.740753 Paf2 485.525085 Paf3 482.818115 Heatmap 183.175446 Total loss 1659.2594\n",
      "Epoch 300 Step 3960/5304 Paf1 794.266418 Paf2 724.53186 Paf3 729.962341 Heatmap 250.089539 Total loss 2498.85\n",
      "Epoch 300 Step 3970/5304 Paf1 582.01416 Paf2 564.126 Paf3 554.795776 Heatmap 145.964325 Total loss 1846.90027\n",
      "Epoch 300 Step 3980/5304 Paf1 813.724487 Paf2 761.967 Paf3 760.275696 Heatmap 281.212585 Total loss 2617.17969\n",
      "Epoch 300 Step 3990/5304 Paf1 824.478 Paf2 776.748962 Paf3 773.996704 Heatmap 223.352142 Total loss 2598.57593\n",
      "Epoch 300 Step 4000/5304 Paf1 568.935059 Paf2 538.632874 Paf3 524.663818 Heatmap 155.788559 Total loss 1788.02026\n",
      "Saved checkpoint for step 4000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1798\n",
      "Epoch 300 Step 4010/5304 Paf1 614.519165 Paf2 600.096313 Paf3 581.863708 Heatmap 205.898453 Total loss 2002.37769\n",
      "Epoch 300 Step 4020/5304 Paf1 475.664795 Paf2 442.711151 Paf3 450.094299 Heatmap 165.700241 Total loss 1534.17053\n",
      "Epoch 300 Step 4030/5304 Paf1 813.735107 Paf2 759.376892 Paf3 759.69458 Heatmap 223.538864 Total loss 2556.34546\n",
      "Epoch 300 Step 4040/5304 Paf1 712.910828 Paf2 684.734192 Paf3 680.537781 Heatmap 201.538788 Total loss 2279.72168\n",
      "Epoch 300 Step 4050/5304 Paf1 916.646545 Paf2 847.77948 Paf3 860.373596 Heatmap 285.466248 Total loss 2910.26587\n",
      "Epoch 300 Step 4060/5304 Paf1 780.318054 Paf2 716.245667 Paf3 703.827698 Heatmap 253.875107 Total loss 2454.2666\n",
      "Epoch 300 Step 4070/5304 Paf1 985.148682 Paf2 982.902039 Paf3 941.648254 Heatmap 372.301636 Total loss 3282.00073\n",
      "Epoch 300 Step 4080/5304 Paf1 680.88031 Paf2 635.752441 Paf3 621.050293 Heatmap 210.276855 Total loss 2147.96\n",
      "Epoch 300 Step 4090/5304 Paf1 878.196289 Paf2 799.906067 Paf3 822.38385 Heatmap 222.632416 Total loss 2723.11865\n",
      "Epoch 300 Step 4100/5304 Paf1 1240.99414 Paf2 1163.99292 Paf3 1186.70361 Heatmap 371.305115 Total loss 3962.99585\n",
      "Epoch 300 Step 4110/5304 Paf1 581.315613 Paf2 516.413696 Paf3 538.247437 Heatmap 182.10965 Total loss 1818.0863\n",
      "Epoch 300 Step 4120/5304 Paf1 602.941406 Paf2 549.131165 Paf3 548.957031 Heatmap 190.511307 Total loss 1891.54077\n",
      "Epoch 300 Step 4130/5304 Paf1 888.769897 Paf2 865.864319 Paf3 866.168762 Heatmap 317.938477 Total loss 2938.74146\n",
      "Epoch 300 Step 4140/5304 Paf1 529.725586 Paf2 485.299896 Paf3 482.903 Heatmap 172.136139 Total loss 1670.0647\n",
      "Epoch 300 Step 4150/5304 Paf1 882.817322 Paf2 861.481689 Paf3 856.314758 Heatmap 247.169403 Total loss 2847.7832\n",
      "Epoch 300 Step 4160/5304 Paf1 625.009033 Paf2 598.871338 Paf3 603.266357 Heatmap 233.394547 Total loss 2060.54126\n",
      "Epoch 300 Step 4170/5304 Paf1 818.562561 Paf2 764.118164 Paf3 737.080444 Heatmap 224.564774 Total loss 2544.32593\n",
      "Epoch 300 Step 4180/5304 Paf1 709.046082 Paf2 633.324646 Paf3 628.18689 Heatmap 198.165039 Total loss 2168.72266\n",
      "Epoch 300 Step 4190/5304 Paf1 528.700256 Paf2 498.617432 Paf3 499.379517 Heatmap 170.535309 Total loss 1697.23242\n",
      "Epoch 300 Step 4200/5304 Paf1 603.152344 Paf2 583.802185 Paf3 565.717712 Heatmap 177.591904 Total loss 1930.26416\n",
      "Epoch 300 Step 4210/5304 Paf1 526.271179 Paf2 489.507874 Paf3 485.069977 Heatmap 169.262543 Total loss 1670.11157\n",
      "Epoch 300 Step 4220/5304 Paf1 448.259705 Paf2 418.530182 Paf3 420.040588 Heatmap 124.082443 Total loss 1410.91296\n",
      "Epoch 300 Step 4230/5304 Paf1 558.977478 Paf2 508.656189 Paf3 535.22937 Heatmap 147.840912 Total loss 1750.70398\n",
      "Epoch 300 Step 4240/5304 Paf1 721.7323 Paf2 674.274841 Paf3 660.063416 Heatmap 195.471786 Total loss 2251.54224\n",
      "Epoch 300 Step 4250/5304 Paf1 810.131897 Paf2 768.011169 Paf3 752.091431 Heatmap 237.720093 Total loss 2567.95459\n",
      "Epoch 300 Step 4260/5304 Paf1 734.738 Paf2 728.597229 Paf3 728.059509 Heatmap 251.43248 Total loss 2442.82715\n",
      "Epoch 300 Step 4270/5304 Paf1 647.907776 Paf2 604.283325 Paf3 592.809692 Heatmap 202.42868 Total loss 2047.42957\n",
      "Epoch 300 Step 4280/5304 Paf1 787.810913 Paf2 756.36853 Paf3 763.22644 Heatmap 247.573334 Total loss 2554.97925\n",
      "Epoch 300 Step 4290/5304 Paf1 606.811523 Paf2 569.485535 Paf3 573.465698 Heatmap 177.045563 Total loss 1926.80835\n",
      "Epoch 300 Step 4300/5304 Paf1 615.553772 Paf2 557.212 Paf3 569.552551 Heatmap 170.117081 Total loss 1912.4353\n",
      "Epoch 300 Step 4310/5304 Paf1 710.085205 Paf2 671.426208 Paf3 678.254944 Heatmap 235.376816 Total loss 2295.14331\n",
      "Epoch 300 Step 4320/5304 Paf1 762.443115 Paf2 743.863342 Paf3 732.358459 Heatmap 251.808197 Total loss 2490.47314\n",
      "Epoch 300 Step 4330/5304 Paf1 935.04 Paf2 869.415344 Paf3 897.547485 Heatmap 278.391296 Total loss 2980.39404\n",
      "Epoch 300 Step 4340/5304 Paf1 821.457581 Paf2 813.610779 Paf3 796.127563 Heatmap 266.113464 Total loss 2697.30933\n",
      "Epoch 300 Step 4350/5304 Paf1 889.472229 Paf2 818.167236 Paf3 816.214539 Heatmap 259.391602 Total loss 2783.24561\n",
      "Epoch 300 Step 4360/5304 Paf1 787.10437 Paf2 734.586243 Paf3 723.383423 Heatmap 225.321198 Total loss 2470.39526\n",
      "Epoch 300 Step 4370/5304 Paf1 605.108276 Paf2 576.181946 Paf3 586.009827 Heatmap 165.928131 Total loss 1933.22827\n",
      "Epoch 300 Step 4380/5304 Paf1 608.937561 Paf2 583.499512 Paf3 582.536194 Heatmap 215.789246 Total loss 1990.76245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300 Step 4390/5304 Paf1 532.071716 Paf2 467.874878 Paf3 475.104767 Heatmap 150.421631 Total loss 1625.4729\n",
      "Epoch 300 Step 4400/5304 Paf1 622.494 Paf2 590.843 Paf3 579.306 Heatmap 215.489273 Total loss 2008.13232\n",
      "Epoch 300 Step 4410/5304 Paf1 687.524292 Paf2 646.050171 Paf3 650.748413 Heatmap 233.536102 Total loss 2217.85889\n",
      "Epoch 300 Step 4420/5304 Paf1 711.301392 Paf2 645.421875 Paf3 623.460754 Heatmap 198.606918 Total loss 2178.79102\n",
      "Epoch 300 Step 4430/5304 Paf1 885.177734 Paf2 775.789307 Paf3 793.52 Heatmap 264.199371 Total loss 2718.68652\n",
      "Epoch 300 Step 4440/5304 Paf1 1027.57434 Paf2 977.55835 Paf3 987.681213 Heatmap 352.846039 Total loss 3345.66\n",
      "Epoch 300 Step 4450/5304 Paf1 684.134521 Paf2 639.516113 Paf3 637.383 Heatmap 225.969543 Total loss 2187.00317\n",
      "Epoch 300 Step 4460/5304 Paf1 585.862549 Paf2 548.871094 Paf3 541.323853 Heatmap 165.27359 Total loss 1841.33105\n",
      "Epoch 300 Step 4470/5304 Paf1 524.549561 Paf2 468.238495 Paf3 484.055511 Heatmap 128.224945 Total loss 1605.0686\n",
      "Epoch 300 Step 4480/5304 Paf1 714.925293 Paf2 666.109375 Paf3 654.834412 Heatmap 199.731598 Total loss 2235.60059\n",
      "Epoch 300 Step 4490/5304 Paf1 800.296143 Paf2 732.573547 Paf3 720.118713 Heatmap 215.837555 Total loss 2468.82593\n",
      "Epoch 300 Step 4500/5304 Paf1 533.482422 Paf2 489.903168 Paf3 496.398865 Heatmap 184.247528 Total loss 1704.03198\n",
      "Epoch 300 Step 4510/5304 Paf1 649.623718 Paf2 610.117 Paf3 605.20874 Heatmap 171.878143 Total loss 2036.82764\n",
      "Epoch 300 Step 4520/5304 Paf1 837.520142 Paf2 812.39917 Paf3 805.144287 Heatmap 268.085144 Total loss 2723.14893\n",
      "Epoch 300 Step 4530/5304 Paf1 582.463074 Paf2 551.890625 Paf3 550.556946 Heatmap 178.709991 Total loss 1863.62073\n",
      "Epoch 300 Step 4540/5304 Paf1 852.154358 Paf2 798.905151 Paf3 788.851379 Heatmap 289.451172 Total loss 2729.36206\n",
      "Epoch 300 Step 4550/5304 Paf1 665.241 Paf2 624.47168 Paf3 615.296753 Heatmap 197.667572 Total loss 2102.677\n",
      "Epoch 300 Step 4560/5304 Paf1 680.997925 Paf2 637.756287 Paf3 618.846497 Heatmap 238.059189 Total loss 2175.66\n",
      "Epoch 300 Step 4570/5304 Paf1 608.149475 Paf2 579.473572 Paf3 571.296143 Heatmap 207.80542 Total loss 1966.72461\n",
      "Epoch 300 Step 4580/5304 Paf1 569.037476 Paf2 533.718933 Paf3 530.2453 Heatmap 172.713379 Total loss 1805.71509\n",
      "Epoch 300 Step 4590/5304 Paf1 561.58136 Paf2 526.426636 Paf3 534.576538 Heatmap 190.724136 Total loss 1813.30872\n",
      "Epoch 300 Step 4600/5304 Paf1 523.452759 Paf2 504.903687 Paf3 498.57666 Heatmap 170.010956 Total loss 1696.94409\n",
      "Epoch 300 Step 4610/5304 Paf1 704.763855 Paf2 660.840271 Paf3 652.866821 Heatmap 206.439697 Total loss 2224.91064\n",
      "Epoch 300 Step 4620/5304 Paf1 603.892334 Paf2 585.91272 Paf3 564.624146 Heatmap 168.898895 Total loss 1923.32812\n",
      "Epoch 300 Step 4630/5304 Paf1 557.776367 Paf2 507.152374 Paf3 500.591339 Heatmap 146.169647 Total loss 1711.6897\n",
      "Epoch 300 Step 4640/5304 Paf1 907.275 Paf2 867.516663 Paf3 856.546509 Heatmap 301.515106 Total loss 2932.85352\n",
      "Epoch 300 Step 4650/5304 Paf1 423.893066 Paf2 394.763397 Paf3 396.109192 Heatmap 106.380493 Total loss 1321.14624\n",
      "Epoch 300 Step 4660/5304 Paf1 363.146393 Paf2 330.515839 Paf3 340.990875 Heatmap 109.101143 Total loss 1143.75427\n",
      "Epoch 300 Step 4670/5304 Paf1 753.45575 Paf2 684.863953 Paf3 682.506897 Heatmap 205.314331 Total loss 2326.14087\n",
      "Epoch 300 Step 4680/5304 Paf1 891.82666 Paf2 851.483704 Paf3 836.374939 Heatmap 259.900452 Total loss 2839.58569\n",
      "Epoch 300 Step 4690/5304 Paf1 519.589417 Paf2 513.294 Paf3 493.086243 Heatmap 140.08638 Total loss 1666.05603\n",
      "Epoch 300 Step 4700/5304 Paf1 748.537476 Paf2 708.233398 Paf3 702.201965 Heatmap 238.539276 Total loss 2397.51221\n",
      "Epoch 300 Step 4710/5304 Paf1 743.044861 Paf2 707.841492 Paf3 708.306702 Heatmap 246.772568 Total loss 2405.96558\n",
      "Epoch 300 Step 4720/5304 Paf1 883.067078 Paf2 836.887573 Paf3 841.242065 Heatmap 291.460388 Total loss 2852.65698\n",
      "Epoch 300 Step 4730/5304 Paf1 733.117371 Paf2 680.338 Paf3 675.775757 Heatmap 246.663757 Total loss 2335.89478\n",
      "Epoch 300 Step 4740/5304 Paf1 614.722107 Paf2 559.178955 Paf3 555.027649 Heatmap 193.712891 Total loss 1922.6416\n",
      "Epoch 300 Step 4750/5304 Paf1 634.799561 Paf2 604.695129 Paf3 603.325684 Heatmap 199.299988 Total loss 2042.12036\n",
      "Epoch 300 Step 4760/5304 Paf1 595.347229 Paf2 579.178345 Paf3 557.7547 Heatmap 175.369446 Total loss 1907.64978\n",
      "Epoch 300 Step 4770/5304 Paf1 529.984253 Paf2 500.219849 Paf3 511.829651 Heatmap 170.30574 Total loss 1712.33948\n",
      "Epoch 300 Step 4780/5304 Paf1 717.594177 Paf2 688.773 Paf3 684.904297 Heatmap 190.202576 Total loss 2281.47412\n",
      "Epoch 300 Step 4790/5304 Paf1 574.593 Paf2 537.260193 Paf3 554.975 Heatmap 148.915924 Total loss 1815.74414\n",
      "Epoch 300 Step 4800/5304 Paf1 581.187622 Paf2 529.413452 Paf3 527.168213 Heatmap 178.199493 Total loss 1815.96875\n",
      "Epoch 300 Step 4810/5304 Paf1 794.789612 Paf2 763.145752 Paf3 763.621765 Heatmap 269.914276 Total loss 2591.47119\n",
      "Epoch 300 Step 4820/5304 Paf1 588.005493 Paf2 539.671509 Paf3 542.75116 Heatmap 158.686279 Total loss 1829.1145\n",
      "Epoch 300 Step 4830/5304 Paf1 535.311035 Paf2 529.917908 Paf3 499.441956 Heatmap 169.520142 Total loss 1734.19116\n",
      "Epoch 300 Step 4840/5304 Paf1 720.398499 Paf2 678.442871 Paf3 664.745361 Heatmap 213.978531 Total loss 2277.56519\n",
      "Epoch 300 Step 4850/5304 Paf1 469.990051 Paf2 447.009766 Paf3 426.950439 Heatmap 145.695129 Total loss 1489.64539\n",
      "Epoch 300 Step 4860/5304 Paf1 690.605408 Paf2 616.001282 Paf3 609.070557 Heatmap 205.834473 Total loss 2121.51172\n",
      "Epoch 300 Step 4870/5304 Paf1 397.340973 Paf2 370.104065 Paf3 357.515381 Heatmap 104.066811 Total loss 1229.02722\n",
      "Epoch 300 Step 4880/5304 Paf1 984.315918 Paf2 915.598 Paf3 923.475708 Heatmap 331.333069 Total loss 3154.72266\n",
      "Epoch 300 Step 4890/5304 Paf1 565.387817 Paf2 520.28595 Paf3 513.798828 Heatmap 190.227203 Total loss 1789.69983\n",
      "Epoch 300 Step 4900/5304 Paf1 558.73291 Paf2 531.532532 Paf3 523.621887 Heatmap 171.137146 Total loss 1785.02441\n",
      "Epoch 300 Step 4910/5304 Paf1 1103.25745 Paf2 1018.22192 Paf3 1026.10046 Heatmap 309.671143 Total loss 3457.25098\n",
      "Epoch 300 Step 4920/5304 Paf1 424.12738 Paf2 396.098511 Paf3 386.559021 Heatmap 124.108505 Total loss 1330.89343\n",
      "Epoch 300 Step 4930/5304 Paf1 809.628296 Paf2 764.709717 Paf3 779.23877 Heatmap 226.864716 Total loss 2580.44141\n",
      "Epoch 300 Step 4940/5304 Paf1 547.959534 Paf2 494.290344 Paf3 487.910614 Heatmap 144.720734 Total loss 1674.88123\n",
      "Epoch 300 Step 4950/5304 Paf1 713.643311 Paf2 669.720093 Paf3 661.519714 Heatmap 206.990906 Total loss 2251.87402\n",
      "Epoch 300 Step 4960/5304 Paf1 964.152588 Paf2 885.912842 Paf3 858.816284 Heatmap 255.323486 Total loss 2964.20508\n",
      "Epoch 300 Step 4970/5304 Paf1 699.849365 Paf2 661.233765 Paf3 657.606445 Heatmap 269.329407 Total loss 2288.01904\n",
      "Epoch 300 Step 4980/5304 Paf1 562.405518 Paf2 533.350708 Paf3 537.095215 Heatmap 169.769043 Total loss 1802.62048\n",
      "Epoch 300 Step 4990/5304 Paf1 595.554871 Paf2 564.952698 Paf3 562.988159 Heatmap 215.066681 Total loss 1938.56238\n",
      "Epoch 300 Step 5000/5304 Paf1 970.223267 Paf2 907.812805 Paf3 904.674072 Heatmap 336.654297 Total loss 3119.3645\n",
      "Saved checkpoint for step 5000: ./tf_ckpts_singlenet/0629_epoch300/ckpt-1799\n",
      "Epoch 300 Step 5010/5304 Paf1 811.728577 Paf2 740.273376 Paf3 731.499 Heatmap 233.19696 Total loss 2516.698\n",
      "Epoch 300 Step 5020/5304 Paf1 708.999573 Paf2 679.698853 Paf3 667.247681 Heatmap 242.019135 Total loss 2297.96533\n",
      "Epoch 300 Step 5030/5304 Paf1 931.109619 Paf2 889.573853 Paf3 896.269958 Heatmap 340.738 Total loss 3057.69141\n",
      "Epoch 300 Step 5040/5304 Paf1 667.573669 Paf2 628.381531 Paf3 625.706787 Heatmap 215.119873 Total loss 2136.78174\n",
      "Epoch 300 Step 5050/5304 Paf1 592.894897 Paf2 540.939148 Paf3 554.703247 Heatmap 172.869354 Total loss 1861.40662\n",
      "Epoch 300 Step 5060/5304 Paf1 496.486908 Paf2 467.171417 Paf3 461.92691 Heatmap 141.575653 Total loss 1567.16089\n",
      "Epoch 300 Step 5070/5304 Paf1 857.358276 Paf2 785.7323 Paf3 796.693909 Heatmap 271.769379 Total loss 2711.55371\n",
      "Epoch 300 Step 5080/5304 Paf1 946.516174 Paf2 898.934082 Paf3 893.944702 Heatmap 265.465271 Total loss 3004.86\n",
      "Epoch 300 Step 5090/5304 Paf1 423.728363 Paf2 389.934753 Paf3 378.448456 Heatmap 116.559052 Total loss 1308.67065\n",
      "Epoch 300 Step 5100/5304 Paf1 771.361938 Paf2 722.541931 Paf3 721.526611 Heatmap 248.954163 Total loss 2464.38452\n",
      "Epoch 300 Step 5110/5304 Paf1 492.766296 Paf2 445.077271 Paf3 448.567139 Heatmap 134.878479 Total loss 1521.28918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300 Step 5120/5304 Paf1 749.65 Paf2 714.565247 Paf3 715.611938 Heatmap 243.297821 Total loss 2423.125\n",
      "Epoch 300 Step 5130/5304 Paf1 384.43988 Paf2 356.984344 Paf3 367.221191 Heatmap 125.634773 Total loss 1234.28015\n",
      "Epoch 300 Step 5140/5304 Paf1 668.983948 Paf2 632.555542 Paf3 617.728 Heatmap 210.139496 Total loss 2129.40723\n",
      "Epoch 300 Step 5150/5304 Paf1 797.634033 Paf2 741.452881 Paf3 764.952576 Heatmap 251.396362 Total loss 2555.43579\n",
      "Epoch 300 Step 5160/5304 Paf1 611.222107 Paf2 562.444092 Paf3 552.874756 Heatmap 199.497986 Total loss 1926.03906\n",
      "Epoch 300 Step 5170/5304 Paf1 542.624512 Paf2 501.748535 Paf3 511.456757 Heatmap 153.903946 Total loss 1709.73376\n",
      "Epoch 300 Step 5180/5304 Paf1 865.142944 Paf2 797.945251 Paf3 807.537 Heatmap 265.729187 Total loss 2736.35425\n",
      "Epoch 300 Step 5190/5304 Paf1 703.665527 Paf2 653.988098 Paf3 654.364258 Heatmap 234.859406 Total loss 2246.8772\n",
      "Epoch 300 Step 5200/5304 Paf1 566.988342 Paf2 535.429443 Paf3 542.869446 Heatmap 156.031174 Total loss 1801.31836\n",
      "Epoch 300 Step 5210/5304 Paf1 670.669861 Paf2 672.468872 Paf3 649.049744 Heatmap 198.778763 Total loss 2190.96729\n",
      "Epoch 300 Step 5220/5304 Paf1 494.870117 Paf2 480.938202 Paf3 473.809021 Heatmap 165.923676 Total loss 1615.54102\n",
      "Epoch 300 Step 5230/5304 Paf1 799.135803 Paf2 783.956055 Paf3 782.320801 Heatmap 235.028442 Total loss 2600.44092\n",
      "Epoch 300 Step 5240/5304 Paf1 632.641846 Paf2 611.455444 Paf3 609.301147 Heatmap 190.208069 Total loss 2043.60645\n",
      "Epoch 300 Step 5250/5304 Paf1 612.647 Paf2 589.298584 Paf3 583.736084 Heatmap 180.024506 Total loss 1965.70618\n",
      "Epoch 300 Step 5260/5304 Paf1 416.400879 Paf2 405.068848 Paf3 394.94397 Heatmap 106.661224 Total loss 1323.07495\n",
      "Epoch 300 Step 5270/5304 Paf1 829.419 Paf2 797.303162 Paf3 808.651184 Heatmap 271.238312 Total loss 2706.61182\n",
      "Epoch 300 Step 5280/5304 Paf1 748.010864 Paf2 662.685852 Paf3 681.539429 Heatmap 207.043365 Total loss 2299.27954\n",
      "Epoch 300 Step 5290/5304 Paf1 749.975586 Paf2 706.467163 Paf3 717.193787 Heatmap 216.610901 Total loss 2390.24756\n",
      "Epoch 300 Step 5300/5304 Paf1 776.434937 Paf2 776.490906 Paf3 758.968262 Heatmap 241.400253 Total loss 2553.29443\n",
      "Completed epoch 300. Saving weights...\n",
      "Epoch training time: 0:15:45.973452\n",
      "Calculating validation losses...\n",
      "Validation step 0 ...\n",
      "Validation losses for epoch: 300 : Loss paf 683.3309936523438, Loss heatmap 226.08786010742188, Total loss 2317.03369140625\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # registering custom blocks types\n",
    "    register_tf_netbuilder_extensions()\n",
    "\n",
    "    # loading datasets\n",
    "    ds_train, ds_train_size = get_dataset(annot_path_train, img_dir_train, batch_size)\n",
    "    ds_val, ds_val_size = get_dataset(annot_path_val, img_dir_val, batch_size, strict=True)\n",
    "    print(f\"Training samples: {ds_train_size} , Validation samples: {ds_val_size}\")\n",
    "\n",
    "    steps_per_epoch = ds_train_size // batch_size\n",
    "    # steps_per_epoch_val = ds_val_size // batch_size\n",
    "\n",
    "    # creating model, optimizers etc\n",
    "    model = create_openpose_singlenet(pretrained=False)\n",
    "    optimizer = Adam(lr)\n",
    "\n",
    "    # loading previous state if required\n",
    "    ckpt = tf.train.Checkpoint(step=tf.Variable(0), epoch=tf.Variable(0), optimizer=optimizer, net=model)\n",
    "    manager = tf.train.CheckpointManager(ckpt, checkpoints_folder, max_to_keep=3)\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    last_step = int(ckpt.step)\n",
    "    last_epoch = int(ckpt.epoch)\n",
    "\n",
    "    if manager.latest_checkpoint:\n",
    "        print(f\"Restored from {manager.latest_checkpoint}\")\n",
    "        print(f\"Resumed from epoch {last_epoch}, step {last_step}\")\n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "\n",
    "        path = download_checkpoint(pretrained_mobilenet_v3_url)\n",
    "        model.load_weights(path).expect_partial()\n",
    "\n",
    "    train(ds_train, ds_val, model, optimizer, ckpt, last_epoch, last_step, max_epochs, steps_per_epoch)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2affef022b9afcdb2036f23fd24a6d21ab56eedbfd7fcf2a2fa3db95b4fa44a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('mp-pose-cap': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}